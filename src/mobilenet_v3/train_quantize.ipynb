{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimet_torch.quantsim import QuantizationSimModel\n",
    "from aimet_common.defs import QuantScheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total annotations before filtering: 860001\n",
      "Total annotations after filtering: 204543\n",
      "Total COCO annotations: 860001\n",
      "Filtered annotations: 204543\n",
      "2025-02-28 13:47:10,743 - Quant - INFO - No config file provided, defaulting to config file at /home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/aimet_common/quantsim_config/default_config.json\n",
      "2025-02-28 13:47:10,753 - Quant - INFO - Unsupported op type Squeeze\n",
      "2025-02-28 13:47:10,753 - Quant - INFO - Unsupported op type Mean\n",
      "2025-02-28 13:47:10,761 - Quant - INFO - Selecting DefaultOpInstanceConfigGenerator to compute the specialized config. hw_version:default\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Encoding dictionary contains modules/parameters that doesn't exist in the model: /avgpool/GlobalAveragePool_output_0, /classifier/0/Gemm_output_0, /classifier/1/HardSwish_output_0, /features/0/0/Conv_output_0, /features/0/2/HardSwish_output_0, /features/1/block/0/2/Relu_output_0, /features/1/block/1/Mul_output_0, /features/1/block/1/activation/Relu_output_0, /features/1/block/1/avgpool/GlobalAveragePool_output_0, /features/1/block/1/fc2/Conv_output_0, /features/1/block/1/scale_activation/HardSigmoid_output_0, /features/1/block/2/0/Conv_output_0, /features/10/Add_output_0, /features/10/block/0/0/Conv_output_0, /features/10/block/0/2/HardSwish_output_0, /features/10/block/1/0/Conv_output_0, /features/10/block/1/2/HardSwish_output_0, /features/10/block/2/Mul_output_0, /features/10/block/2/activation/Relu_output_0, /features/10/block/2/avgpool/GlobalAveragePool_output_0, /features/10/block/2/fc2/Conv_output_0, /features/10/block/2/scale_activation/HardSigmoid_output_0, /features/10/block/3/0/Conv_output_0, /features/11/Add_output_0, /features/11/block/0/0/Conv_output_0, /features/11/block/0/2/HardSwish_output_0, /features/11/block/1/0/Conv_output_0, /features/11/block/1/2/HardSwish_output_0, /features/11/block/2/Mul_output_0, /features/11/block/2/activation/Relu_output_0, /features/11/block/2/avgpool/GlobalAveragePool_output_0, /features/11/block/2/fc2/Conv_output_0, /features/11/block/2/scale_activation/HardSigmoid_output_0, /features/11/block/3/0/Conv_output_0, /features/12/0/Conv_output_0, /features/12/2/HardSwish_output_0, /features/2/block/0/2/Relu_output_0, /features/2/block/1/2/Relu_output_0, /features/2/block/2/0/Conv_output_0, /features/3/Add_output_0, /features/3/block/0/2/Relu_output_0, /features/3/block/1/2/Relu_output_0, /features/3/block/2/0/Conv_output_0, /features/4/block/0/0/Conv_output_0, /features/4/block/0/2/HardSwish_output_0, /features/4/block/1/0/Conv_output_0, /features/4/block/1/2/HardSwish_output_0, /features/4/block/2/Mul_output_0, /features/4/block/2/activation/Relu_output_0, /features/4/block/2/avgpool/GlobalAveragePool_output_0, /features/4/block/2/fc2/Conv_output_0, /features/4/block/2/scale_activation/HardSigmoid_output_0, /features/4/block/3/0/Conv_output_0, /features/5/Add_output_0, /features/5/block/0/0/Conv_output_0, /features/5/block/0/2/HardSwish_output_0, /features/5/block/1/0/Conv_output_0, /features/5/block/1/2/HardSwish_output_0, /features/5/block/2/Mul_output_0, /features/5/block/2/activation/Relu_output_0, /features/5/block/2/avgpool/GlobalAveragePool_output_0, /features/5/block/2/fc2/Conv_output_0, /features/5/block/2/scale_activation/HardSigmoid_output_0, /features/5/block/3/0/Conv_output_0, /features/6/Add_output_0, /features/6/block/0/0/Conv_output_0, /features/6/block/0/2/HardSwish_output_0, /features/6/block/1/0/Conv_output_0, /features/6/block/1/2/HardSwish_output_0, /features/6/block/2/Mul_output_0, /features/6/block/2/activation/Relu_output_0, /features/6/block/2/avgpool/GlobalAveragePool_output_0, /features/6/block/2/fc2/Conv_output_0, /features/6/block/2/scale_activation/HardSigmoid_output_0, /features/6/block/3/0/Conv_output_0, /features/7/block/0/0/Conv_output_0, /features/7/block/0/2/HardSwish_output_0, /features/7/block/1/0/Conv_output_0, /features/7/block/1/2/HardSwish_output_0, /features/7/block/2/Mul_output_0, /features/7/block/2/activation/Relu_output_0, /features/7/block/2/avgpool/GlobalAveragePool_output_0, /features/7/block/2/fc2/Conv_output_0, /features/7/block/2/scale_activation/HardSigmoid_output_0, /features/7/block/3/0/Conv_output_0, /features/8/Add_output_0, /features/8/block/0/0/Conv_output_0, /features/8/block/0/2/HardSwish_output_0, /features/8/block/1/0/Conv_output_0, /features/8/block/1/2/HardSwish_output_0, /features/8/block/2/Mul_output_0, /features/8/block/2/activation/Relu_output_0, /features/8/block/2/avgpool/GlobalAveragePool_output_0, /features/8/block/2/fc2/Conv_output_0, /features/8/block/2/scale_activation/HardSigmoid_output_0, /features/8/block/3/0/Conv_output_0, /features/9/block/0/0/Conv_output_0, /features/9/block/0/2/HardSwish_output_0, /features/9/block/1/0/Conv_output_0, /features/9/block/1/2/HardSwish_output_0, /features/9/block/2/Mul_output_0, /features/9/block/2/activation/Relu_output_0, /features/9/block/2/avgpool/GlobalAveragePool_output_0, /features/9/block/2/fc2/Conv_output_0, /features/9/block/2/scale_activation/HardSigmoid_output_0, /features/9/block/3/0/Conv_output_0, 646, t.1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 78\u001b[0m\n\u001b[1;32m     69\u001b[0m dummy_input\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m224\u001b[39m,\u001b[38;5;241m224\u001b[39m)\n\u001b[1;32m     70\u001b[0m sim\u001b[38;5;241m=\u001b[39mQuantizationSimModel(\n\u001b[1;32m     71\u001b[0m     model\u001b[38;5;241m=\u001b[39mbase_model,\n\u001b[1;32m     72\u001b[0m     dummy_input\u001b[38;5;241m=\u001b[39mdummy_input,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m \n\u001b[1;32m     77\u001b[0m )\n\u001b[0;32m---> 78\u001b[0m \u001b[43msim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_encodings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquantized/mobilenetv3quantized.encodings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mLightningQuantModel\u001b[39;00m(pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_for_qat, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m,alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m):\n",
      "File \u001b[0;32m~/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/aimet_torch/_base/quantsim.py:1577\u001b[0m, in \u001b[0;36m_QuantizationSimModelBase.load_encodings\u001b[0;34m(self, encodings, strict, partial, requires_grad, allow_overwrite)\u001b[0m\n\u001b[1;32m   1574\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(encodings, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m   1575\u001b[0m         encodings \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m-> 1577\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_encodings_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencodings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_overwrite\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/aimet_torch/_base/quantsim.py:1606\u001b[0m, in \u001b[0;36m_QuantizationSimModelBase._load_encodings_impl\u001b[0;34m(self, encodings, strict, partial, requires_grad, allow_overwrite)\u001b[0m\n\u001b[1;32m   1604\u001b[0m         keys_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28msorted\u001b[39m(keys_not_found))\n\u001b[1;32m   1605\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoding dictionary contains modules/parameters that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist in the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeys_not_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1606\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m   1608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param_encodings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1609\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_param_encodings(param_encodings,\n\u001b[1;32m   1610\u001b[0m                               strict, partial, requires_grad, allow_overwrite)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Encoding dictionary contains modules/parameters that doesn't exist in the model: /avgpool/GlobalAveragePool_output_0, /classifier/0/Gemm_output_0, /classifier/1/HardSwish_output_0, /features/0/0/Conv_output_0, /features/0/2/HardSwish_output_0, /features/1/block/0/2/Relu_output_0, /features/1/block/1/Mul_output_0, /features/1/block/1/activation/Relu_output_0, /features/1/block/1/avgpool/GlobalAveragePool_output_0, /features/1/block/1/fc2/Conv_output_0, /features/1/block/1/scale_activation/HardSigmoid_output_0, /features/1/block/2/0/Conv_output_0, /features/10/Add_output_0, /features/10/block/0/0/Conv_output_0, /features/10/block/0/2/HardSwish_output_0, /features/10/block/1/0/Conv_output_0, /features/10/block/1/2/HardSwish_output_0, /features/10/block/2/Mul_output_0, /features/10/block/2/activation/Relu_output_0, /features/10/block/2/avgpool/GlobalAveragePool_output_0, /features/10/block/2/fc2/Conv_output_0, /features/10/block/2/scale_activation/HardSigmoid_output_0, /features/10/block/3/0/Conv_output_0, /features/11/Add_output_0, /features/11/block/0/0/Conv_output_0, /features/11/block/0/2/HardSwish_output_0, /features/11/block/1/0/Conv_output_0, /features/11/block/1/2/HardSwish_output_0, /features/11/block/2/Mul_output_0, /features/11/block/2/activation/Relu_output_0, /features/11/block/2/avgpool/GlobalAveragePool_output_0, /features/11/block/2/fc2/Conv_output_0, /features/11/block/2/scale_activation/HardSigmoid_output_0, /features/11/block/3/0/Conv_output_0, /features/12/0/Conv_output_0, /features/12/2/HardSwish_output_0, /features/2/block/0/2/Relu_output_0, /features/2/block/1/2/Relu_output_0, /features/2/block/2/0/Conv_output_0, /features/3/Add_output_0, /features/3/block/0/2/Relu_output_0, /features/3/block/1/2/Relu_output_0, /features/3/block/2/0/Conv_output_0, /features/4/block/0/0/Conv_output_0, /features/4/block/0/2/HardSwish_output_0, /features/4/block/1/0/Conv_output_0, /features/4/block/1/2/HardSwish_output_0, /features/4/block/2/Mul_output_0, /features/4/block/2/activation/Relu_output_0, /features/4/block/2/avgpool/GlobalAveragePool_output_0, /features/4/block/2/fc2/Conv_output_0, /features/4/block/2/scale_activation/HardSigmoid_output_0, /features/4/block/3/0/Conv_output_0, /features/5/Add_output_0, /features/5/block/0/0/Conv_output_0, /features/5/block/0/2/HardSwish_output_0, /features/5/block/1/0/Conv_output_0, /features/5/block/1/2/HardSwish_output_0, /features/5/block/2/Mul_output_0, /features/5/block/2/activation/Relu_output_0, /features/5/block/2/avgpool/GlobalAveragePool_output_0, /features/5/block/2/fc2/Conv_output_0, /features/5/block/2/scale_activation/HardSigmoid_output_0, /features/5/block/3/0/Conv_output_0, /features/6/Add_output_0, /features/6/block/0/0/Conv_output_0, /features/6/block/0/2/HardSwish_output_0, /features/6/block/1/0/Conv_output_0, /features/6/block/1/2/HardSwish_output_0, /features/6/block/2/Mul_output_0, /features/6/block/2/activation/Relu_output_0, /features/6/block/2/avgpool/GlobalAveragePool_output_0, /features/6/block/2/fc2/Conv_output_0, /features/6/block/2/scale_activation/HardSigmoid_output_0, /features/6/block/3/0/Conv_output_0, /features/7/block/0/0/Conv_output_0, /features/7/block/0/2/HardSwish_output_0, /features/7/block/1/0/Conv_output_0, /features/7/block/1/2/HardSwish_output_0, /features/7/block/2/Mul_output_0, /features/7/block/2/activation/Relu_output_0, /features/7/block/2/avgpool/GlobalAveragePool_output_0, /features/7/block/2/fc2/Conv_output_0, /features/7/block/2/scale_activation/HardSigmoid_output_0, /features/7/block/3/0/Conv_output_0, /features/8/Add_output_0, /features/8/block/0/0/Conv_output_0, /features/8/block/0/2/HardSwish_output_0, /features/8/block/1/0/Conv_output_0, /features/8/block/1/2/HardSwish_output_0, /features/8/block/2/Mul_output_0, /features/8/block/2/activation/Relu_output_0, /features/8/block/2/avgpool/GlobalAveragePool_output_0, /features/8/block/2/fc2/Conv_output_0, /features/8/block/2/scale_activation/HardSigmoid_output_0, /features/8/block/3/0/Conv_output_0, /features/9/block/0/0/Conv_output_0, /features/9/block/0/2/HardSwish_output_0, /features/9/block/1/0/Conv_output_0, /features/9/block/1/2/HardSwish_output_0, /features/9/block/2/Mul_output_0, /features/9/block/2/activation/Relu_output_0, /features/9/block/2/avgpool/GlobalAveragePool_output_0, /features/9/block/2/fc2/Conv_output_0, /features/9/block/2/scale_activation/HardSigmoid_output_0, /features/9/block/3/0/Conv_output_0, 646, t.1"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.profilers import PyTorchProfiler\n",
    "import sys\n",
    "\n",
    "# Import your dataset modules\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "import dataset.DatasetReader as DatasetReader\n",
    "import dataset.utils as utils\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p = 0.3),\n",
    "    transforms.ColorJitter(brightness = 0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define dataset paths\n",
    "root_folder = r\"../../datasets/imagenet/new_imgnet_coco\"\n",
    "annotation_file = os.path.join(r\"../../datasets\", \"coco\", \"annotations\", \"instances_train2017.json\")\n",
    "image_dir = os.path.join(r\"../../datasets\", \"coco\", \"train2017\")\n",
    "\n",
    "# Load datasets\n",
    "dataset_coco = DatasetReader.COCODataset(\n",
    "    annotation_file=annotation_file,\n",
    "    image_dir=image_dir,\n",
    "    target_classes=[s.lower() for s in utils.GLOBAL_CLASSES],\n",
    "    transform=transform,\n",
    "    min_size = 60\n",
    ")\n",
    "\n",
    "class_names = [s.lower().replace(' ', '_') for s in utils.GLOBAL_CLASSES]\n",
    "dataset_imagenet = DatasetReader.CustomImageFolder(root_dir=root_folder, class_names=class_names, transform=transform)\n",
    "\n",
    "dataset = torch.utils.data.ConcatDataset([dataset_coco, dataset_imagenet])\n",
    "\n",
    "# Define DataLoader\n",
    "batch_size = 128*5\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=15, persistent_workers=True, pin_memory=True,prefetch_factor=2)\n",
    "\n",
    "#Napravi cist AIMET Model bez kvantizacije\n",
    "num_classes=64\n",
    "\n",
    "base_model=models.mobilenet_v3_small(pretrained=False)\n",
    "base_model.classifier[3]=nn.Linear(base_model.classifier[3].in_features,num_classes)\n",
    "\n",
    "q_model=\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Kreira fake model, pa cemo mu ubaciti nas\n",
    "\n",
    "dummy_input=torch.randn(1,3,224,224)\n",
    "sim=QuantizationSimModel(\n",
    "    model=q_model,\n",
    "    dummy_input=dummy_input,\n",
    "    quant_scheme=QuantScheme.training_range_learning_with_tf_enhanced_init,\n",
    "    default_param_bw=8,\n",
    "    default_output_bw=8\n",
    "\n",
    ")\n",
    "sim.load_encodings(\"quantized/mobilenetv3quantized.encodings\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LightningQuantModel(pl.LightningModule):\n",
    "    def __init__(self, model_for_qat, num_classes=64, learning_rate=2e-5,alpha=0.7):\n",
    "        super(LightningQuantModel, self).__init__()\n",
    "        self.model = model_for_qat\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels, ds = batch \n",
    "        images, labels = images.to(self.device), labels.to(self.device)\n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "\n",
    "        ds = ds.to(self.device)\n",
    "        alpha_mask = (ds == 0).float() * self.alpha + (ds == 1).float() * (1 - self.alpha)\n",
    "        loss = loss * alpha_mask\n",
    "        loss = loss.mean()\n",
    "\n",
    "        # if(ds == 0):\n",
    "        #     loss = self.alpha*loss\n",
    "        # elif(ds == 1):\n",
    "        #     loss = (1 - self.alpha)*loss\n",
    "        \n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "\n",
    "# Define model checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"models/\",\n",
    "    filename=\"quant_mobilenet_v3-{epoch:02d}-{train_loss:.2f}\",\n",
    "    save_top_k=3,\n",
    "    monitor=\"train_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "logger = TensorBoardLogger(\"logs/\", name = \"quant_mobilenet_v3\", default_hp_metric=False)\n",
    "profiler= PyTorchProfiler(on_trace_ready=torch.profiler.tensorboard_trace_handler(\"tb_logs/profiler0\"),trace_memory=True,\n",
    "                          schedule=torch.profiler.schedule(skip_first=10,warmup=1,wait=1,active=20))\n",
    "\n",
    "\n",
    "quant_model = LightningQuantModel(sim.model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    profiler=profiler,\n",
    "    max_epochs=200,\n",
    "    devices=1,  # Use only one GPU or CPU\n",
    "    accelerator=\"auto\",\n",
    "    strategy=\"auto\",  # Allows later multi-GPU setup without changing code\n",
    "    precision = \"16-mixed\",\n",
    "    callbacks=[checkpoint_callback],\n",
    "    logger = logger\n",
    ")\n",
    "\n",
    "trainer.fit(quant_model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.profilers import PyTorchProfiler\n",
    "\n",
    "# Import your dataset modules\n",
    "from src.dataset.coco import get_coco\n",
    "import src.dataset.DatasetReader as DatasetReader\n",
    "import src.dataset.utils as utils\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p = 0.3),\n",
    "    transforms.ColorJitter(brightness = 0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define dataset paths\n",
    "root_folder = r\"datasets/imagenet/new_imgnet_coco\"\n",
    "annotation_file = os.path.join(r\"datasets\", \"coco\", \"annotations\", \"instances_train2017.json\")\n",
    "image_dir = os.path.join(r\"datasets\", \"coco\", \"train2017\")\n",
    "\n",
    "# Load datasets\n",
    "dataset_coco = DatasetReader.COCODataset(\n",
    "    annotation_file=annotation_file,\n",
    "    image_dir=image_dir,\n",
    "    target_classes=[s.lower() for s in utils.GLOBAL_CLASSES],\n",
    "    transform=transform,\n",
    "    min_size = 60\n",
    ")\n",
    "\n",
    "class_names = [s.lower().replace(' ', '_') for s in utils.GLOBAL_CLASSES]\n",
    "dataset_imagenet = DatasetReader.CustomImageFolder(root_dir=root_folder, class_names=class_names, transform=transform)\n",
    "\n",
    "dataset = torch.utils.data.ConcatDataset([dataset_coco, dataset_imagenet])\n",
    "\n",
    "# Define DataLoader\n",
    "batch_size = 128*5\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=15, persistent_workers=True, pin_memory=True,prefetch_factor=2)\n",
    "\n",
    "# Define Lightning Model\n",
    "class LightningModel(pl.LightningModule):\n",
    "    def __init__(self, num_classes=64, learning_rate=2e-5):\n",
    "        super(LightningModel, self).__init__()\n",
    "        self.model = models.mobilenet_v3_small(pretrained=True)\n",
    "        self.model.classifier[3] = nn.Linear(self.model.classifier[3].in_features, num_classes)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.alpha = 0.7\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels, ds = batch # ds = 0 - imagenet, ds = 1 - COCO\n",
    "        images, labels = images.to(self.device), labels.to(self.device)\n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "\n",
    "        ds = ds.to(self.device)\n",
    "        alpha_mask = (ds == 0).float() * self.alpha + (ds == 1).float() * (1 - self.alpha)\n",
    "        loss = loss * alpha_mask\n",
    "        loss = loss.mean()\n",
    "\n",
    "        # if(ds == 0):\n",
    "        #     loss = self.alpha*loss\n",
    "        # elif(ds == 1):\n",
    "        #     loss = (1 - self.alpha)*loss\n",
    "        \n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "# Define model checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"models/\",\n",
    "    filename=\"mobilenet_v3-{epoch:02d}-{train_loss:.2f}\",\n",
    "    save_top_k=3,\n",
    "    monitor=\"train_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "logger = TensorBoardLogger(\"logs/\", name = \"mobilenet_v3\", default_hp_metric=False)\n",
    "profiler= PyTorchProfiler(on_trace_ready=torch.profiler.tensorboard_trace_handler(\"tb_logs/profiler0\"),trace_memory=True,\n",
    "                          schedule=torch.profiler.schedule(skip_first=10,warmup=1,wait=1,active=20))\n",
    "\n",
    "# Initialize and train model using Lightning Trainer\n",
    "\n",
    "model = LightningModel()\n",
    "\n",
    "# model = LightningModel.load_from_checkpoint(\"models/mobilenet_v3-epoch=47-train_loss=0.18.ckpt\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    profiler=profiler,\n",
    "    max_epochs=200,\n",
    "    devices=1,  # Use only one GPU or CPU\n",
    "    accelerator=\"auto\",\n",
    "    strategy=\"auto\",  # Allows later multi-GPU setup without changing code\n",
    "    precision = \"16-mixed\",\n",
    "    callbacks=[checkpoint_callback],\n",
    "    logger = logger\n",
    ")\n",
    "\n",
    "trainer.fit(model, dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
