{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 14:52:00,024 - root - INFO - AIMET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/lib/python3.10/abc.py:106: FutureWarning: `NLLLoss2d` has been deprecated. Please use `NLLLoss` instead as a drop-in replacement and see https://pytorch.org/docs/main/nn.html#torch.nn.NLLLoss for more details.\n",
      "  cls = super().__new__(mcls, name, bases, namespace, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "PreprocessedMobileNetV2(\n",
      "  (mobilenet_v2): MobileNetV2(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (2): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (3): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (4): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (5): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (6): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (7): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (8): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (9): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (10): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (11): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (12): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (13): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (14): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (15): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (16): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (17): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (18): Conv2dNormActivation(\n",
      "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=1280, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "PreprocessedMobileNetV2(\n",
      "  (mobilenet_v2): MobileNetV2(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (2): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (3): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (4): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (5): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (6): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (7): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (8): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (9): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (10): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (11): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (12): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (13): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (14): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (15): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (16): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (17): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU6(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (18): Conv2dNormActivation(\n",
      "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=1280, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from aimet_torch.cross_layer_equalization import equalize_model\n",
    "from torchvision import models\n",
    "import torch\n",
    "import MobileNetV2Pretrained\n",
    "import os\n",
    "import sys\n",
    "\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "from utils.input_getter import mug_image_getter\n",
    "\n",
    "# Load a pretrained model\n",
    "\n",
    "image_getter = mug_image_getter()\n",
    "mug_image = image_getter.get_input_torch()\n",
    "print(mug_image.shape)\n",
    "\n",
    "modelOriginal   = MobileNetV2Pretrained.PreprocessedMobileNetV2(64, r\"model/mobilenet_v2_coco.pth\")\n",
    "modelQuantized  = MobileNetV2Pretrained.PreprocessedMobileNetV2(64, r\"model/mobilenet_v2_coco.pth\")\n",
    "\n",
    "modelOriginal.eval()\n",
    "modelQuantized.eval()\n",
    "\n",
    "# model = models.resnet18(pretrained=True)\n",
    "# model = torch.nn.Sequential(\n",
    "#     torch.nn.Flatten(),\n",
    "#     torch.nn.Linear(3 * 224 * 224, 1024),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(1024, 512),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(512, 256),\n",
    "#     torch.nn.ReLU()\n",
    "# )\n",
    "\n",
    "\n",
    "print(modelQuantized)\n",
    "print(modelOriginal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 predictions for  on :\n",
      "31 Cup                   78.5%\n",
      "34 Bowl                   6.4%\n",
      "37 Orange                 1.2%\n",
      "33 Spoon                  1.2%\n",
      "41 Donut                  0.9%\n",
      "Top-5 predictions for  on :\n",
      "31 Cup                   78.5%\n",
      "34 Bowl                   6.4%\n",
      "37 Orange                 1.2%\n",
      "33 Spoon                  1.2%\n",
      "41 Donut                  0.9%\n"
     ]
    }
   ],
   "source": [
    "from utils import helper\n",
    "from dataset import utils as dsutils\n",
    "\n",
    "helper.print_probablities_from_output(modelOriginal(mug_image), dsutils.GLOBAL_CLASSES)\n",
    "helper.print_probablities_from_output(modelQuantized(mug_image), dsutils.GLOBAL_CLASSES)\n",
    "\n",
    "# print(modelOriginal(mug_image))\n",
    "# print(modelQuantized(mug_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLE Started\n",
      "2025-02-26 14:52:14,899 - ConnectedGraph - WARNING - Unable to isolate model outputs.\n",
      "Cross-Layer Equalization applied successfully!\n",
      "PreprocessedMobileNetV2(\n",
      "  (mobilenet_v2): MobileNetV2(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "      (2): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (5): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (6): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (7): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (8): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (9): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (10): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (11): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (12): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (13): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (14): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (15): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (16): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (17): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)\n",
      "            (1): Identity()\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (18): Conv2dNormActivation(\n",
      "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Identity()\n",
      "        (2): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=1280, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "param_num = 0\n",
    "\n",
    "# print(list(model.parameters())[param_num])\n",
    "# print(list(model.parameters())[param_num].shape)\n",
    "\n",
    "# Apply Cross-Layer Equalization (CLE)\n",
    "# This modifies the model in place\n",
    "\n",
    "input_shape = (1, 3, 224, 224)\n",
    "dummy_input = torch.randn(input_shape)\n",
    "\n",
    "print(\"CLE Started\")\n",
    "\n",
    "equalize_model(modelQuantized, dummy_input = dummy_input)\n",
    "\n",
    "print(\"Cross-Layer Equalization applied successfully!\")\n",
    "\n",
    "print(modelQuantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 predictions for  on :\n",
      "31 Cup                   78.5%\n",
      "34 Bowl                   6.4%\n",
      "37 Orange                 1.2%\n",
      "33 Spoon                  1.2%\n",
      "41 Donut                  0.9%\n",
      "Top-5 predictions for  on :\n",
      "31 Cup                   94.5%\n",
      "34 Bowl                   1.8%\n",
      "35 Banana                 0.4%\n",
      "37 Orange                 0.3%\n",
      "33 Spoon                  0.2%\n"
     ]
    }
   ],
   "source": [
    "helper.print_probablities_from_output(modelOriginal(mug_image), dsutils.GLOBAL_CLASSES)\n",
    "helper.print_probablities_from_output(modelQuantized(mug_image), dsutils.GLOBAL_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix posle nemam pojma kako radi\n",
    "\n",
    "# from aimet_torch.bias_correction import correct_bias\n",
    "# import torch\n",
    "\n",
    "# # Define your model (example: ResNet18)\n",
    "# model = models.resnet18(pretrained=True)\n",
    "\n",
    "# # Define a dummy dataset (batch size = 1, shape = [1, 3, 224, 224])\n",
    "# dummy_input = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "# # Apply Bias Correction\n",
    "# correct_bias(model)\n",
    "\n",
    "# print(\"Bias Correction applied successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 14:52:25,676 - ConnectedGraph - WARNING - Unable to isolate model outputs.\n",
      "2025-02-26 14:52:25,714 - Quant - INFO - No config file provided, defaulting to config file at /home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/aimet_common/quantsim_config/default_config.json\n",
      "2025-02-26 14:52:25,726 - Quant - INFO - Unsupported op type Squeeze\n",
      "2025-02-26 14:52:25,727 - Quant - INFO - Unsupported op type Mean\n",
      "2025-02-26 14:52:25,733 - Quant - INFO - Selecting DefaultOpInstanceConfigGenerator to compute the specialized config. hw_version:default\n",
      "Total annotations before filtering: 36781\n",
      "Total annotations after filtering: 13691\n",
      "Total COCO annotations: 36781\n",
      "Filtered annotations: 13691\n",
      "13691\n"
     ]
    }
   ],
   "source": [
    "from aimet_torch.quantsim import QuantizationSimModel\n",
    "from aimet_common.defs import QuantScheme\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Dummy input to define the model input size\n",
    "dummy_input = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "# Step 1: Create QuantizationSimModel\n",
    "sim = QuantizationSimModel(modelQuantized, dummy_input=dummy_input,\n",
    "                                     quant_scheme=QuantScheme.post_training_tf_enhanced,\n",
    "                                     default_param_bw=8, default_output_bw=8)\n",
    "\n",
    "from dataset import DatasetReader\n",
    "\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "datasetCOCO = DatasetReader.COCODataset(\n",
    "    annotation_file = r\"/home/centar15-desktop1/LPCV_2025_T1/datasets/coco/annotations/instances_val2017.json\", \n",
    "    image_dir= r'/home/centar15-desktop1/LPCV_2025_T1/datasets/coco/val2017',\n",
    "    target_classes=[s.lower() for s in dsutils.GLOBAL_CLASSES],\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]))\n",
    "\n",
    "dataloader = DataLoader(datasetCOCO, batch_size=1, shuffle=True)\n",
    "\n",
    "print(len(dataloader))\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Compute Encodings (calibration)\n",
    "def calibration_function(model, eval_iterations = 500, use_cuda = False):\n",
    "    for i, data in enumerate(dataloader):\n",
    "        image, _, _ = data\n",
    "        model(image)\n",
    "        if (i > eval_iterations):\n",
    "            break\n",
    "        if(i % 50 == 0):\n",
    "            print(i)\n",
    "    print(eval_iterations)\n",
    "    print(\"cal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreprocessedMobileNetV2(\n",
      "  (mobilenet_v2): MobileNetV2(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): QuantizedConv2d(\n",
      "          3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "        )\n",
      "        (1): Identity()\n",
      "        (2): QuantizedReLU(\n",
      "          (param_quantizers): ModuleDict()\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedConv2d(\n",
      "            32, 16, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "      (2): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              16, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            96, 24, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              24, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            144, 24, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              24, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            144, 32, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (5): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              32, 192, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            192, 32, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (6): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              32, 192, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            192, 32, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (7): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              32, 192, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            192, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (8): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (9): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (10): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (11): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            384, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (12): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              96, 576, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            576, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (13): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              96, 576, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            576, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (14): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              96, 576, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            576, 160, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (15): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              160, 960, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            960, 160, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (16): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              160, 960, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            960, 160, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (17): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              160, 960, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            960, 320, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (18): Conv2dNormActivation(\n",
      "        (0): QuantizedConv2d(\n",
      "          320, 1280, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "        )\n",
      "        (1): Identity()\n",
      "        (2): QuantizedReLU(\n",
      "          (param_quantizers): ModuleDict()\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): QuantizedDropout(\n",
      "        p=0.2, inplace=False\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (1): QuantizedLinear(\n",
      "        in_features=1280, out_features=64, bias=True\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(sim.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "500\n",
      "cal\n"
     ]
    }
   ],
   "source": [
    "sim.compute_encodings(calibration_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreprocessedMobileNetV2(\n",
      "  (mobilenet_v2): MobileNetV2(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): QuantizedConv2d(\n",
      "          3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "        )\n",
      "        (1): Identity()\n",
      "        (2): QuantizedReLU(\n",
      "          (param_quantizers): ModuleDict()\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedConv2d(\n",
      "            32, 16, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "      (2): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              16, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            96, 24, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              24, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            144, 24, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              24, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            144, 32, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (5): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              32, 192, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            192, 32, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (6): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              32, 192, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            192, 32, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (7): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              32, 192, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            192, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (8): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (9): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (10): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (11): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            384, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (12): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              96, 576, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            576, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (13): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              96, 576, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            576, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (14): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              96, 576, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            576, 160, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (15): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              160, 960, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            960, 160, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (16): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              160, 960, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            960, 160, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (17): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              160, 960, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            960, 320, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (18): Conv2dNormActivation(\n",
      "        (0): QuantizedConv2d(\n",
      "          320, 1280, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "        )\n",
      "        (1): Identity()\n",
      "        (2): QuantizedReLU(\n",
      "          (param_quantizers): ModuleDict()\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): QuantizedDropout(\n",
      "        p=0.2, inplace=False\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (1): QuantizedLinear(\n",
      "        in_features=1280, out_features=64, bias=True\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(sim.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DequantizedTensor([[-0.3103,  0.3103, -0.9828, -0.6724, -0.2069, -0.6207,\n",
       "                    -0.7241, -0.6724, -0.1034,  0.1552, -1.0345, -0.2069,\n",
       "                     0.1034,  0.1552, -0.8276, -0.5172, -0.3621,  0.0000,\n",
       "                     0.0517,  0.1552, -0.5690, -0.9310,  0.2586, -0.4655,\n",
       "                     0.0000,  0.3103, -0.5172, -0.2069, -0.5172,  0.2069,\n",
       "                     1.0345,  7.2932,  0.5690,  1.8621,  3.9828,  2.2242,\n",
       "                     1.3448,  1.8621, -0.3621,  0.7241, -1.5000,  1.3448,\n",
       "                     0.3621,  0.1034,  1.2414, -0.5690,  0.4138,  0.4655,\n",
       "                    -0.4655,  0.5172,  0.5172,  0.6207,  0.2069,  0.1034,\n",
       "                    -0.0517, -0.0517,  0.5172,  0.5690, -0.2069,  0.0000,\n",
       "                    -0.7241,  1.9655, -0.6207,  0.0000]],\n",
       "                  grad_fn=<AliasBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.model(mug_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 11:52:02,977 - Utils - INFO - successfully created onnx model with 88/100 node names updated\n",
      "2025-02-26 11:52:03,003 - Quant - INFO - Layers excluded from quantization: []\n",
      "2025-02-26 11:52:03,005 - Quant - WARNING - \u001b[31;21mQuantsim export will stop exporting encodings for saving and loading in a future AIMET release.\n",
      "To export encodings for saving and loading, use QuantizationSimModel's save_encodings_to_json() utility instead.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Step 3: Export Quantized Model\n",
    "os.makedirs('quantized', exist_ok=True)\n",
    "sim.export(path='quantized', filename_prefix='ref_solution_quantized', dummy_input=dummy_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreprocessedMobileNetV2(\n",
      "  (mobilenet_v2): MobileNetV2(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): QuantizedConv2d(\n",
      "          3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "        )\n",
      "        (1): Identity()\n",
      "        (2): QuantizedReLU(\n",
      "          (param_quantizers): ModuleDict()\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedConv2d(\n",
      "            32, 16, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): Identity()\n",
      "        )\n",
      "      )\n",
      "      (2): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              16, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            96, 24, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              24, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            144, 24, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              24, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            144, 32, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (5): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              32, 192, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            192, 32, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (6): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              32, 192, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            192, 32, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (7): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              32, 192, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            192, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (8): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (9): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (10): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (11): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            384, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (12): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              96, 576, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            576, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (13): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              96, 576, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            576, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (14): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              96, 576, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            576, 160, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (15): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              160, 960, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            960, 160, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (16): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              160, 960, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            960, 160, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (17): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              160, 960, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): Identity()\n",
      "            (2): QuantizedReLU(\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            960, 320, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (18): Conv2dNormActivation(\n",
      "        (0): QuantizedConv2d(\n",
      "          320, 1280, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "        )\n",
      "        (1): Identity()\n",
      "        (2): QuantizedReLU(\n",
      "          (param_quantizers): ModuleDict()\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): QuantizedDropout(\n",
      "        p=0.2, inplace=False\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (1): QuantizedLinear(\n",
      "        in_features=1280, out_features=64, bias=True\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(sim.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizedConv2d(\n",
       "  3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
       "  (param_quantizers): ModuleDict(\n",
       "    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
       "    (bias): None\n",
       "  )\n",
       "  (input_quantizers): ModuleList(\n",
       "    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
       "  )\n",
       "  (output_quantizers): ModuleList(\n",
       "    (0): None\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.model.mobilenet_v2.features[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DequantizedTensor([[-0.3103,  0.3103, -0.9828, -0.6724, -0.2069, -0.6207,\n",
       "                    -0.7241, -0.6724, -0.1034,  0.1552, -1.0345, -0.2069,\n",
       "                     0.1034,  0.1552, -0.8276, -0.5172, -0.3621,  0.0000,\n",
       "                     0.0517,  0.1552, -0.5690, -0.9310,  0.2586, -0.4655,\n",
       "                     0.0000,  0.3103, -0.5172, -0.2069, -0.5172,  0.2069,\n",
       "                     1.0345,  7.2932,  0.5690,  1.8621,  3.9828,  2.2242,\n",
       "                     1.3448,  1.8621, -0.3621,  0.7241, -1.5000,  1.3448,\n",
       "                     0.3621,  0.1034,  1.2414, -0.5690,  0.4138,  0.4655,\n",
       "                    -0.4655,  0.5172,  0.5172,  0.6207,  0.2069,  0.1034,\n",
       "                    -0.0517, -0.0517,  0.5172,  0.5690, -0.2069,  0.0000,\n",
       "                    -0.7241,  1.9655, -0.6207,  0.0000]],\n",
       "                  grad_fn=<AliasBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.model(mug_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 predictions for  on :\n",
      "31 Cup                   90.8%\n",
      "34 Bowl                   3.3%\n",
      "35 Banana                 0.6%\n",
      "61 Vase                   0.4%\n",
      "37 Orange                 0.4%\n"
     ]
    }
   ],
   "source": [
    "helper.print_probablities_from_output(sim.model(mug_image), dsutils.GLOBAL_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading reference_solution_quantized.aimet.zip\n",
      "2025-02-26 11:59:47,382 - root - INFO - Uploading asset to https://tetrahub-qprod-userdata.s3-accelerate.amazonaws.com/models/mn068z98n_AXSAzw8ZnA18ydUt.aimet.zip?uploadId=fJXpEztz2HWtchFIee37UgsAVKehi9yP1IfKf8abuWfTbiKowtDYLBTg3KyJpST2YbXUJ_VefJqYxc4C75vvJDCQuJw46Akafb4kx2j69RhkJkhEGZwOQcpiwVrhhVd_MM07tB2a3aDpQNzzt0jpvotsGPNFcxC9n4p6ztAFMJ8-&partNumber=1&A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[34m\u001b[0m| 3.46M/3.46M [00:02<00:00, 1.74MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 11:59:49,474 - root - INFO - Successfully uploaded asset with response status: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled compile job (jpyzv4erg) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/jpyzv4erg/\n",
      "\n",
      "Uploading reference_solution_quantized.aimet.zip\n",
      "2025-02-26 11:59:51,657 - root - INFO - Uploading asset to https://tetrahub-qprod-userdata.s3-accelerate.amazonaws.com/models/mqy1pzwrq_XouymYO04J5xUKB4.aimet.zip?uploadId=JXasc.YmZRzTPuPLzPBFB_rQFXP4GQ_rmIDvC4lUT8W3Frxvg1uo9FpUExpW825JSAuHWYjI7oroz8LUexUmeLeuPgAjmaiFanU6mMDs9h_oAY4r9mUoMnCj4Rm7wwHXMZame4eUfyzyXrSXy.4483av7hUrZqZMa_2_ZViMsyU-&partNumber=1&A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[34m\u001b[0m| 3.46M/3.46M [00:02<00:00, 1.73MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 11:59:53,753 - root - INFO - Successfully uploaded asset with response status: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled compile job (jp04e1y2g) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/jp04e1y2g/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compile to TensorFlow Lite\n",
    "\n",
    "import qai_hub as hub\n",
    "\n",
    "compile_job = hub.submit_compile_job(\n",
    "    model=\"reference_solution_quantized.aimet\",\n",
    "    device=hub.Device(\"Samsung Galaxy S24 (Family)\"),\n",
    ")\n",
    "assert isinstance(compile_job, hub.CompileJob)\n",
    "\n",
    "# Compile to a QNN Model Library\n",
    "compile_job_qnn = hub.submit_compile_job(\n",
    "    model=\"reference_solution_quantized.aimet\",\n",
    "    device=hub.Device(\"Samsung Galaxy S24 (Family)\"),\n",
    "    options=\"--target_runtime qnn_lib_aarch64_android --quantize_full_type int8\",\n",
    ")\n",
    "assert isinstance(compile_job_qnn, hub.CompileJob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled profile job (jgkvrlzyp) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/jgkvrlzyp/\n",
      "\n",
      "Scheduled profile job (j5q09787p) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/j5q09787p/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile_job     = hub.submit_profile_job(compile_job.get_target_model(), device=hub.Device(\"Snapdragon 8 Elite QRD\"),)\n",
    "\n",
    "profile_job_qnn = hub.submit_profile_job(compile_job_qnn.get_target_model(), device=hub.Device(\"Snapdragon 8 Elite QRD\"),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading tmp4pf1vaxn.pt\n",
      "2025-02-26 12:14:01,797 - root - INFO - Uploading asset to https://tetrahub-qprod-userdata.s3-accelerate.amazonaws.com/models/mqe8lrgvq_UbyJbnu52Lbvwv7x.pt?uploadId=vl73_sWooq3J7zrf68Q1NxOpTm6qtNuAOeLIMVkyu9se3C4ZAHlbpQTyR9CfoDZwANt90PkcbCcrYgac1kBc0EwiVfaokgYqisTJEcHQjb1BMy_E7kMFaC6l46oL81.B30tuPCiJmErW0KLGVt6SVvOw8dEmHVW_41917Zn.R0E-&partNumber=1&AWSAcces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[34m\u001b[0m| 9.37M/9.37M [00:02<00:00, 3.99MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 12:14:04,260 - root - INFO - Successfully uploaded asset with response status: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled compile job (j5mevw2qp) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/j5mevw2qp/\n",
      "\n",
      "Uploading tmpbi1vprnt.pt\n",
      "2025-02-26 12:14:06,315 - root - INFO - Uploading asset to https://tetrahub-qprod-userdata.s3-accelerate.amazonaws.com/models/mnzok56dm_gzFsP1EzMDDg71Uw.pt?uploadId=QeX9kBCpvLj9ly_sdvRtzX90qnF0qa.K0QxwTFfqhmfOsB.RJW0UCzUOsQlOC.3Uy.wRIxmjgb_bBY4ygi7fS3sNlyXyQgvGEMXPbawYvmcJgi_ffuLhcSc39Yod6f3HJe2VedEz5OgJXm00FhPhRrVhPjTAStHscvmctLlPrdc-&partNumber=1&AWSAcces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[34m\u001b[0m| 9.37M/9.37M [00:01<00:00, 5.59MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 12:14:08,075 - root - INFO - Successfully uploaded asset with response status: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled compile job (jgn0r9ym5) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/jgn0r9ym5/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_traced_original = torch.jit.trace(modelOriginal, dummy_input)\n",
    "\n",
    "\n",
    "\n",
    "compile_job_original = hub.submit_compile_job(\n",
    "    model=model_traced_original,\n",
    "    device=hub.Device(\"Samsung Galaxy S24 (Family)\"),\n",
    "    input_specs=dict(image=input_shape)\n",
    ")\n",
    "assert isinstance(compile_job_original, hub.CompileJob)\n",
    "\n",
    "# Compile to a QNN Model Library\n",
    "compile_job_original_qnn = hub.submit_compile_job(\n",
    "    model=model_traced_original,\n",
    "    device=hub.Device(\"Samsung Galaxy S24 (Family)\"),\n",
    "    input_specs=dict(image=input_shape),\n",
    "    options=\"--target_runtime qnn_lib_aarch64_android\",\n",
    ")\n",
    "assert isinstance(compile_job_original_qnn, hub.CompileJob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled profile job (jp2x376mg) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/jp2x376mg/\n",
      "\n",
      "Scheduled profile job (jpyzv4w4g) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/jpyzv4w4g/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile_job_original     = hub.submit_profile_job(compile_job_original.get_target_model(), device=hub.Device(\"Snapdragon 8 Elite QRD\"),)\n",
    "\n",
    "profile_job_original_qnn = hub.submit_profile_job(compile_job_original_qnn.get_target_model(), device=hub.Device(\"Snapdragon 8 Elite QRD\"),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved profile results to /home/centar15-desktop1/LPCV_2025_T1/src/reference_solution/profile_results/job_jpyzv4erg_optimized_tflite_jgkvrlzyp_results_12.23.47.701037.json\n",
      "Saved profile results to /home/centar15-desktop1/LPCV_2025_T1/src/reference_solution/profile_results/job_jp04e1y2g_optimized_so_j5q09787p_results.json\n"
     ]
    }
   ],
   "source": [
    "quanTFLITE = profile_job.download_results('profile_results')\n",
    "quanQNN    = profile_job_qnn.download_results('profile_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProfileJobResult\n",
       "----------------\n",
       "status                        : JobStatus\n",
       "---------\n",
       "code    : SUCCESS\n",
       "message : \n",
       "\n",
       "url                           : https://app.aihub.qualcomm.com/jobs/jgkvrlzyp/\n",
       "artifacts_dir                 : /home/centar15-desktop1/LPCV_2025_T1/src/reference_solution/profile_results\n",
       "Estimated Inference Time (ms) : 0.269\n",
       "Load Time (ms)                : 235.482\n",
       "Peak Memory (MB)              : 129.34375\n",
       "Compute Units (layers)        : NPU: 74"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quanTFLITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProfileJobResult\n",
       "----------------\n",
       "status                        : JobStatus\n",
       "---------\n",
       "code    : SUCCESS\n",
       "message : \n",
       "\n",
       "url                           : https://app.aihub.qualcomm.com/jobs/j5q09787p/\n",
       "artifacts_dir                 : /home/centar15-desktop1/LPCV_2025_T1/src/reference_solution/profile_results\n",
       "Estimated Inference Time (ms) : 0.206\n",
       "Load Time (ms)                : 201.704\n",
       "Peak Memory (MB)              : 179.52734375\n",
       "Compute Units (layers)        : NPU: 70"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quanQNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved profile results to /home/centar15-desktop1/LPCV_2025_T1/src/reference_solution/profile_results/job_j5mevw2qp_optimized_tflite_jp2x376mg_results.json\n",
      "Saved profile results to /home/centar15-desktop1/LPCV_2025_T1/src/reference_solution/profile_results/job_jgn0r9ym5_optimized_so_jpyzv4w4g_results.json\n"
     ]
    }
   ],
   "source": [
    "quanTFLITE_original = profile_job_original.download_results('profile_results')\n",
    "quanQNN_original    = profile_job_original_qnn.download_results('profile_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProfileJobResult\n",
       "----------------\n",
       "status                        : JobStatus\n",
       "---------\n",
       "code    : SUCCESS\n",
       "message : \n",
       "\n",
       "url                           : https://app.aihub.qualcomm.com/jobs/jp2x376mg/\n",
       "artifacts_dir                 : /home/centar15-desktop1/LPCV_2025_T1/src/reference_solution/profile_results\n",
       "Estimated Inference Time (ms) : 0.39\n",
       "Load Time (ms)                : 365.974\n",
       "Peak Memory (MB)              : 230.34375\n",
       "Compute Units (layers)        : NPU: 70"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quanTFLITE_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProfileJobResult\n",
       "----------------\n",
       "status                        : JobStatus\n",
       "---------\n",
       "code    : SUCCESS\n",
       "message : \n",
       "\n",
       "url                           : https://app.aihub.qualcomm.com/jobs/jpyzv4w4g/\n",
       "artifacts_dir                 : /home/centar15-desktop1/LPCV_2025_T1/src/reference_solution/profile_results\n",
       "Estimated Inference Time (ms) : 0.394\n",
       "Load Time (ms)                : 321.29\n",
       "Peak Memory (MB)              : 176.10546875\n",
       "Compute Units (layers)        : NPU: 103"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quanQNN_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original  model accuracy =  0.3803228398217807\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "validationDataloader = DataLoader(datasetCOCO, batch_size = 64, shuffle = False)\n",
    "with torch.no_grad():\n",
    "    print(\"Original  model accuracy = \", helper.check_accuracy(modelOriginal.cuda(), validationDataloader, device))\n",
    "    # print(\"Quantized model accuracy = \", helper.check_accuracy(sim.model.cuda(), validationDataloader, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model accuracy =  0.24775399897743042\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    #print(\"Original  model accuracy = \", helper.check_accuracy(modelOriginal.cuda(), validationDataloader, device))\n",
    "    print(\"Quantized model accuracy = \", helper.check_accuracy(sim.model.cuda(), validationDataloader, device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
