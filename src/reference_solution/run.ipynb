{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-18 12:46:03,916 - root - INFO - AIMET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\venv\\Lib\\site-packages\\onnxscript\\converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "<frozen abc>:106: FutureWarning: `NLLLoss2d` has been deprecated. Please use `NLLLoss` instead as a drop-in replacement and see https://pytorch.org/docs/main/nn.html#torch.nn.NLLLoss for more details.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import MobileNetV2Pretrained\n",
    "import sys\n",
    "import os\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(ROOT_DIR)\n",
    "import utils.helper as helper\n",
    "import utils.qai_hub_jobs as qai_hub_jobs\n",
    "import utils.tfhelper as tfhelper\n",
    "import qai_hub as hub\n",
    "import aimet_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = MobileNetV2Pretrained.PreprocessedMobileNetV2(64, \"model/mobilenet_v2_coco.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model.eval()\n",
    "input_shape: tuple[int, ...] = (1, 3, 224, 224)\n",
    "example_input = torch.rand(input_shape)\n",
    "pt_model = torch.jit.trace(pretrained_model, example_input)\n",
    "\n",
    "# # Compile model on a specific device\n",
    "# compile_job = hub.submit_compile_job(\n",
    "#     pt_model,\n",
    "#     name=\"reference_model\", # Replace with your model name\n",
    "#     device=hub.Device(\"Snapdragon 8 Elite QRD\"),\n",
    "#     input_specs=dict(image=input_shape),\n",
    "# )\n",
    "\n",
    "# compiled_model = compile_job.get_target_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled profile job (jp4nv88vg) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/jp4nv88vg/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile_job = qai_hub_jobs.profile_job(compiled_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_job.download_results(\"profile_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-18 13:35:25,747 - Quant - INFO - Unsupported op type Squeeze\n",
      "2025-02-18 13:35:25,748 - Quant - INFO - Unsupported op type Mean\n",
      "2025-02-18 13:35:25,754 - Quant - INFO - Selecting DefaultOpInstanceConfigGenerator to compute the specialized config. hw_version:default\n",
      "-------------------------\n",
      "Quantized Model Report\n",
      "-------------------------\n",
      "PreprocessedMobileNetV2(\n",
      "  (mobilenet_v2): MobileNetV2(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): QuantizedConv2d(\n",
      "          3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(32, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "        )\n",
      "        (1): QuantizedBatchNorm2d(\n",
      "          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): None\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "        )\n",
      "        (2): QuantizedReLU6(\n",
      "          inplace=True\n",
      "          (param_quantizers): ModuleDict()\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(32, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedConv2d(\n",
      "            32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(16, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedBatchNorm2d(\n",
      "            16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(96, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(96, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(24, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(144, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(144, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(24, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(144, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(144, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(32, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(192, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(192, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(32, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(192, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(192, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(32, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(192, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(192, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(64, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(384, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(384, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(64, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(384, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(384, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(64, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(384, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(384, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(64, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(384, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(384, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(96, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (12): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(576, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(576, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(96, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(576, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(576, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(96, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (14): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(576, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(576, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(160, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (15): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(960, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(960, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(160, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (16): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(960, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(960, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(160, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (17): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(960, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(960, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(320, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (18): Conv2dNormActivation(\n",
      "        (0): QuantizedConv2d(\n",
      "          320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(1280, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "        )\n",
      "        (1): QuantizedBatchNorm2d(\n",
      "          1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): None\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "        )\n",
      "        (2): QuantizedReLU6(\n",
      "          inplace=True\n",
      "          (param_quantizers): ModuleDict()\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): QuantizedDropout(\n",
      "        p=0.2, inplace=False\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (1): QuantizedLinear(\n",
      "        in_features=1280, out_features=64, bias=True\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from aimet_common.defs import QuantScheme\n",
    "from aimet_common.quantsim_config.utils import get_path_for_per_channel_config\n",
    "from aimet_torch.quantsim import QuantizationSimModel\n",
    "\n",
    "sim = QuantizationSimModel(pretrained_model, \n",
    "                           example_input,\n",
    "                           quant_scheme=QuantScheme.post_training_tf,\n",
    "                           config_file=get_path_for_per_channel_config(),\n",
    "                           default_param_bw=8,\n",
    "                           default_output_bw=8)\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-18 13:35:35,519 - Quant - INFO - Unsupported op type Squeeze\n",
      "2025-02-18 13:35:35,520 - Quant - INFO - Unsupported op type Mean\n",
      "2025-02-18 13:35:35,526 - Quant - INFO - Selecting DefaultOpInstanceConfigGenerator to compute the specialized config. hw_version:default\n",
      "-------------------------\n",
      "Quantized Model Report\n",
      "-------------------------\n",
      "PreprocessedMobileNetV2(\n",
      "  (mobilenet_v2): MobileNetV2(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): QuantizedConv2d(\n",
      "          3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(32, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "        )\n",
      "        (1): QuantizedBatchNorm2d(\n",
      "          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): None\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "        )\n",
      "        (2): QuantizedReLU6(\n",
      "          inplace=True\n",
      "          (param_quantizers): ModuleDict()\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(32, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedConv2d(\n",
      "            32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(16, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedBatchNorm2d(\n",
      "            16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(96, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(96, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(24, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(144, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(144, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(24, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(144, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(144, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(32, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(192, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(192, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(32, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(192, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(192, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(32, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(192, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(192, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(64, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(384, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(384, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(64, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(384, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(384, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(64, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(384, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(384, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(64, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(384, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(384, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(96, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (12): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(576, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(576, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(96, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(576, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(576, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(96, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (14): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(576, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(576, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(160, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (15): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(960, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(960, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(160, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (16): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(960, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(960, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(160, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (17): InvertedResidual(\n",
      "        (conv): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(960, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): QuantizedConv2d(\n",
      "              960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(960, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedReLU6(\n",
      "              inplace=True\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedConv2d(\n",
      "            960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(320, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (3): QuantizedBatchNorm2d(\n",
      "            320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (18): Conv2dNormActivation(\n",
      "        (0): QuantizedConv2d(\n",
      "          320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(1280, 1, 1, 1), qmin=-128, qmax=127, symmetric=True)\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "        )\n",
      "        (1): QuantizedBatchNorm2d(\n",
      "          1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): None\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "        )\n",
      "        (2): QuantizedReLU6(\n",
      "          inplace=True\n",
      "          (param_quantizers): ModuleDict()\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): QuantizedDropout(\n",
      "        p=0.2, inplace=False\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (1): QuantizedLinear(\n",
      "        in_features=1280, out_features=64, bias=True\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sim = QuantizationSimModel(pretrained_model, \n",
    "                           example_input,\n",
    "                           quant_scheme=QuantScheme.training_range_learning_with_tf_init,\n",
    "                           config_file=get_path_for_per_channel_config(),\n",
    "                           default_param_bw=8,\n",
    "                           default_output_bw=8)\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run QuantizeDequantize since quantization parameters are not initialized. Please initialize the quantization parameters using `compute_encodings()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\src\\reference_solution\\MobileNetV2Pretrained.py:33\u001b[0m, in \u001b[0;36mPreprocessedMobileNetV2.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     30\u001b[0m     img_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(img)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Pass the preprocessed image through the model\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmobilenet_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\venv\\Lib\\site-packages\\torchvision\\models\\mobilenetv2.py:174\u001b[0m, in \u001b[0;36mMobileNetV2.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\venv\\Lib\\site-packages\\torchvision\\models\\mobilenetv2.py:166\u001b[0m, in \u001b[0;36mMobileNetV2._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# This exists since TorchScript doesn't support inheritance, so the superclass method\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# (this one) needs to have a name other than `forward` that can be accessed in a subclass\u001b[39;00m\n\u001b[1;32m--> 166\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Cannot use \"squeeze\" as batch-size can be 1\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39madaptive_avg_pool2d(x, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\venv\\Lib\\site-packages\\aimet_torch\\v2\\nn\\base.py:107\u001b[0m, in \u001b[0;36mBaseQuantizationMixin.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_param_encodings(overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\venv\\Lib\\site-packages\\aimet_torch\\v2\\nn\\true_quant.py:473\u001b[0m, in \u001b[0;36m_DispatchMixin.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_patch_quantized_parameters():\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _dispatch(builtin_torch_fn, kernel):\n\u001b[1;32m--> 473\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _dequantize_if_applicable(output)\n",
      "File \u001b[1;32mc:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\venv\\Lib\\site-packages\\aimet_torch\\v2\\nn\\true_quant.py:190\u001b[0m, in \u001b[0;36mQuantizationMixin.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;129m@abstractmethod\u001b[39m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    178\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Computes a quantized version of the parent module's forward method.\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \n\u001b[0;32m    180\u001b[0m \u001b[38;5;124;03m    The :meth:`forward` method should perform the following logic in order:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;124;03m    its parent module's forward pass.\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\venv\\Lib\\site-packages\\aimet_torch\\v2\\nn\\base.py:117\u001b[0m, in \u001b[0;36mBaseQuantizationMixin.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;129m@abc\u001b[39m\u001b[38;5;241m.\u001b[39mabstractmethod\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    111\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward function for quantized module.\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    This method will replace the original forward function of the base :class:`nn.Module` class and is\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03m    responsible for computing a quantized version of the base class' forward function using the configuration of\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;124;03m    the layer's :class:`QuantizerBase` objects.\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\venv\\Lib\\site-packages\\aimet_torch\\v2\\nn\\true_quant.py:425\u001b[0m, in \u001b[0;36m_Dispatcher.__torch_function__\u001b[1;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    423\u001b[0m     impl \u001b[38;5;241m=\u001b[39m func\n\u001b[1;32m--> 425\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\venv\\Lib\\site-packages\\torch\\overrides.py:2086\u001b[0m, in \u001b[0;36mBaseTorchFunctionMode.__torch_function__\u001b[1;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m   2084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2085\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 2086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\venv\\Lib\\site-packages\\aimet_torch\\v2\\nn\\true_quant.py:492\u001b[0m, in \u001b[0;36m_DispatchMixin._builtin_torch_fn_helper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    483\u001b[0m others \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    484\u001b[0m     _dequantize_if_applicable(x)\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m args[\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_quantizers):]\n\u001b[0;32m    486\u001b[0m )\n\u001b[0;32m    487\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    488\u001b[0m     key: _dequantize_if_applicable(value)\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    490\u001b[0m }\n\u001b[1;32m--> 492\u001b[0m output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39mqtzd_args, \u001b[38;5;241m*\u001b[39mothers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _quantize_dequantize_if_applicable(output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_quantizers[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\venv\\Lib\\site-packages\\aimet_torch\\v2\\nn\\true_quant.py:480\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    479\u001b[0m     qtzd_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 480\u001b[0m         \u001b[43m_quantize_dequantize_if_applicable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqtzr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    481\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x, qtzr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_quantizers)\n\u001b[0;32m    482\u001b[0m     )\n\u001b[0;32m    483\u001b[0m     others \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    484\u001b[0m         _dequantize_if_applicable(x)\n\u001b[0;32m    485\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m args[\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_quantizers):]\n\u001b[0;32m    486\u001b[0m     )\n\u001b[0;32m    487\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    488\u001b[0m         key: _dequantize_if_applicable(value)\n\u001b[0;32m    489\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    490\u001b[0m     }\n",
      "File \u001b[1;32mc:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\venv\\Lib\\site-packages\\aimet_torch\\v2\\nn\\true_quant.py:95\u001b[0m, in \u001b[0;36m_quantize_dequantize_if_applicable\u001b[1;34m(data, quantizer)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, QuantizedTensorBase):\n\u001b[0;32m     94\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdequantize()\n\u001b[1;32m---> 95\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mquantizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, QuantizedTensorBase):\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\u001b[38;5;241m.\u001b[39mdequantize()\n",
      "File \u001b[1;32mc:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Pro\\Documents\\___FAKULTET___\\LPCV\\LPCV_2025_T1\\LPCV_2025_T1\\venv\\Lib\\site-packages\\aimet_torch\\v2\\quantization\\affine\\quantizer.py:687\u001b[0m, in \u001b[0;36mQuantizeDequantize.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Quantizes and dequantizes the input tensor\u001b[39;00m\n\u001b[0;32m    678\u001b[0m \n\u001b[0;32m    679\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    684\u001b[0m \n\u001b[0;32m    685\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_initialized():\n\u001b[1;32m--> 687\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    688\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to run QuantizeDequantize since quantization parameters are not initialized.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    689\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Please initialize the quantization parameters using `compute_encodings()`.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    690\u001b[0m     )\n\u001b[0;32m    692\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_encodings()\n\u001b[0;32m    694\u001b[0m \u001b[38;5;66;03m# Subclasses of torch.Tensor with custom __torch_function__ (in our case, QuantizedTensorBase)\u001b[39;00m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;66;03m# is known to introduce substantial CPU overhead.\u001b[39;00m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;66;03m# Cast types of the inputs to plain torch.Tensor for faster execution.\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to run QuantizeDequantize since quantization parameters are not initialized. Please initialize the quantization parameters using `compute_encodings()`."
     ]
    }
   ],
   "source": [
    "# sim.model(torch.rand(input_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model):\n",
    "    with torch.no_grad():\n",
    "        model(torch.randn((1, 3, 224, 224)))\n",
    "\n",
    "sim.compute_encodings(forward_pass)\n",
    "\n",
    "# output = sim.model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DequantizedTensor([[ 0.1940, -0.4850, -0.6628, -0.3071, -1.0184, -0.7759,\n",
       "                    -0.0808,  0.7274,  1.2286,  0.5011, -0.5011, -0.4365,\n",
       "                     2.0530, -0.3395, -0.1778, -0.4203,  0.6951, -0.5011,\n",
       "                    -0.2263, -0.2586,  0.3718,  0.0162,  0.0485,  0.0970,\n",
       "                     0.1455,  0.7759,  0.1940,  3.0714, -0.0162,  0.4526,\n",
       "                     0.7436, -0.2425,  0.0485,  0.3556, -0.5011,  0.2748,\n",
       "                     0.0970,  0.4041, -0.9538, -0.5335, -0.8406, -0.7274,\n",
       "                     0.1455, -0.2263, -0.2910, -0.9861, -0.4203,  0.0323,\n",
       "                     0.0647,  0.2101,  0.3071,  0.5496,  0.2586,  0.3556,\n",
       "                    -0.3718, -0.3718,  0.0485, -0.5011,  0.4041, -0.6304,\n",
       "                     1.7620,  0.3071,  0.0323,  0.0808]],\n",
       "                  grad_fn=<AliasBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.model(torch.rand(input_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in sim.model.parameters():\n",
    "#     print(type(param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-18 13:37:47,655 - Utils - INFO - successfully created onnx model with 140/222 node names updated\n",
      "2025-02-18 13:37:47,752 - Quant - WARNING - The following layers were not found in the exported onnx model. Encodings for these layers will not appear in the exported encodings file, however it will continue to exist in torch encoding file:\n",
      "['mobilenet_v2.classifier.0']\n",
      "This can be due to several reasons:\n",
      "\t- The layer is set to quantize with float datatype, but was not exercised in compute encodings. Not an issue if the layer is not meant to be run.\n",
      "\t- The layer has valid encodings but was not seen while exporting to onnx using the dummy input provided in sim.export(). Ensure that the dummy input covers all layers.\n",
      "2025-02-18 13:37:47,752 - Quant - INFO - Layers excluded from quantization: []\n",
      "2025-02-18 13:37:47,754 - Quant - WARNING - \u001b[31;21mQuantsim export will stop exporting encodings for saving and loading in a future AIMET release.\n",
      "To export encodings for saving and loading, use QuantizationSimModel's save_encodings_to_json() utility instead.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"model/quantized\", exist_ok=True)\n",
    "sim.export(path = \"model/quantized\", filename_prefix = \"mobilenet_v2_coco_quantized\", dummy_input=example_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading quantized.aimet.zip\n",
      "2025-02-18 12:58:33,859 - root - INFO - Uploading asset to https://tetrahub-qprod-userdata.s3-accelerate.amazonaws.com/models/mq9wzp70m_Y2H3Zu7belpwK0Vu.aimet.zip?uploadId=U4fgA.2_5fOWMIyzvqPpOiuuKYP0g.lF0oTZwwNRhu5CVlcIgGqtDIK0uyKuWam11c2v9IDmeIZXLVNkyidolcwNEJxwZvPmSwPi37gxsDTfGhYSK0H0oASdcd8bZKtO6m5aYno6l1yTVbP27nUbIrzyn2SSxAHk4HFwKIy2I3I-&partNumber=1&A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[34m██████████\u001b[0m| 5.08M/5.08M [00:01<00:00, 3.62MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-18 12:58:35,335 - root - INFO - Successfully uploaded asset with response status: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled compile job (jp12e332g) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/jp12e332g/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cj = hub.submit_compile_job(\n",
    "    model = \"model/quantized/quantized.aimet\",\n",
    "    name=\"quantized_model\", # Replace with your model name\n",
    "    device=hub.Device(\"Snapdragon 8 Elite QRD\"),\n",
    "    options=\"--target_runtime qnn_lib_aarch64_android --quantize_full_type int8\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled profile job (jgn03xxr5) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/jgn03xxr5/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pf = qai_hub_jobs.profile_job(cj.get_target_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset.utils as dsutils\n",
    "import utils.input_getter as ig\n",
    "\n",
    "download_path = tfhelper.download_model_from_compile_job(compile, 'ref_model')\n",
    "TFModel = tfhelper.TFHelper(download_path)\n",
    "\n",
    "input = ig.mug_image_getter()\n",
    "\n",
    "helper.print_probablities_from_output(TFModel.run_inference(input.get_input_numpy()), categories=dsutils.GLOBAL_CLASSES, modelname = 'TensorFlow')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
