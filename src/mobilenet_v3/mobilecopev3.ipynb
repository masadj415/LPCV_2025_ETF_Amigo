{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-28 17:53:14,531 - root - INFO - AIMET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/lib/python3.10/abc.py:106: FutureWarning: `NLLLoss2d` has been deprecated. Please use `NLLLoss` instead as a drop-in replacement and see https://pytorch.org/docs/main/nn.html#torch.nn.NLLLoss for more details.\n",
      "  cls = super().__new__(mcls, name, bases, namespace, **kwargs)\n",
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from aimet_torch.cross_layer_equalization import equalize_model\n",
    "from torchvision import models\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import torch.optim as optim\n",
    "\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "from utils.input_getter import mug_image_getter\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningModel(pl.LightningModule):\n",
    "    def __init__(self, num_classes=64, learning_rate=2e-5):\n",
    "        super(LightningModel, self).__init__()\n",
    "        self.model = models.mobilenet_v3_small(pretrained=True)\n",
    "        self.model.classifier[3] = nn.Linear(self.model.classifier[3].in_features, num_classes)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        images, labels = images.to(self.device), labels.to(self.device)\n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "modelOriginal   = LightningModel.load_from_checkpoint(\"models/mobilenet_v3-epoch=192-train_loss=0.01.ckpt\")\n",
    "modelOriginal   = modelOriginal.model\n",
    "\n",
    "import torch.nn as nn\n",
    "import torchvision.ops as ops\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class NormalizedV3(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, mobileNetV3):\n",
    "        super(NormalizedV3, self).__init__()\n",
    "        self.model = mobileNetV3\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    def forward(self, img):\n",
    "        # Pass the preprocessed image through the model\n",
    "        return self.model(self.preprocess(img))\n",
    "\n",
    "model = NormalizedV3(modelOriginal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total annotations before filtering: 36781\n",
      "Total annotations after filtering: 8862\n",
      "Total COCO annotations: 36781\n",
      "Filtered annotations: 8862\n",
      "Slika: 0\n",
      "Slika: 1000\n",
      "Slika: 2000\n",
      "Slika: 3000\n",
      "Slika: 4000\n",
      "Slika: 5000\n",
      "Slika: 6000\n",
      "Slika: 7000\n",
      "Slika: 8000\n",
      "Accuracy: 0.791582035657865\n"
     ]
    }
   ],
   "source": [
    "totalPost = 0\n",
    "accuracyPost = 0\n",
    "\n",
    "from dataset import DatasetReader\n",
    "from dataset import utils\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "dataset_coco_val = DatasetReader.COCODataset(annotation_file='../../datasets/coco/annotations/instances_val2017.json',\n",
    "    image_dir= '../../datasets/coco/val2017',\n",
    "    target_classes=[s.lower() for s in utils.GLOBAL_CLASSES],\n",
    "    transform=transforms.Compose([\n",
    "            transforms.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "            transforms.ToTensor()\n",
    "        ]),\n",
    "    min_size = 60)\n",
    "\n",
    "\n",
    "# batch_size = 256\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=11, prefetch_factor=4, persistent_workers=True)\n",
    "\n",
    "# Ovo sigurno moze da se optimizuje nekako\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for i in range(0, len(dataset_coco_val)):\n",
    "    totalPost += 1\n",
    "    image, label, _ = dataset_coco_val[i]\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    label = torch.tensor([label]).to(device)\n",
    "    output = model(image)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    # print(f\"Predicted: {predicted.item()}, Actual: {label.item()}\")\n",
    "    if(i % 1000 == 0):\n",
    "        print(f\"Slika: {i}\")\n",
    "    if predicted.item() == label.item():\n",
    "        accuracyPost += 1\n",
    "    \n",
    "\n",
    "print(f\"Accuracy: {accuracyPost/totalPost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading tmpdhz4tivm.pt\n",
      "2025-02-28 18:11:16,508 - root - INFO - Uploading asset to https://tetrahub-qprod-userdata.s3-accelerate.amazonaws.com/models/mngry5keq_A6k1VQQETUVBqjhG.pt?uploadId=5nCHYpprJnFNNsvqPFeuMJ.cdehxMd0Pm3iRju.WGwZ2cDmUSVSCtUTWaQVBQVzO_Z_uappbaeZmVyflq67Ql6xwjpD4ktS8RBvT.Lg0kx0m71CgN..OueNOlnUcx1HMCGRWpllM_.cY7FEEIl9dpYiyx4b7YhohxRT41WsJlak-&partNumber=1&AWSAcces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[34m██████████\u001b[0m| 6.51M/6.51M [00:02<00:00, 3.06MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-28 18:11:18,739 - root - INFO - Successfully uploaded asset with response status: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled compile job (jpeo82q7g) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/jpeo82q7g/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import qai_hub\n",
    "\n",
    "input_shape = (1, 3, 224, 224)\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "pt_model = torch.jit.trace(model.cpu().eval(), dummy_input)\n",
    "\n",
    "compile_job = qai_hub.submit_compile_job(\n",
    "    pt_model,\n",
    "    name=\"MobileNetV3FineTuned\", # Replace with your model name\n",
    "    device=qai_hub.Device(\"Samsung Galaxy S24 (Family)\"),\n",
    "    input_specs=dict(image=input_shape),\n",
    ")\n",
    "\n",
    "compile_job.modify_sharing(add_emails=['lowpowervision@gmail.com']) ## Share your model for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled profile job (jp12739kg) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/jp12739kg/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile_job = qai_hub.submit_profile_job(compile_job.get_target_model(), device=qai_hub.Device(\"Samsung Galaxy S24 (Family)\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
