{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-28 18:31:26,819 - root - INFO - AIMET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-jet/LPCV_2025_T1/.venv3.10/lib/python3.10/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/lib/python3.10/abc.py:106: FutureWarning: `NLLLoss2d` has been deprecated. Please use `NLLLoss` instead as a drop-in replacement and see https://pytorch.org/docs/main/nn.html#torch.nn.NLLLoss for more details.\n",
      "  cls = super().__new__(mcls, name, bases, namespace, **kwargs)\n",
      "/home/centar15-jet/LPCV_2025_T1/.venv3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from aimet_torch.cross_layer_equalization import equalize_model\n",
    "from torchvision import models\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import torch.optim as optim\n",
    "\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "from utils.input_getter import mug_image_getter\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningModel(pl.LightningModule):\n",
    "    def __init__(self, num_classes=64, learning_rate=2e-5):\n",
    "        super(LightningModel, self).__init__()\n",
    "        self.model = models.mobilenet_v3_small(pretrained=True)\n",
    "        self.model.classifier[3] = nn.Linear(self.model.classifier[3].in_features, num_classes)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        images, labels = images.to(self.device), labels.to(self.device)\n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-jet/LPCV_2025_T1/.venv3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/centar15-jet/LPCV_2025_T1/.venv3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV3(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
      "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
      "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): Conv2dNormActivation(\n",
      "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=576, out_features=1024, bias=True)\n",
      "    (1): Hardswish()\n",
      "    (2): Dropout(p=0.2, inplace=True)\n",
      "    (3): Linear(in_features=1024, out_features=64, bias=True)\n",
      "  )\n",
      ")\n",
      "MobileNetV3(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
      "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
      "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): Conv2dNormActivation(\n",
      "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=576, out_features=1024, bias=True)\n",
      "    (1): Hardswish()\n",
      "    (2): Dropout(p=0.2, inplace=True)\n",
      "    (3): Linear(in_features=1024, out_features=64, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "image_getter = mug_image_getter()\n",
    "mug_image = image_getter.get_input_torch()\n",
    "print(mug_image.shape)\n",
    "\n",
    "modelOriginal   = LightningModel.load_from_checkpoint(\"models/mobilenet_v3-epoch=192-train_loss=0.01.ckpt\")\n",
    "modelOriginal   = modelOriginal.model\n",
    "modelQuantized  = LightningModel.load_from_checkpoint(\"models/mobilenet_v3-epoch=192-train_loss=0.01.ckpt\")\n",
    "modelQuantized  = modelQuantized.model\n",
    "\n",
    "\n",
    "modelOriginal.eval()\n",
    "modelQuantized.eval()\n",
    "\n",
    "print(modelQuantized)\n",
    "print(modelOriginal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 predictions for  on :\n",
      "31 Cup                  100.0%\n",
      "29 Bottle                 0.0%\n",
      "34 Bowl                   0.0%\n",
      "46 Dining Table           0.0%\n",
      "51 Remote                 0.0%\n",
      "Top-5 predictions for  on :\n",
      "31 Cup                  100.0%\n",
      "29 Bottle                 0.0%\n",
      "34 Bowl                   0.0%\n",
      "46 Dining Table           0.0%\n",
      "51 Remote                 0.0%\n"
     ]
    }
   ],
   "source": [
    "from utils import helper\n",
    "from dataset import utils as dsutils\n",
    "\n",
    "helper.print_probablities_from_output(modelOriginal(mug_image.cuda()), dsutils.GLOBAL_CLASSES)\n",
    "helper.print_probablities_from_output(modelQuantized(mug_image.cuda()), dsutils.GLOBAL_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV3(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
      "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
      "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): Conv2dNormActivation(\n",
      "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=576, out_features=1024, bias=True)\n",
      "    (1): Hardswish()\n",
      "    (2): Dropout(p=0.2, inplace=True)\n",
      "    (3): Linear(in_features=1024, out_features=64, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#proba da  li radi \n",
    "param_num = 0\n",
    "\n",
    "input_shape = (1, 3, 224, 224)\n",
    "dummy_input = torch.randn(input_shape)\n",
    "\n",
    "# print(\"CLE Started\")\n",
    "\n",
    "# equalize_model(modelQuantized, dummy_input = dummy_input)\n",
    "\n",
    "# print(\"Cross-Layer Equalization applied successfully!\")\n",
    "\n",
    "print(modelQuantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "has_bn=any(isinstance(layer,nn.BatchNorm2d) for layer in modelQuantized.modules())\n",
    "print(has_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 predictions for  on :\n",
      "31 Cup                  100.0%\n",
      "29 Bottle                 0.0%\n",
      "34 Bowl                   0.0%\n",
      "46 Dining Table           0.0%\n",
      "51 Remote                 0.0%\n",
      "Top-5 predictions for  on :\n",
      "31 Cup                  100.0%\n",
      "29 Bottle                 0.0%\n",
      "34 Bowl                   0.0%\n",
      "46 Dining Table           0.0%\n",
      "51 Remote                 0.0%\n"
     ]
    }
   ],
   "source": [
    "helper.print_probablities_from_output(modelOriginal(mug_image.cuda()), dsutils.GLOBAL_CLASSES)\n",
    "helper.print_probablities_from_output(modelQuantized(mug_image.cuda()), dsutils.GLOBAL_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix posle nemam pojma kako radi\n",
    "\n",
    "# from aimet_torch.bias_correction import correct_bias\n",
    "# import torch\n",
    "\n",
    "# # Define your model (example: ResNet18)\n",
    "# model = models.resnet18(pretrained=True)\n",
    "\n",
    "# # Define a dummy dataset (batch size = 1, shape = [1, 3, 224, 224])\n",
    "# dummy_input = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "# # Apply Bias Correction\n",
    "# correct_bias(model)\n",
    "\n",
    "# print(\"Bias Correction applied successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-28 18:31:30,834 - Quant - INFO - No config file provided, defaulting to config file at /home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/aimet_common/quantsim_config/default_config.json\n",
      "2025-02-28 18:31:30,855 - Quant - INFO - Unsupported op type Squeeze\n",
      "2025-02-28 18:31:30,855 - Quant - INFO - Unsupported op type Mean\n",
      "2025-02-28 18:31:30,863 - Quant - INFO - Selecting DefaultOpInstanceConfigGenerator to compute the specialized config. hw_version:default\n",
      "Total annotations before filtering: 36781\n",
      "Total annotations after filtering: 23043\n",
      "Total COCO annotations: 36781\n",
      "Filtered annotations: 23043\n",
      "23043\n"
     ]
    }
   ],
   "source": [
    "from aimet_torch.quantsim import QuantizationSimModel\n",
    "from aimet_common.defs import QuantScheme\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import DatasetReader\n",
    "\n",
    "# Dummy input to define the model input size\n",
    "dummy_input = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "transform = dsutils.transforms.RESIZE_NORMALIZE\n",
    "\n",
    "# Step 1: Create QuantizationSimModel\n",
    "sim = QuantizationSimModel(modelQuantized.cpu(), dummy_input=dummy_input,\n",
    "                                     quant_scheme=QuantScheme.post_training_tf_enhanced,\n",
    "                                     default_param_bw=8, default_output_bw=8)\n",
    "\n",
    "\n",
    "coco_config = dsutils.COCOConfig(min_size=0)\n",
    "\n",
    "datasetCOCO = dsutils.get_coco_dataset(config=coco_config, transform=transform, mode='val')\n",
    "datasetIMGNET = dsutils.get_imgnet_dataset(config=None, transform=transform)\n",
    "\n",
    "dataloader = DataLoader(datasetCOCO, batch_size=1, shuffle=True)\n",
    "\n",
    "print(len(dataloader))\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Compute Encodings (calibration)\n",
    "def calibration_function(model, eval_iterations = 30, use_cuda = False):\n",
    "    for i, data in enumerate(dataloader):\n",
    "        image, _, _ = data\n",
    "        model(image)\n",
    "        if (i > eval_iterations):\n",
    "            break\n",
    "        if(i % 50 == 0):\n",
    "            print(i)\n",
    "    print(eval_iterations)\n",
    "    print(\"cal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV3(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): QuantizedConv2d(\n",
      "        3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (1): QuantizedBatchNorm2d(\n",
      "        16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): None\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (2): QuantizedHardswish(\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedReLU(\n",
      "            inplace=True\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): SqueezeExcitation(\n",
      "          (avgpool): QuantizedAdaptiveAvgPool2d(\n",
      "            output_size=1\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc1): QuantizedConv2d(\n",
      "            16, 8, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            8, 16, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (activation): QuantizedReLU(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (scale_activation): QuantizedHardsigmoid(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedReLU(\n",
      "            inplace=True\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedReLU(\n",
      "            inplace=True\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedReLU(\n",
      "            inplace=True\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedReLU(\n",
      "            inplace=True\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): QuantizedAdaptiveAvgPool2d(\n",
      "            output_size=1\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc1): QuantizedConv2d(\n",
      "            96, 24, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            24, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (activation): QuantizedReLU(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (scale_activation): QuantizedHardsigmoid(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): QuantizedAdaptiveAvgPool2d(\n",
      "            output_size=1\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc1): QuantizedConv2d(\n",
      "            240, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            64, 240, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (activation): QuantizedReLU(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (scale_activation): QuantizedHardsigmoid(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): QuantizedAdaptiveAvgPool2d(\n",
      "            output_size=1\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc1): QuantizedConv2d(\n",
      "            240, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            64, 240, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (activation): QuantizedReLU(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (scale_activation): QuantizedHardsigmoid(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): QuantizedAdaptiveAvgPool2d(\n",
      "            output_size=1\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc1): QuantizedConv2d(\n",
      "            120, 32, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            32, 120, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (activation): QuantizedReLU(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (scale_activation): QuantizedHardsigmoid(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): QuantizedAdaptiveAvgPool2d(\n",
      "            output_size=1\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc1): QuantizedConv2d(\n",
      "            144, 40, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            40, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (activation): QuantizedReLU(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (scale_activation): QuantizedHardsigmoid(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): QuantizedAdaptiveAvgPool2d(\n",
      "            output_size=1\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc1): QuantizedConv2d(\n",
      "            288, 72, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            72, 288, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (activation): QuantizedReLU(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (scale_activation): QuantizedHardsigmoid(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): QuantizedAdaptiveAvgPool2d(\n",
      "            output_size=1\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc1): QuantizedConv2d(\n",
      "            576, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            144, 576, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (activation): QuantizedReLU(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (scale_activation): QuantizedHardsigmoid(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): QuantizedAdaptiveAvgPool2d(\n",
      "            output_size=1\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc1): QuantizedConv2d(\n",
      "            576, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            144, 576, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (activation): QuantizedReLU(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (scale_activation): QuantizedHardsigmoid(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): Conv2dNormActivation(\n",
      "      (0): QuantizedConv2d(\n",
      "        96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (1): QuantizedBatchNorm2d(\n",
      "        576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): None\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (2): QuantizedHardswish(\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (avgpool): QuantizedAdaptiveAvgPool2d(\n",
      "    output_size=1\n",
      "    (param_quantizers): ModuleDict()\n",
      "    (input_quantizers): ModuleList(\n",
      "      (0): None\n",
      "    )\n",
      "    (output_quantizers): ModuleList(\n",
      "      (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): QuantizedLinear(\n",
      "      in_features=576, out_features=1024, bias=True\n",
      "      (param_quantizers): ModuleDict(\n",
      "        (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "        (bias): None\n",
      "      )\n",
      "      (input_quantizers): ModuleList(\n",
      "        (0): None\n",
      "      )\n",
      "      (output_quantizers): ModuleList(\n",
      "        (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "      )\n",
      "    )\n",
      "    (1): QuantizedHardswish(\n",
      "      (param_quantizers): ModuleDict()\n",
      "      (input_quantizers): ModuleList(\n",
      "        (0): None\n",
      "      )\n",
      "      (output_quantizers): ModuleList(\n",
      "        (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "      )\n",
      "    )\n",
      "    (2): QuantizedDropout(\n",
      "      p=0.2, inplace=True\n",
      "      (param_quantizers): ModuleDict()\n",
      "      (input_quantizers): ModuleList(\n",
      "        (0): None\n",
      "      )\n",
      "      (output_quantizers): ModuleList(\n",
      "        (0): None\n",
      "      )\n",
      "    )\n",
      "    (3): QuantizedLinear(\n",
      "      in_features=1024, out_features=64, bias=True\n",
      "      (param_quantizers): ModuleDict(\n",
      "        (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "        (bias): None\n",
      "      )\n",
      "      (input_quantizers): ModuleList(\n",
      "        (0): None\n",
      "      )\n",
      "      (output_quantizers): ModuleList(\n",
      "        (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(sim.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "30\n",
      "cal\n"
     ]
    }
   ],
   "source": [
    "sim.compute_encodings(calibration_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV3(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): QuantizedConv2d(\n",
      "        3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (1): QuantizedBatchNorm2d(\n",
      "        16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): None\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (2): QuantizedHardswish(\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedReLU(\n",
      "            inplace=True\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): SqueezeExcitation(\n",
      "          (avgpool): QuantizedAdaptiveAvgPool2d(\n",
      "            output_size=1\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc1): QuantizedConv2d(\n",
      "            16, 8, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            8, 16, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (activation): QuantizedReLU(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (scale_activation): QuantizedHardsigmoid(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedReLU(\n",
      "            inplace=True\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedReLU(\n",
      "            inplace=True\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedReLU(\n",
      "            inplace=True\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedReLU(\n",
      "            inplace=True\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): QuantizedAdaptiveAvgPool2d(\n",
      "            output_size=1\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc1): QuantizedConv2d(\n",
      "            96, 24, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            24, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (activation): QuantizedReLU(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (scale_activation): QuantizedHardsigmoid(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): QuantizedAdaptiveAvgPool2d(\n",
      "            output_size=1\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc1): QuantizedConv2d(\n",
      "            240, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            64, 240, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (activation): QuantizedReLU(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (scale_activation): QuantizedHardsigmoid(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): QuantizedAdaptiveAvgPool2d(\n",
      "            output_size=1\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc1): QuantizedConv2d(\n",
      "            240, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            64, 240, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (activation): QuantizedReLU(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (scale_activation): QuantizedHardsigmoid(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): QuantizedAdaptiveAvgPool2d(\n",
      "            output_size=1\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc1): QuantizedConv2d(\n",
      "            120, 32, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            32, 120, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (activation): QuantizedReLU(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (scale_activation): QuantizedHardsigmoid(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): QuantizedAdaptiveAvgPool2d(\n",
      "            output_size=1\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc1): QuantizedConv2d(\n",
      "            144, 40, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            40, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (activation): QuantizedReLU(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (scale_activation): QuantizedHardsigmoid(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): QuantizedAdaptiveAvgPool2d(\n",
      "            output_size=1\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc1): QuantizedConv2d(\n",
      "            288, 72, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            72, 288, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (activation): QuantizedReLU(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (scale_activation): QuantizedHardsigmoid(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): QuantizedAdaptiveAvgPool2d(\n",
      "            output_size=1\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc1): QuantizedConv2d(\n",
      "            576, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            144, 576, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (activation): QuantizedReLU(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (scale_activation): QuantizedHardsigmoid(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (2): QuantizedHardswish(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): QuantizedAdaptiveAvgPool2d(\n",
      "            output_size=1\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc1): QuantizedConv2d(\n",
      "            576, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            144, 576, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (activation): QuantizedReLU(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (scale_activation): QuantizedHardsigmoid(\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): QuantizedConv2d(\n",
      "            576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "          (1): QuantizedBatchNorm2d(\n",
      "            96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): None\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): Conv2dNormActivation(\n",
      "      (0): QuantizedConv2d(\n",
      "        96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (1): QuantizedBatchNorm2d(\n",
      "        576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): None\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (2): QuantizedHardswish(\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (avgpool): QuantizedAdaptiveAvgPool2d(\n",
      "    output_size=1\n",
      "    (param_quantizers): ModuleDict()\n",
      "    (input_quantizers): ModuleList(\n",
      "      (0): None\n",
      "    )\n",
      "    (output_quantizers): ModuleList(\n",
      "      (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): QuantizedLinear(\n",
      "      in_features=576, out_features=1024, bias=True\n",
      "      (param_quantizers): ModuleDict(\n",
      "        (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "        (bias): None\n",
      "      )\n",
      "      (input_quantizers): ModuleList(\n",
      "        (0): None\n",
      "      )\n",
      "      (output_quantizers): ModuleList(\n",
      "        (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "      )\n",
      "    )\n",
      "    (1): QuantizedHardswish(\n",
      "      (param_quantizers): ModuleDict()\n",
      "      (input_quantizers): ModuleList(\n",
      "        (0): None\n",
      "      )\n",
      "      (output_quantizers): ModuleList(\n",
      "        (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "      )\n",
      "    )\n",
      "    (2): QuantizedDropout(\n",
      "      p=0.2, inplace=True\n",
      "      (param_quantizers): ModuleDict()\n",
      "      (input_quantizers): ModuleList(\n",
      "        (0): None\n",
      "      )\n",
      "      (output_quantizers): ModuleList(\n",
      "        (0): None\n",
      "      )\n",
      "    )\n",
      "    (3): QuantizedLinear(\n",
      "      in_features=1024, out_features=64, bias=True\n",
      "      (param_quantizers): ModuleDict(\n",
      "        (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "        (bias): None\n",
      "      )\n",
      "      (input_quantizers): ModuleList(\n",
      "        (0): None\n",
      "      )\n",
      "      (output_quantizers): ModuleList(\n",
      "        (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(sim.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total annotations before filtering: 860001\n",
      "Total annotations after filtering: 204543\n",
      "Total COCO annotations: 860001\n",
      "Filtered annotations: 204543\n",
      "2025-02-28 18:31:50,012 - pytorch_lightning.utilities.rank_zero - INFO - Using 16bit Automatic Mixed Precision (AMP)\n",
      "2025-02-28 18:31:50,015 - pytorch_lightning.utilities.rank_zero - INFO - GPU available: True (cuda), used: True\n",
      "2025-02-28 18:31:50,016 - pytorch_lightning.utilities.rank_zero - INFO - TPU available: False, using: 0 TPU cores\n",
      "2025-02-28 18:31:50,016 - pytorch_lightning.utilities.rank_zero - INFO - HPU available: False, using: 0 HPUs\n",
      "2025-02-28 18:31:50,017 - pytorch_lightning.utilities.rank_zero - INFO - You are using a CUDA device ('NVIDIA GeForce RTX 3050') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 18:31:50.150950: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-28 18:31:50.157846: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740763910.166817   67400 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740763910.169540   67400 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-28 18:31:50.180106: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-28 18:31:50,909 - pytorch_lightning.accelerators.cuda - INFO - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2025-02-28 18:31:50,942 - pytorch_lightning.callbacks.model_summary - INFO - \n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | MobileNetV3      | 1.6 M  | eval \n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "1.6 M     Trainable params\n",
      "324       Non-trainable params\n",
      "1.6 M     Total params\n",
      "6.335     Total estimated model params size (MB)\n",
      "586       Modules in train mode\n",
      "209       Modules in eval mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/centar15-desktop1/LPCV_2025_T1/src/mobilenet_v3/models exists and is not empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1124 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W228 18:32:01.480440844 kineto_shim.cpp:412] Profiler is not initialized: skipping profiling metadata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 1/1124 [00:11<3:32:01,  0.09it/s, v_num=5, train_loss=5.270]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W228 18:32:04.115237970 kineto_shim.cpp:412] Profiler is not initialized: skipping profiling metadata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 2/1124 [00:12<1:58:46,  0.16it/s, v_num=5, train_loss=5.310]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W228 18:32:05.331258303 kineto_shim.cpp:412] Profiler is not initialized: skipping profiling metadata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 3/1124 [00:13<1:25:21,  0.22it/s, v_num=5, train_loss=5.300]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W228 18:32:06.339322233 kineto_shim.cpp:412] Profiler is not initialized: skipping profiling metadata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 4/1124 [00:14<1:08:41,  0.27it/s, v_num=5, train_loss=5.310]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W228 18:32:07.352526230 kineto_shim.cpp:412] Profiler is not initialized: skipping profiling metadata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 5/1124 [00:15<58:40,  0.32it/s, v_num=5, train_loss=5.180]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W228 18:32:08.366550351 kineto_shim.cpp:412] Profiler is not initialized: skipping profiling metadata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 6/1124 [00:16<52:02,  0.36it/s, v_num=5, train_loss=5.110]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W228 18:32:09.400646416 kineto_shim.cpp:412] Profiler is not initialized: skipping profiling metadata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 7/1124 [00:17<47:25,  0.39it/s, v_num=5, train_loss=5.170]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W228 18:32:10.447304740 kineto_shim.cpp:412] Profiler is not initialized: skipping profiling metadata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 8/1124 [00:18<43:48,  0.42it/s, v_num=5, train_loss=4.410]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W228 18:32:11.486392076 kineto_shim.cpp:412] Profiler is not initialized: skipping profiling metadata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 9/1124 [00:19<41:02,  0.45it/s, v_num=5, train_loss=4.430]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W228 18:32:12.512107022 kineto_shim.cpp:412] Profiler is not initialized: skipping profiling metadata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%|▎         | 31/1124 [00:41<24:37,  0.74it/s, v_num=5, train_loss=2.100]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W228 18:32:35.651735988 collection.cpp:634] Warning: void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}) (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  12%|█▏        | 132/1124 [02:24<18:02,  0.92it/s, v_num=5, train_loss=1.290]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  14%|█▍        | 160/1124 [02:50<17:05,  0.94it/s, v_num=5, train_loss=1.160]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  52%|█████▏    | 584/1124 [09:27<08:44,  1.03it/s, v_num=5, train_loss=0.800]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (158787870 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  69%|██████▉   | 779/1124 [12:11<05:24,  1.06it/s, v_num=5, train_loss=0.469] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (158787870 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  72%|███████▏  | 811/1124 [12:41<04:53,  1.06it/s, v_num=5, train_loss=0.498]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 5/1124 [00:08<29:57,  0.62it/s, v_num=5, train_loss=0.501]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (158787870 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:   3%|▎         | 35/1124 [00:36<18:43,  0.97it/s, v_num=5, train_loss=0.519]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  32%|███▏      | 362/1124 [05:42<12:00,  1.06it/s, v_num=5, train_loss=0.478]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  12%|█▏        | 130/1124 [02:03<15:45,  1.05it/s, v_num=5, train_loss=0.416] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  89%|████████▉ | 998/1124 [15:36<01:58,  1.07it/s, v_num=5, train_loss=0.380]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (158787870 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:   5%|▍         | 52/1124 [00:51<17:40,  1.01it/s, v_num=5, train_loss=0.397]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  19%|█▉        | 216/1124 [03:24<14:21,  1.05it/s, v_num=5, train_loss=0.352]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (158787870 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:  32%|███▏      | 358/1124 [05:37<12:01,  1.06it/s, v_num=5, train_loss=0.292] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (158787870 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:  38%|███▊      | 431/1124 [06:45<10:52,  1.06it/s, v_num=5, train_loss=0.306]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:   6%|▌         | 66/1124 [01:05<17:23,  1.01it/s, v_num=5, train_loss=0.258]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:  23%|██▎       | 264/1124 [04:09<13:33,  1.06it/s, v_num=5, train_loss=0.291] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (158787870 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:  38%|███▊      | 428/1124 [06:43<10:55,  1.06it/s, v_num=5, train_loss=0.281] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:  96%|█████████▌| 1076/1124 [16:49<00:45,  1.07it/s, v_num=5, train_loss=0.256]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (158787870 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:   9%|▊         | 98/1124 [01:33<16:24,  1.04it/s, v_num=5, train_loss=0.288] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (158787870 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  34%|███▎      | 379/1124 [05:57<11:41,  1.06it/s, v_num=5, train_loss=0.248]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:  68%|██████▊   | 767/1124 [12:00<05:35,  1.06it/s, v_num=5, train_loss=0.234] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (158787870 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:  87%|████████▋ | 978/1124 [15:18<02:17,  1.07it/s, v_num=5, train_loss=0.241]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:  88%|████████▊ | 990/1124 [15:28<02:05,  1.07it/s, v_num=5, train_loss=0.258] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:   2%|▏         | 28/1124 [00:28<18:51,  0.97it/s, v_num=5, train_loss=0.220]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:  29%|██▉       | 331/1124 [05:12<12:28,  1.06it/s, v_num=5, train_loss=0.240]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (158787870 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14:  15%|█▍        | 164/1124 [02:36<15:14,  1.05it/s, v_num=5, train_loss=0.190] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:  78%|███████▊  | 874/1124 [13:42<03:55,  1.06it/s, v_num=5, train_loss=0.192]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (158787870 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17:  78%|███████▊  | 876/1124 [13:42<03:52,  1.06it/s, v_num=5, train_loss=0.180] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (158787870 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23:   0%|          | 0/1124 [00:00<?, ?it/s, v_num=5, train_loss=0.106]           "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.profilers import PyTorchProfiler\n",
    "import sys\n",
    "\n",
    "# Import your dataset modules\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "import dataset.DatasetReader as DatasetReader\n",
    "import dataset.utils as utils\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p = 0.3),\n",
    "    transforms.ColorJitter(brightness = 0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define dataset paths\n",
    "root_folder = r\"../../datasets/imagenet/new_imgnet_coco\"\n",
    "annotation_file = os.path.join(r\"../../datasets\", \"coco\", \"annotations\", \"instances_train2017.json\")\n",
    "image_dir = os.path.join(r\"../../datasets\", \"coco\", \"train2017\")\n",
    "\n",
    "# Load datasets\n",
    "dataset_coco = DatasetReader.COCODataset(\n",
    "    annotation_file=annotation_file,\n",
    "    image_dir=image_dir,\n",
    "    target_classes=[s.lower() for s in utils.GLOBAL_CLASSES],\n",
    "    transform=transform,\n",
    "    min_size = 60\n",
    ")\n",
    "\n",
    "class_names = [s.lower().replace(' ', '_') for s in utils.GLOBAL_CLASSES]\n",
    "dataset_imagenet = DatasetReader.CustomImageFolder(root_dir=root_folder, class_names=class_names, transform=transform)\n",
    "\n",
    "dataset = torch.utils.data.ConcatDataset([dataset_coco, dataset_imagenet])\n",
    "\n",
    "# Define DataLoader\n",
    "batch_size = 128*4\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=15, persistent_workers=True, pin_memory=True,prefetch_factor=2)\n",
    "\n",
    "#Napravi cist AIMET Model bez kvantizacije\n",
    "num_classes=64\n",
    "\n",
    "\n",
    "# Kreira fake model, pa cemo mu ubaciti nas\n",
    "\n",
    "\n",
    "class LightningQuantModel(pl.LightningModule):\n",
    "    def __init__(self, model_for_qat, num_classes=64, learning_rate=2e-5,alpha=0.7):\n",
    "        super(LightningQuantModel, self).__init__()\n",
    "        self.model = model_for_qat\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels, ds = batch \n",
    "        images, labels = images.to(self.device), labels.to(self.device)\n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "\n",
    "        ds = ds.to(self.device)\n",
    "        alpha_mask = (ds == 0).float() * self.alpha + (ds == 1).float() * (1 - self.alpha)\n",
    "        loss = loss * alpha_mask\n",
    "        loss = loss.mean()\n",
    "\n",
    "        # if(ds == 0):\n",
    "        #     loss = self.alpha*loss\n",
    "        # elif(ds == 1):\n",
    "        #     loss = (1 - self.alpha)*loss\n",
    "        \n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "\n",
    "# Define model checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"models/\",\n",
    "    filename=\"quant_mobilenet_v3-{epoch:02d}-{train_loss:.2f}\",\n",
    "    save_top_k=3,\n",
    "    monitor=\"train_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "logger = TensorBoardLogger(\"logs/\", name = \"quant_mobilenet_v3\", default_hp_metric=False)\n",
    "profiler= PyTorchProfiler(on_trace_ready=torch.profiler.tensorboard_trace_handler(\"tb_logs/profiler0\"),trace_memory=True,\n",
    "                          schedule=torch.profiler.schedule(skip_first=10,warmup=1,wait=1,active=20))\n",
    "\n",
    "\n",
    "quant_model = LightningQuantModel(sim.model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    profiler=profiler,\n",
    "    max_epochs=200,\n",
    "    devices=1,  # Use only one GPU or CPU\n",
    "    accelerator=\"auto\",\n",
    "    strategy=\"auto\",  # Allows later multi-GPU setup without changing code\n",
    "    precision = \"16-mixed\",\n",
    "    callbacks=[checkpoint_callback],\n",
    "    logger = logger\n",
    ")\n",
    "\n",
    "trainer.fit(quant_model, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform2 = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DequantizedTensor([[-19.2462,  -5.8871, -15.6234,   1.3586, -18.5669, -12.6799,\n",
       "                    -11.0949,  -5.4342,  -3.8492,  -3.3964, -19.2462,  -8.1513,\n",
       "                     -6.7928, -20.3783, -13.1327, -22.8690, -10.1892,  -9.5099,\n",
       "                    -13.8120, -12.2270, -22.8690, -19.2462,  -2.2643, -14.9441,\n",
       "                    -17.6612,  -9.7363,  -4.5285,  -4.0757, -18.1141, -19.2462,\n",
       "                     -6.3399,   2.7171, -15.6234, -19.0198,   5.8871,  -6.5664,\n",
       "                     -7.4721,  -4.0757, -27.3975,  -6.5664, -23.5483, -13.3591,\n",
       "                     -8.1513, -10.8684, -12.0006, -14.4913,  -2.0378, -10.1892,\n",
       "                    -14.4913, -13.8120, -21.0576, -18.5669, -19.9255, -14.2648,\n",
       "                    -20.3783, -14.0384, -22.1897,   0.4529, -15.3970, -14.0384,\n",
       "                    -10.1892,   0.2264, -15.1705, -17.4348]],\n",
       "                  grad_fn=<AliasBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.model(transform2(mug_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-28 13:07:42,912 - Utils - INFO - successfully created onnx model with 106/122 node names updated\n",
      "2025-02-28 13:07:42,942 - Quant - INFO - Layers excluded from quantization: []\n",
      "2025-02-28 13:07:42,945 - Quant - WARNING - \u001b[31;21mQuantsim export will stop exporting encodings for saving and loading in a future AIMET release.\n",
      "To export encodings for saving and loading, use QuantizationSimModel's save_encodings_to_json() utility instead.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Step 3: Export Quantized Model\n",
    "os.makedirs('quantized', exist_ok=True)\n",
    "sim.export(path='quantized', filename_prefix='mobilenetv3quantized', dummy_input=dummy_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 predictions for  on :\n",
      "34 Bowl                  94.2%\n",
      "31 Cup                    4.0%\n",
      "3 Airplane               1.0%\n",
      "57 Sink                   0.4%\n",
      "61 Vase                   0.3%\n"
     ]
    }
   ],
   "source": [
    "helper.print_probablities_from_output(sim.model(transform2(mug_image)), dsutils.GLOBAL_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading mobilenetv3.aimet.zip\n",
      "2025-02-26 16:36:31,620 - root - INFO - Uploading asset to https://tetrahub-qprod-userdata.s3-accelerate.amazonaws.com/models/mn74rp24n_2WWOphhfIrSwEWoZ.aimet.zip?uploadId=gCO3B5wSYPnDr1MTH4mXCmumwQFRI.7A2VrwE_0ohRROXDjdZWg4sB1hPGkxAqewGE0Zi6oLjJ8m9cIRPY.L6QkbO7OCnvMAqUCrFlqhPaZcSq1H2c8fOQIMXS68N5HoWiTrkOFrtgrDlWNj7F3KSQVE.SADCucqjYNPVV_BJkE-&partNumber=1&A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[34m██████████\u001b[0m| 2.31M/2.31M [00:00<00:00, 5.59MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 16:36:32,055 - root - INFO - Successfully uploaded asset with response status: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled compile job (j5q09w7np) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/j5q09w7np/\n",
      "\n",
      "Uploading mobilenetv3.aimet.zip\n",
      "2025-02-26 16:36:34,592 - root - INFO - Uploading asset to https://tetrahub-qprod-userdata.s3-accelerate.amazonaws.com/models/mq2x57wln_0yRBjrhy9TLTo8S6.aimet.zip?uploadId=Bm16ZfqP28K0ppOUPLO5FdZcmPoT97mozw63SjHmLt2M3TXpmMwoJXUuwhnY83c1dl5JFwnUSy2ejGyUQ4pnsw6CBqPg.rcgmMvo18FiJXLdhkE3dj6lWHbCdU_sR4h_ez1bVOzdAW4lUmMD0niljH8kidAoW.BgrLu.VvbcHzw-&partNumber=1&A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[34m██████████\u001b[0m| 2.31M/2.31M [00:01<00:00, 1.89MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 16:36:35,871 - root - INFO - Successfully uploaded asset with response status: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled compile job (jgl4e70j5) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/jgl4e70j5/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import qai_hub as hub\n",
    "\n",
    "compile_job = hub.submit_compile_job(\n",
    "    model=\"mobilenetv3.aimet\",\n",
    "    device=hub.Device(\"Samsung Galaxy S24 (Family)\"),\n",
    ")\n",
    "assert isinstance(compile_job, hub.CompileJob)\n",
    "\n",
    "# Compile to a QNN Model Library\n",
    "compile_job_qnn = hub.submit_compile_job(\n",
    "    model=\"mobilenetv3.aimet\",\n",
    "    device=hub.Device(\"Samsung Galaxy S24 (Family)\"),\n",
    "    options=\"--target_runtime qnn_lib_aarch64_android --quantize_full_type int8\",\n",
    ")\n",
    "assert isinstance(compile_job_qnn, hub.CompileJob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled profile job (jp12q338g) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/jp12q338g/\n",
      "\n",
      "Scheduled profile job (jgdn700r5) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/jgdn700r5/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile_job     = hub.submit_profile_job(compile_job.get_target_model(), device=hub.Device(\"Snapdragon 8 Elite QRD\"),)\n",
    "\n",
    "profile_job_qnn = hub.submit_profile_job(compile_job_qnn.get_target_model(), device=hub.Device(\"Snapdragon 8 Elite QRD\"),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading tmp38txkz2e.pt\n",
      "2025-02-26 16:36:46,195 - root - INFO - Uploading asset to https://tetrahub-qprod-userdata.s3-accelerate.amazonaws.com/models/mmd9zreon_jzDXEc6omiivcwsA.pt?uploadId=12VxES_BY3ogLc_sGVpfwyRnXTyYUAZQ9WaauKDPxfuCpRcKbv.bxrB7JgmFb64Boms.90LC.z3rhlELZYYnn48UMA.CF5eOSjXZ0WBpajWzId0slmSBePbImkKpQRaaYGe9IvzyVPdZl3.lA5D2L24hqxu9nvhSoqcyftrBs38-&partNumber=1&AWSAcces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[34m██████████\u001b[0m| 6.51M/6.51M [00:00<00:00, 7.51MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 16:36:47,105 - root - INFO - Successfully uploaded asset with response status: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled compile job (jpvqz4zkg) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/jpvqz4zkg/\n",
      "\n",
      "Uploading tmp8kz4p03f.pt\n",
      "2025-02-26 16:36:49,477 - root - INFO - Uploading asset to https://tetrahub-qprod-userdata.s3-accelerate.amazonaws.com/models/mnlx2kekn_Psy4FUQThP9wyFDh.pt?uploadId=iP.eGzVhGb04LXKfEmwEZd1HS0dw.GojWG5h1fEP1pUkVUsGRFdmd8aAqpAOsiEv3JrgTX6E4goZz1mxoDyrbZrdv1iJW5cc.2YIWgKN3qbCiyROUcmfBJVJnKinTmPgw5Xa9DtK5c1U3z2sCAYrnAqLmWhc5Kbl2DJIzeFXW4E-&partNumber=1&AWSAcces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[34m██████████\u001b[0m| 6.51M/6.51M [00:00<00:00, 14.7MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 16:36:49,943 - root - INFO - Successfully uploaded asset with response status: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled compile job (jg90989wg) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/jg90989wg/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_traced_original = torch.jit.trace(modelOriginal.cpu(), dummy_input)\n",
    "\n",
    "\n",
    "\n",
    "compile_job_original = hub.submit_compile_job(\n",
    "    model=model_traced_original,\n",
    "    device=hub.Device(\"Samsung Galaxy S24 (Family)\"),\n",
    "    input_specs=dict(image=input_shape)\n",
    ")\n",
    "assert isinstance(compile_job_original, hub.CompileJob)\n",
    "\n",
    "# Compile to a QNN Model Library\n",
    "compile_job_original_qnn = hub.submit_compile_job(\n",
    "    model=model_traced_original,\n",
    "    device=hub.Device(\"Samsung Galaxy S24 (Family)\"),\n",
    "    input_specs=dict(image=input_shape),\n",
    "    options=\"--target_runtime qnn_lib_aarch64_android\",\n",
    ")\n",
    "assert isinstance(compile_job_original_qnn, hub.CompileJob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled profile job (jpx9emm3p) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/jpx9emm3p/\n",
      "\n",
      "Scheduled profile job (j5mev44dp) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/j5mev44dp/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile_job_original     = hub.submit_profile_job(compile_job_original.get_target_model(), device=hub.Device(\"Snapdragon 8 Elite QRD\"),)\n",
    "\n",
    "profile_job_original_qnn = hub.submit_profile_job(compile_job_original_qnn.get_target_model(), device=hub.Device(\"Snapdragon 8 Elite QRD\"),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved profile results to /home/centar15-desktop1/LPCV_2025_T1/src/mobilenet_v3/profile_results/job_j5q09w7np_optimized_tflite_jp12q338g_results.json\n",
      "Saved profile results to /home/centar15-desktop1/LPCV_2025_T1/src/mobilenet_v3/profile_results/job_jgl4e70j5_optimized_so_jgdn700r5_results.json\n"
     ]
    }
   ],
   "source": [
    "quanTFLITE = profile_job.download_results('profile_results')\n",
    "quanQNN    = profile_job_qnn.download_results('profile_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProfileJobResult\n",
       "----------------\n",
       "status                        : JobStatus\n",
       "---------\n",
       "code    : SUCCESS\n",
       "message : \n",
       "\n",
       "url                           : https://app.aihub.qualcomm.com/jobs/jp12q338g/\n",
       "artifacts_dir                 : /home/centar15-desktop1/LPCV_2025_T1/src/mobilenet_v3/profile_results\n",
       "Estimated Inference Time (ms) : 0.196\n",
       "Load Time (ms)                : 205.472\n",
       "Peak Memory (MB)              : 174.859375\n",
       "Compute Units (layers)        : NPU: 124"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quanTFLITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProfileJobResult\n",
       "----------------\n",
       "status                        : JobStatus\n",
       "---------\n",
       "code    : SUCCESS\n",
       "message : \n",
       "\n",
       "url                           : https://app.aihub.qualcomm.com/jobs/jgdn700r5/\n",
       "artifacts_dir                 : /home/centar15-desktop1/LPCV_2025_T1/src/mobilenet_v3/profile_results\n",
       "Estimated Inference Time (ms) : 0.274\n",
       "Load Time (ms)                : 220.315\n",
       "Peak Memory (MB)              : 136.109375\n",
       "Compute Units (layers)        : NPU: 113"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quanQNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved profile results to /home/centar15-desktop1/LPCV_2025_T1/src/mobilenet_v3/profile_results/job_jpvqz4zkg_optimized_tflite_jpx9emm3p_results.json\n",
      "Saved profile results to /home/centar15-desktop1/LPCV_2025_T1/src/mobilenet_v3/profile_results/job_jg90989wg_optimized_so_j5mev44dp_results.json\n"
     ]
    }
   ],
   "source": [
    "quanTFLITE_original = profile_job_original.download_results('profile_results')\n",
    "quanQNN_original    = profile_job_original_qnn.download_results('profile_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProfileJobResult\n",
       "----------------\n",
       "status                        : JobStatus\n",
       "---------\n",
       "code    : SUCCESS\n",
       "message : \n",
       "\n",
       "url                           : https://app.aihub.qualcomm.com/jobs/jpx9emm3p/\n",
       "artifacts_dir                 : /home/centar15-desktop1/LPCV_2025_T1/src/mobilenet_v3/profile_results\n",
       "Estimated Inference Time (ms) : 0.332\n",
       "Load Time (ms)                : 313.951\n",
       "Peak Memory (MB)              : 209.109375\n",
       "Compute Units (layers)        : NPU: 114"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quanTFLITE_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProfileJobResult\n",
       "----------------\n",
       "status                        : JobStatus\n",
       "---------\n",
       "code    : SUCCESS\n",
       "message : \n",
       "\n",
       "url                           : https://app.aihub.qualcomm.com/jobs/j5mev44dp/\n",
       "artifacts_dir                 : /home/centar15-desktop1/LPCV_2025_T1/src/mobilenet_v3/profile_results\n",
       "Estimated Inference Time (ms) : 0.317\n",
       "Load Time (ms)                : 273.872\n",
       "Peak Memory (MB)              : 162.33203125\n",
       "Compute Units (layers)        : NPU: 125"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quanQNN_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total annotations before filtering: 36781\n",
      "Total annotations after filtering: 8862\n",
      "Total COCO annotations: 36781\n",
      "Filtered annotations: 8862\n"
     ]
    }
   ],
   "source": [
    "datasetCOCO2 = DatasetReader.COCODataset(\n",
    "    annotation_file = r\"/home/centar15-desktop1/LPCV_2025_T1/datasets/coco/annotations/instances_val2017.json\", \n",
    "    image_dir= r'/home/centar15-desktop1/LPCV_2025_T1/datasets/coco/val2017',\n",
    "    target_classes=[s.lower() for s in dsutils.GLOBAL_CLASSES],\n",
    "    transform = transform,\n",
    "    min_size = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original  model accuracy =  0.7958700067704807\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "validationDataloader = DataLoader(datasetCOCO2, batch_size = 64, shuffle = False)\n",
    "with torch.no_grad():\n",
    "    print(\"Original  model accuracy = \", helper.check_accuracy(modelOriginal.cuda(), validationDataloader, device))\n",
    "    # print(\"Quantized model accuracy = \", helper.check_accuracy(sim.model.cuda(), validationDataloader, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model accuracy =  0.02561498533062514\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    #print(\"Original  model accuracy = \", helper.check_accuracy(modelOriginal.cuda(), validationDataloader, device))\n",
    "    print(\"Quantized model accuracy = \", helper.check_accuracy(sim.model.cuda(), validationDataloader, device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
