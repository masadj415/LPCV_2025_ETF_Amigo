{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-22 09:33:38.631232: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-22 09:33:38.639502: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742632418.648265  309473 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742632418.650897  309473 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-22 09:33:38.661631: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/home/centar15-desktop1/LPCV_2025_T1/src/casvit/rcvit.py:313: UserWarning: Overwriting rcvit_xs in registry with rcvit.rcvit_xs. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def rcvit_xs(**kwargs):\n",
      "/home/centar15-desktop1/LPCV_2025_T1/src/casvit/rcvit.py:329: UserWarning: Overwriting rcvit_m in registry with rcvit.rcvit_m. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def rcvit_m(**kwargs):\n",
      "/home/centar15-desktop1/LPCV_2025_T1/src/casvit/rcvit.py:337: UserWarning: Overwriting rcvit_t in registry with rcvit.rcvit_t. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def rcvit_t(**kwargs):\n",
      "/home/centar15-desktop1/LPCV_2025_T1/src/casvit/rcvit.py:345: UserWarning: Overwriting rcvit_t_modified in registry with rcvit.rcvit_t_modified. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def rcvit_t_modified(**kwargs):\n",
      "/home/centar15-desktop1/LPCV_2025_T1/src/casvit/rcvit.py:362: UserWarning: Overwriting rcvit_t_extended in registry with rcvit.rcvit_t_extended. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def rcvit_t_extended(**kwargs):\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchsummary\n",
    "import os\n",
    "import sys\n",
    "import pytorch_lightning as pl\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "ROOT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "from utils import helper, input_getter, qai_hub_jobs, tfhelper\n",
    "import training\n",
    "import rcvit_extended_train\n",
    "import rcvit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/src/casvit/rcvit_extended_train.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained = torch.load(pretrained_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing classifier head\n",
      "Loaded 20.8M/31.1M parameters\n"
     ]
    }
   ],
   "source": [
    "model: nn.Module = rcvit_extended_train.get_rcvit_extended(pretrained_path = \"CASVIT_t.pth\")\n",
    "model.dist = False\n",
    "# model = rcvit.rcvit_t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/src/training/lightning_model.py:18: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/centar15-desktop1/LPCV_2025_T1/src/training/lightning_model.py:23: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init device\n",
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RCViT(\n",
       "  (patch_embed): Sequential(\n",
       "    (0): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (network): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): AdditiveBlock(\n",
       "        (local_perception): LocalIntegration(\n",
       "          (network): Sequential(\n",
       "            (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "            (3): GELU(approximate='none')\n",
       "            (4): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (attn): AdditiveTokenMixer(\n",
       "          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (oper_q): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "                (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (oper_k): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "                (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dwc): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "          (proj): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): AdditiveBlock(\n",
       "        (local_perception): LocalIntegration(\n",
       "          (network): Sequential(\n",
       "            (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "            (3): GELU(approximate='none')\n",
       "            (4): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (attn): AdditiveTokenMixer(\n",
       "          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (oper_q): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "                (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (oper_k): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "                (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dwc): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "          (proj): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): AdditiveBlock(\n",
       "        (local_perception): LocalIntegration(\n",
       "          (network): Sequential(\n",
       "            (0): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "            (3): GELU(approximate='none')\n",
       "            (4): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (attn): AdditiveTokenMixer(\n",
       "          (qkv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (oper_q): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "                (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (oper_k): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "                (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dwc): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "          (proj): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Embedding(\n",
       "      (proj): Conv2d(96, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): AdditiveBlock(\n",
       "        (local_perception): LocalIntegration(\n",
       "          (network): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "            (3): GELU(approximate='none')\n",
       "            (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (attn): AdditiveTokenMixer(\n",
       "          (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (oper_q): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (oper_k): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dwc): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "          (proj): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): AdditiveBlock(\n",
       "        (local_perception): LocalIntegration(\n",
       "          (network): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "            (3): GELU(approximate='none')\n",
       "            (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (attn): AdditiveTokenMixer(\n",
       "          (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (oper_q): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (oper_k): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dwc): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "          (proj): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): AdditiveBlock(\n",
       "        (local_perception): LocalIntegration(\n",
       "          (network): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "            (3): GELU(approximate='none')\n",
       "            (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (attn): AdditiveTokenMixer(\n",
       "          (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (oper_q): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (oper_k): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dwc): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "          (proj): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Embedding(\n",
       "      (proj): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): AdditiveBlock(\n",
       "        (local_perception): LocalIntegration(\n",
       "          (network): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            (3): GELU(approximate='none')\n",
       "            (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (attn): AdditiveTokenMixer(\n",
       "          (qkv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (oper_q): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (oper_k): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dwc): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "          (proj): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): AdditiveBlock(\n",
       "        (local_perception): LocalIntegration(\n",
       "          (network): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            (3): GELU(approximate='none')\n",
       "            (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (attn): AdditiveTokenMixer(\n",
       "          (qkv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (oper_q): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (oper_k): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dwc): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "          (proj): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): AdditiveBlock(\n",
       "        (local_perception): LocalIntegration(\n",
       "          (network): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            (3): GELU(approximate='none')\n",
       "            (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (attn): AdditiveTokenMixer(\n",
       "          (qkv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (oper_q): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (oper_k): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dwc): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "          (proj): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): AdditiveBlock(\n",
       "        (local_perception): LocalIntegration(\n",
       "          (network): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            (3): GELU(approximate='none')\n",
       "            (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (attn): AdditiveTokenMixer(\n",
       "          (qkv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (oper_q): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (oper_k): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dwc): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "          (proj): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): AdditiveBlock(\n",
       "        (local_perception): LocalIntegration(\n",
       "          (network): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            (3): GELU(approximate='none')\n",
       "            (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (attn): AdditiveTokenMixer(\n",
       "          (qkv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (oper_q): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (oper_k): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dwc): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "          (proj): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): AdditiveBlock(\n",
       "        (local_perception): LocalIntegration(\n",
       "          (network): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "            (3): GELU(approximate='none')\n",
       "            (4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (attn): AdditiveTokenMixer(\n",
       "          (qkv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (oper_q): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (oper_k): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dwc): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "          (proj): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): Embedding(\n",
       "      (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): AdditiveBlock(\n",
       "        (local_perception): LocalIntegration(\n",
       "          (network): Sequential(\n",
       "            (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "            (3): GELU(approximate='none')\n",
       "            (4): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (attn): AdditiveTokenMixer(\n",
       "          (qkv): Conv2d(512, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (oper_q): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (oper_k): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dwc): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "          (proj): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): AdditiveBlock(\n",
       "        (local_perception): LocalIntegration(\n",
       "          (network): Sequential(\n",
       "            (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "            (3): GELU(approximate='none')\n",
       "            (4): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (attn): AdditiveTokenMixer(\n",
       "          (qkv): Conv2d(512, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (oper_q): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (oper_k): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dwc): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "          (proj): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): AdditiveBlock(\n",
       "        (local_perception): LocalIntegration(\n",
       "          (network): Sequential(\n",
       "            (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "            (3): GELU(approximate='none')\n",
       "            (4): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (attn): AdditiveTokenMixer(\n",
       "          (qkv): Conv2d(512, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (oper_q): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (oper_k): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dwc): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "          (proj): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): Embedding(\n",
       "      (proj): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): AdditiveBlock(\n",
       "        (local_perception): LocalIntegration(\n",
       "          (network): Sequential(\n",
       "            (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "            (3): GELU(approximate='none')\n",
       "            (4): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (attn): AdditiveTokenMixer(\n",
       "          (qkv): Conv2d(512, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (oper_q): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (oper_k): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dwc): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "          (proj): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): AdditiveBlock(\n",
       "        (local_perception): LocalIntegration(\n",
       "          (network): Sequential(\n",
       "            (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "            (3): GELU(approximate='none')\n",
       "            (4): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (attn): AdditiveTokenMixer(\n",
       "          (qkv): Conv2d(512, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (oper_q): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (oper_k): Sequential(\n",
       "            (0): SpatialOperation(\n",
       "              (block): Sequential(\n",
       "                (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "                (3): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (4): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "            (1): ChannelOperation(\n",
       "              (block): Sequential(\n",
       "                (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "                (1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (2): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (dwc): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "          (proj): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (head): Linear(in_features=512, out_features=64, bias=True)\n",
       "  (dist_head): Linear(in_features=512, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch\n",
    "\n",
    "import training.lightning_model\n",
    "import training.lightning_train_function\n",
    "\n",
    "weights = torch.randn([64])\n",
    "\n",
    "# lightningModel = training.lightning_model.LightningModel(modelBase, class_weights=weights)\n",
    "# lightningModel.load_from_checkpoint(\"../../models/CASVIT_T_SD_BALANCING/Ver1.0/CASVIT_T_SD_BALANCING-epoch=14-train_loss=0.0492-val_loss=0.1135.ckpt\")\n",
    "\n",
    "lightningModel = training.lightning_model.LightningModel.load_from_checkpoint(\"../../models/CASVIT_T_WITH_512_FINAL_LAYER/Ver1.1/CASVIT_T_WITH_512_FINAL_LAYER-epoch=22-train_loss=0.3280-val_loss=0.5063.ckpt\", model = model)\n",
    "\n",
    "modelFp32 = lightningModel.model\n",
    "modelFp32.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodelFp32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/LPCV_2025_T1/src/casvit/rcvit.py:295\u001b[0m, in \u001b[0;36mRCViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 295\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_tokens(x)\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfork_feat:\n\u001b[1;32m    298\u001b[0m         \u001b[38;5;66;03m# otuput features of four stages for dense prediction\u001b[39;00m\n",
      "File \u001b[0;32m~/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "modelFp32(torch.randn((1, 3, 224, 224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 48, 112, 112]           1,344\n",
      "       BatchNorm2d-2         [-1, 48, 112, 112]              96\n",
      "              ReLU-3         [-1, 48, 112, 112]               0\n",
      "            Conv2d-4           [-1, 96, 56, 56]          41,568\n",
      "       BatchNorm2d-5           [-1, 96, 56, 56]             192\n",
      "              ReLU-6           [-1, 96, 56, 56]               0\n",
      "            Conv2d-7           [-1, 96, 56, 56]           9,312\n",
      "       BatchNorm2d-8           [-1, 96, 56, 56]             192\n",
      "            Conv2d-9           [-1, 96, 56, 56]             960\n",
      "             GELU-10           [-1, 96, 56, 56]               0\n",
      "           Conv2d-11           [-1, 96, 56, 56]           9,312\n",
      " LocalIntegration-12           [-1, 96, 56, 56]               0\n",
      "      BatchNorm2d-13           [-1, 96, 56, 56]             192\n",
      "           Conv2d-14          [-1, 288, 56, 56]          27,648\n",
      "           Conv2d-15           [-1, 96, 56, 56]             960\n",
      "      BatchNorm2d-16           [-1, 96, 56, 56]             192\n",
      "             ReLU-17           [-1, 96, 56, 56]               0\n",
      "           Conv2d-18            [-1, 1, 56, 56]              96\n",
      "          Sigmoid-19            [-1, 1, 56, 56]               0\n",
      " SpatialOperation-20           [-1, 96, 56, 56]               0\n",
      "AdaptiveAvgPool2d-21             [-1, 96, 1, 1]               0\n",
      "           Conv2d-22             [-1, 96, 1, 1]           9,216\n",
      "          Sigmoid-23             [-1, 96, 1, 1]               0\n",
      " ChannelOperation-24           [-1, 96, 56, 56]               0\n",
      "           Conv2d-25           [-1, 96, 56, 56]             960\n",
      "      BatchNorm2d-26           [-1, 96, 56, 56]             192\n",
      "             ReLU-27           [-1, 96, 56, 56]               0\n",
      "           Conv2d-28            [-1, 1, 56, 56]              96\n",
      "          Sigmoid-29            [-1, 1, 56, 56]               0\n",
      " SpatialOperation-30           [-1, 96, 56, 56]               0\n",
      "AdaptiveAvgPool2d-31             [-1, 96, 1, 1]               0\n",
      "           Conv2d-32             [-1, 96, 1, 1]           9,216\n",
      "          Sigmoid-33             [-1, 96, 1, 1]               0\n",
      " ChannelOperation-34           [-1, 96, 56, 56]               0\n",
      "           Conv2d-35           [-1, 96, 56, 56]             960\n",
      "           Conv2d-36           [-1, 96, 56, 56]             960\n",
      "          Dropout-37           [-1, 96, 56, 56]               0\n",
      "AdditiveTokenMixer-38           [-1, 96, 56, 56]               0\n",
      "         Identity-39           [-1, 96, 56, 56]               0\n",
      "      BatchNorm2d-40           [-1, 96, 56, 56]             192\n",
      "           Conv2d-41          [-1, 384, 56, 56]          37,248\n",
      "             GELU-42          [-1, 384, 56, 56]               0\n",
      "          Dropout-43          [-1, 384, 56, 56]               0\n",
      "           Conv2d-44           [-1, 96, 56, 56]          36,960\n",
      "          Dropout-45           [-1, 96, 56, 56]               0\n",
      "              Mlp-46           [-1, 96, 56, 56]               0\n",
      "         Identity-47           [-1, 96, 56, 56]               0\n",
      "    AdditiveBlock-48           [-1, 96, 56, 56]               0\n",
      "           Conv2d-49           [-1, 96, 56, 56]           9,312\n",
      "      BatchNorm2d-50           [-1, 96, 56, 56]             192\n",
      "           Conv2d-51           [-1, 96, 56, 56]             960\n",
      "             GELU-52           [-1, 96, 56, 56]               0\n",
      "           Conv2d-53           [-1, 96, 56, 56]           9,312\n",
      " LocalIntegration-54           [-1, 96, 56, 56]               0\n",
      "      BatchNorm2d-55           [-1, 96, 56, 56]             192\n",
      "           Conv2d-56          [-1, 288, 56, 56]          27,648\n",
      "           Conv2d-57           [-1, 96, 56, 56]             960\n",
      "      BatchNorm2d-58           [-1, 96, 56, 56]             192\n",
      "             ReLU-59           [-1, 96, 56, 56]               0\n",
      "           Conv2d-60            [-1, 1, 56, 56]              96\n",
      "          Sigmoid-61            [-1, 1, 56, 56]               0\n",
      " SpatialOperation-62           [-1, 96, 56, 56]               0\n",
      "AdaptiveAvgPool2d-63             [-1, 96, 1, 1]               0\n",
      "           Conv2d-64             [-1, 96, 1, 1]           9,216\n",
      "          Sigmoid-65             [-1, 96, 1, 1]               0\n",
      " ChannelOperation-66           [-1, 96, 56, 56]               0\n",
      "           Conv2d-67           [-1, 96, 56, 56]             960\n",
      "      BatchNorm2d-68           [-1, 96, 56, 56]             192\n",
      "             ReLU-69           [-1, 96, 56, 56]               0\n",
      "           Conv2d-70            [-1, 1, 56, 56]              96\n",
      "          Sigmoid-71            [-1, 1, 56, 56]               0\n",
      " SpatialOperation-72           [-1, 96, 56, 56]               0\n",
      "AdaptiveAvgPool2d-73             [-1, 96, 1, 1]               0\n",
      "           Conv2d-74             [-1, 96, 1, 1]           9,216\n",
      "          Sigmoid-75             [-1, 96, 1, 1]               0\n",
      " ChannelOperation-76           [-1, 96, 56, 56]               0\n",
      "           Conv2d-77           [-1, 96, 56, 56]             960\n",
      "           Conv2d-78           [-1, 96, 56, 56]             960\n",
      "          Dropout-79           [-1, 96, 56, 56]               0\n",
      "AdditiveTokenMixer-80           [-1, 96, 56, 56]               0\n",
      "         Identity-81           [-1, 96, 56, 56]               0\n",
      "      BatchNorm2d-82           [-1, 96, 56, 56]             192\n",
      "           Conv2d-83          [-1, 384, 56, 56]          37,248\n",
      "             GELU-84          [-1, 384, 56, 56]               0\n",
      "          Dropout-85          [-1, 384, 56, 56]               0\n",
      "           Conv2d-86           [-1, 96, 56, 56]          36,960\n",
      "          Dropout-87           [-1, 96, 56, 56]               0\n",
      "              Mlp-88           [-1, 96, 56, 56]               0\n",
      "         Identity-89           [-1, 96, 56, 56]               0\n",
      "    AdditiveBlock-90           [-1, 96, 56, 56]               0\n",
      "           Conv2d-91           [-1, 96, 56, 56]           9,312\n",
      "      BatchNorm2d-92           [-1, 96, 56, 56]             192\n",
      "           Conv2d-93           [-1, 96, 56, 56]             960\n",
      "             GELU-94           [-1, 96, 56, 56]               0\n",
      "           Conv2d-95           [-1, 96, 56, 56]           9,312\n",
      " LocalIntegration-96           [-1, 96, 56, 56]               0\n",
      "      BatchNorm2d-97           [-1, 96, 56, 56]             192\n",
      "           Conv2d-98          [-1, 288, 56, 56]          27,648\n",
      "           Conv2d-99           [-1, 96, 56, 56]             960\n",
      "     BatchNorm2d-100           [-1, 96, 56, 56]             192\n",
      "            ReLU-101           [-1, 96, 56, 56]               0\n",
      "          Conv2d-102            [-1, 1, 56, 56]              96\n",
      "         Sigmoid-103            [-1, 1, 56, 56]               0\n",
      "SpatialOperation-104           [-1, 96, 56, 56]               0\n",
      "AdaptiveAvgPool2d-105             [-1, 96, 1, 1]               0\n",
      "          Conv2d-106             [-1, 96, 1, 1]           9,216\n",
      "         Sigmoid-107             [-1, 96, 1, 1]               0\n",
      "ChannelOperation-108           [-1, 96, 56, 56]               0\n",
      "          Conv2d-109           [-1, 96, 56, 56]             960\n",
      "     BatchNorm2d-110           [-1, 96, 56, 56]             192\n",
      "            ReLU-111           [-1, 96, 56, 56]               0\n",
      "          Conv2d-112            [-1, 1, 56, 56]              96\n",
      "         Sigmoid-113            [-1, 1, 56, 56]               0\n",
      "SpatialOperation-114           [-1, 96, 56, 56]               0\n",
      "AdaptiveAvgPool2d-115             [-1, 96, 1, 1]               0\n",
      "          Conv2d-116             [-1, 96, 1, 1]           9,216\n",
      "         Sigmoid-117             [-1, 96, 1, 1]               0\n",
      "ChannelOperation-118           [-1, 96, 56, 56]               0\n",
      "          Conv2d-119           [-1, 96, 56, 56]             960\n",
      "          Conv2d-120           [-1, 96, 56, 56]             960\n",
      "         Dropout-121           [-1, 96, 56, 56]               0\n",
      "AdditiveTokenMixer-122           [-1, 96, 56, 56]               0\n",
      "        Identity-123           [-1, 96, 56, 56]               0\n",
      "     BatchNorm2d-124           [-1, 96, 56, 56]             192\n",
      "          Conv2d-125          [-1, 384, 56, 56]          37,248\n",
      "            GELU-126          [-1, 384, 56, 56]               0\n",
      "         Dropout-127          [-1, 384, 56, 56]               0\n",
      "          Conv2d-128           [-1, 96, 56, 56]          36,960\n",
      "         Dropout-129           [-1, 96, 56, 56]               0\n",
      "             Mlp-130           [-1, 96, 56, 56]               0\n",
      "        Identity-131           [-1, 96, 56, 56]               0\n",
      "   AdditiveBlock-132           [-1, 96, 56, 56]               0\n",
      "          Conv2d-133          [-1, 128, 28, 28]         110,720\n",
      "     BatchNorm2d-134          [-1, 128, 28, 28]             256\n",
      "       Embedding-135          [-1, 128, 28, 28]               0\n",
      "          Conv2d-136          [-1, 128, 28, 28]          16,512\n",
      "     BatchNorm2d-137          [-1, 128, 28, 28]             256\n",
      "          Conv2d-138          [-1, 128, 28, 28]           1,280\n",
      "            GELU-139          [-1, 128, 28, 28]               0\n",
      "          Conv2d-140          [-1, 128, 28, 28]          16,512\n",
      "LocalIntegration-141          [-1, 128, 28, 28]               0\n",
      "     BatchNorm2d-142          [-1, 128, 28, 28]             256\n",
      "          Conv2d-143          [-1, 384, 28, 28]          49,152\n",
      "          Conv2d-144          [-1, 128, 28, 28]           1,280\n",
      "     BatchNorm2d-145          [-1, 128, 28, 28]             256\n",
      "            ReLU-146          [-1, 128, 28, 28]               0\n",
      "          Conv2d-147            [-1, 1, 28, 28]             128\n",
      "         Sigmoid-148            [-1, 1, 28, 28]               0\n",
      "SpatialOperation-149          [-1, 128, 28, 28]               0\n",
      "AdaptiveAvgPool2d-150            [-1, 128, 1, 1]               0\n",
      "          Conv2d-151            [-1, 128, 1, 1]          16,384\n",
      "         Sigmoid-152            [-1, 128, 1, 1]               0\n",
      "ChannelOperation-153          [-1, 128, 28, 28]               0\n",
      "          Conv2d-154          [-1, 128, 28, 28]           1,280\n",
      "     BatchNorm2d-155          [-1, 128, 28, 28]             256\n",
      "            ReLU-156          [-1, 128, 28, 28]               0\n",
      "          Conv2d-157            [-1, 1, 28, 28]             128\n",
      "         Sigmoid-158            [-1, 1, 28, 28]               0\n",
      "SpatialOperation-159          [-1, 128, 28, 28]               0\n",
      "AdaptiveAvgPool2d-160            [-1, 128, 1, 1]               0\n",
      "          Conv2d-161            [-1, 128, 1, 1]          16,384\n",
      "         Sigmoid-162            [-1, 128, 1, 1]               0\n",
      "ChannelOperation-163          [-1, 128, 28, 28]               0\n",
      "          Conv2d-164          [-1, 128, 28, 28]           1,280\n",
      "          Conv2d-165          [-1, 128, 28, 28]           1,280\n",
      "         Dropout-166          [-1, 128, 28, 28]               0\n",
      "AdditiveTokenMixer-167          [-1, 128, 28, 28]               0\n",
      "        Identity-168          [-1, 128, 28, 28]               0\n",
      "     BatchNorm2d-169          [-1, 128, 28, 28]             256\n",
      "          Conv2d-170          [-1, 512, 28, 28]          66,048\n",
      "            GELU-171          [-1, 512, 28, 28]               0\n",
      "         Dropout-172          [-1, 512, 28, 28]               0\n",
      "          Conv2d-173          [-1, 128, 28, 28]          65,664\n",
      "         Dropout-174          [-1, 128, 28, 28]               0\n",
      "             Mlp-175          [-1, 128, 28, 28]               0\n",
      "        Identity-176          [-1, 128, 28, 28]               0\n",
      "   AdditiveBlock-177          [-1, 128, 28, 28]               0\n",
      "          Conv2d-178          [-1, 128, 28, 28]          16,512\n",
      "     BatchNorm2d-179          [-1, 128, 28, 28]             256\n",
      "          Conv2d-180          [-1, 128, 28, 28]           1,280\n",
      "            GELU-181          [-1, 128, 28, 28]               0\n",
      "          Conv2d-182          [-1, 128, 28, 28]          16,512\n",
      "LocalIntegration-183          [-1, 128, 28, 28]               0\n",
      "     BatchNorm2d-184          [-1, 128, 28, 28]             256\n",
      "          Conv2d-185          [-1, 384, 28, 28]          49,152\n",
      "          Conv2d-186          [-1, 128, 28, 28]           1,280\n",
      "     BatchNorm2d-187          [-1, 128, 28, 28]             256\n",
      "            ReLU-188          [-1, 128, 28, 28]               0\n",
      "          Conv2d-189            [-1, 1, 28, 28]             128\n",
      "         Sigmoid-190            [-1, 1, 28, 28]               0\n",
      "SpatialOperation-191          [-1, 128, 28, 28]               0\n",
      "AdaptiveAvgPool2d-192            [-1, 128, 1, 1]               0\n",
      "          Conv2d-193            [-1, 128, 1, 1]          16,384\n",
      "         Sigmoid-194            [-1, 128, 1, 1]               0\n",
      "ChannelOperation-195          [-1, 128, 28, 28]               0\n",
      "          Conv2d-196          [-1, 128, 28, 28]           1,280\n",
      "     BatchNorm2d-197          [-1, 128, 28, 28]             256\n",
      "            ReLU-198          [-1, 128, 28, 28]               0\n",
      "          Conv2d-199            [-1, 1, 28, 28]             128\n",
      "         Sigmoid-200            [-1, 1, 28, 28]               0\n",
      "SpatialOperation-201          [-1, 128, 28, 28]               0\n",
      "AdaptiveAvgPool2d-202            [-1, 128, 1, 1]               0\n",
      "          Conv2d-203            [-1, 128, 1, 1]          16,384\n",
      "         Sigmoid-204            [-1, 128, 1, 1]               0\n",
      "ChannelOperation-205          [-1, 128, 28, 28]               0\n",
      "          Conv2d-206          [-1, 128, 28, 28]           1,280\n",
      "          Conv2d-207          [-1, 128, 28, 28]           1,280\n",
      "         Dropout-208          [-1, 128, 28, 28]               0\n",
      "AdditiveTokenMixer-209          [-1, 128, 28, 28]               0\n",
      "        Identity-210          [-1, 128, 28, 28]               0\n",
      "     BatchNorm2d-211          [-1, 128, 28, 28]             256\n",
      "          Conv2d-212          [-1, 512, 28, 28]          66,048\n",
      "            GELU-213          [-1, 512, 28, 28]               0\n",
      "         Dropout-214          [-1, 512, 28, 28]               0\n",
      "          Conv2d-215          [-1, 128, 28, 28]          65,664\n",
      "         Dropout-216          [-1, 128, 28, 28]               0\n",
      "             Mlp-217          [-1, 128, 28, 28]               0\n",
      "        Identity-218          [-1, 128, 28, 28]               0\n",
      "   AdditiveBlock-219          [-1, 128, 28, 28]               0\n",
      "          Conv2d-220          [-1, 128, 28, 28]          16,512\n",
      "     BatchNorm2d-221          [-1, 128, 28, 28]             256\n",
      "          Conv2d-222          [-1, 128, 28, 28]           1,280\n",
      "            GELU-223          [-1, 128, 28, 28]               0\n",
      "          Conv2d-224          [-1, 128, 28, 28]          16,512\n",
      "LocalIntegration-225          [-1, 128, 28, 28]               0\n",
      "     BatchNorm2d-226          [-1, 128, 28, 28]             256\n",
      "          Conv2d-227          [-1, 384, 28, 28]          49,152\n",
      "          Conv2d-228          [-1, 128, 28, 28]           1,280\n",
      "     BatchNorm2d-229          [-1, 128, 28, 28]             256\n",
      "            ReLU-230          [-1, 128, 28, 28]               0\n",
      "          Conv2d-231            [-1, 1, 28, 28]             128\n",
      "         Sigmoid-232            [-1, 1, 28, 28]               0\n",
      "SpatialOperation-233          [-1, 128, 28, 28]               0\n",
      "AdaptiveAvgPool2d-234            [-1, 128, 1, 1]               0\n",
      "          Conv2d-235            [-1, 128, 1, 1]          16,384\n",
      "         Sigmoid-236            [-1, 128, 1, 1]               0\n",
      "ChannelOperation-237          [-1, 128, 28, 28]               0\n",
      "          Conv2d-238          [-1, 128, 28, 28]           1,280\n",
      "     BatchNorm2d-239          [-1, 128, 28, 28]             256\n",
      "            ReLU-240          [-1, 128, 28, 28]               0\n",
      "          Conv2d-241            [-1, 1, 28, 28]             128\n",
      "         Sigmoid-242            [-1, 1, 28, 28]               0\n",
      "SpatialOperation-243          [-1, 128, 28, 28]               0\n",
      "AdaptiveAvgPool2d-244            [-1, 128, 1, 1]               0\n",
      "          Conv2d-245            [-1, 128, 1, 1]          16,384\n",
      "         Sigmoid-246            [-1, 128, 1, 1]               0\n",
      "ChannelOperation-247          [-1, 128, 28, 28]               0\n",
      "          Conv2d-248          [-1, 128, 28, 28]           1,280\n",
      "          Conv2d-249          [-1, 128, 28, 28]           1,280\n",
      "         Dropout-250          [-1, 128, 28, 28]               0\n",
      "AdditiveTokenMixer-251          [-1, 128, 28, 28]               0\n",
      "        Identity-252          [-1, 128, 28, 28]               0\n",
      "     BatchNorm2d-253          [-1, 128, 28, 28]             256\n",
      "          Conv2d-254          [-1, 512, 28, 28]          66,048\n",
      "            GELU-255          [-1, 512, 28, 28]               0\n",
      "         Dropout-256          [-1, 512, 28, 28]               0\n",
      "          Conv2d-257          [-1, 128, 28, 28]          65,664\n",
      "         Dropout-258          [-1, 128, 28, 28]               0\n",
      "             Mlp-259          [-1, 128, 28, 28]               0\n",
      "        Identity-260          [-1, 128, 28, 28]               0\n",
      "   AdditiveBlock-261          [-1, 128, 28, 28]               0\n",
      "          Conv2d-262          [-1, 256, 14, 14]         295,168\n",
      "     BatchNorm2d-263          [-1, 256, 14, 14]             512\n",
      "       Embedding-264          [-1, 256, 14, 14]               0\n",
      "          Conv2d-265          [-1, 256, 14, 14]          65,792\n",
      "     BatchNorm2d-266          [-1, 256, 14, 14]             512\n",
      "          Conv2d-267          [-1, 256, 14, 14]           2,560\n",
      "            GELU-268          [-1, 256, 14, 14]               0\n",
      "          Conv2d-269          [-1, 256, 14, 14]          65,792\n",
      "LocalIntegration-270          [-1, 256, 14, 14]               0\n",
      "     BatchNorm2d-271          [-1, 256, 14, 14]             512\n",
      "          Conv2d-272          [-1, 768, 14, 14]         196,608\n",
      "          Conv2d-273          [-1, 256, 14, 14]           2,560\n",
      "     BatchNorm2d-274          [-1, 256, 14, 14]             512\n",
      "            ReLU-275          [-1, 256, 14, 14]               0\n",
      "          Conv2d-276            [-1, 1, 14, 14]             256\n",
      "         Sigmoid-277            [-1, 1, 14, 14]               0\n",
      "SpatialOperation-278          [-1, 256, 14, 14]               0\n",
      "AdaptiveAvgPool2d-279            [-1, 256, 1, 1]               0\n",
      "          Conv2d-280            [-1, 256, 1, 1]          65,536\n",
      "         Sigmoid-281            [-1, 256, 1, 1]               0\n",
      "ChannelOperation-282          [-1, 256, 14, 14]               0\n",
      "          Conv2d-283          [-1, 256, 14, 14]           2,560\n",
      "     BatchNorm2d-284          [-1, 256, 14, 14]             512\n",
      "            ReLU-285          [-1, 256, 14, 14]               0\n",
      "          Conv2d-286            [-1, 1, 14, 14]             256\n",
      "         Sigmoid-287            [-1, 1, 14, 14]               0\n",
      "SpatialOperation-288          [-1, 256, 14, 14]               0\n",
      "AdaptiveAvgPool2d-289            [-1, 256, 1, 1]               0\n",
      "          Conv2d-290            [-1, 256, 1, 1]          65,536\n",
      "         Sigmoid-291            [-1, 256, 1, 1]               0\n",
      "ChannelOperation-292          [-1, 256, 14, 14]               0\n",
      "          Conv2d-293          [-1, 256, 14, 14]           2,560\n",
      "          Conv2d-294          [-1, 256, 14, 14]           2,560\n",
      "         Dropout-295          [-1, 256, 14, 14]               0\n",
      "AdditiveTokenMixer-296          [-1, 256, 14, 14]               0\n",
      "        Identity-297          [-1, 256, 14, 14]               0\n",
      "     BatchNorm2d-298          [-1, 256, 14, 14]             512\n",
      "          Conv2d-299         [-1, 1024, 14, 14]         263,168\n",
      "            GELU-300         [-1, 1024, 14, 14]               0\n",
      "         Dropout-301         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-302          [-1, 256, 14, 14]         262,400\n",
      "         Dropout-303          [-1, 256, 14, 14]               0\n",
      "             Mlp-304          [-1, 256, 14, 14]               0\n",
      "        Identity-305          [-1, 256, 14, 14]               0\n",
      "   AdditiveBlock-306          [-1, 256, 14, 14]               0\n",
      "          Conv2d-307          [-1, 256, 14, 14]          65,792\n",
      "     BatchNorm2d-308          [-1, 256, 14, 14]             512\n",
      "          Conv2d-309          [-1, 256, 14, 14]           2,560\n",
      "            GELU-310          [-1, 256, 14, 14]               0\n",
      "          Conv2d-311          [-1, 256, 14, 14]          65,792\n",
      "LocalIntegration-312          [-1, 256, 14, 14]               0\n",
      "     BatchNorm2d-313          [-1, 256, 14, 14]             512\n",
      "          Conv2d-314          [-1, 768, 14, 14]         196,608\n",
      "          Conv2d-315          [-1, 256, 14, 14]           2,560\n",
      "     BatchNorm2d-316          [-1, 256, 14, 14]             512\n",
      "            ReLU-317          [-1, 256, 14, 14]               0\n",
      "          Conv2d-318            [-1, 1, 14, 14]             256\n",
      "         Sigmoid-319            [-1, 1, 14, 14]               0\n",
      "SpatialOperation-320          [-1, 256, 14, 14]               0\n",
      "AdaptiveAvgPool2d-321            [-1, 256, 1, 1]               0\n",
      "          Conv2d-322            [-1, 256, 1, 1]          65,536\n",
      "         Sigmoid-323            [-1, 256, 1, 1]               0\n",
      "ChannelOperation-324          [-1, 256, 14, 14]               0\n",
      "          Conv2d-325          [-1, 256, 14, 14]           2,560\n",
      "     BatchNorm2d-326          [-1, 256, 14, 14]             512\n",
      "            ReLU-327          [-1, 256, 14, 14]               0\n",
      "          Conv2d-328            [-1, 1, 14, 14]             256\n",
      "         Sigmoid-329            [-1, 1, 14, 14]               0\n",
      "SpatialOperation-330          [-1, 256, 14, 14]               0\n",
      "AdaptiveAvgPool2d-331            [-1, 256, 1, 1]               0\n",
      "          Conv2d-332            [-1, 256, 1, 1]          65,536\n",
      "         Sigmoid-333            [-1, 256, 1, 1]               0\n",
      "ChannelOperation-334          [-1, 256, 14, 14]               0\n",
      "          Conv2d-335          [-1, 256, 14, 14]           2,560\n",
      "          Conv2d-336          [-1, 256, 14, 14]           2,560\n",
      "         Dropout-337          [-1, 256, 14, 14]               0\n",
      "AdditiveTokenMixer-338          [-1, 256, 14, 14]               0\n",
      "        Identity-339          [-1, 256, 14, 14]               0\n",
      "     BatchNorm2d-340          [-1, 256, 14, 14]             512\n",
      "          Conv2d-341         [-1, 1024, 14, 14]         263,168\n",
      "            GELU-342         [-1, 1024, 14, 14]               0\n",
      "         Dropout-343         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-344          [-1, 256, 14, 14]         262,400\n",
      "         Dropout-345          [-1, 256, 14, 14]               0\n",
      "             Mlp-346          [-1, 256, 14, 14]               0\n",
      "        Identity-347          [-1, 256, 14, 14]               0\n",
      "   AdditiveBlock-348          [-1, 256, 14, 14]               0\n",
      "          Conv2d-349          [-1, 256, 14, 14]          65,792\n",
      "     BatchNorm2d-350          [-1, 256, 14, 14]             512\n",
      "          Conv2d-351          [-1, 256, 14, 14]           2,560\n",
      "            GELU-352          [-1, 256, 14, 14]               0\n",
      "          Conv2d-353          [-1, 256, 14, 14]          65,792\n",
      "LocalIntegration-354          [-1, 256, 14, 14]               0\n",
      "     BatchNorm2d-355          [-1, 256, 14, 14]             512\n",
      "          Conv2d-356          [-1, 768, 14, 14]         196,608\n",
      "          Conv2d-357          [-1, 256, 14, 14]           2,560\n",
      "     BatchNorm2d-358          [-1, 256, 14, 14]             512\n",
      "            ReLU-359          [-1, 256, 14, 14]               0\n",
      "          Conv2d-360            [-1, 1, 14, 14]             256\n",
      "         Sigmoid-361            [-1, 1, 14, 14]               0\n",
      "SpatialOperation-362          [-1, 256, 14, 14]               0\n",
      "AdaptiveAvgPool2d-363            [-1, 256, 1, 1]               0\n",
      "          Conv2d-364            [-1, 256, 1, 1]          65,536\n",
      "         Sigmoid-365            [-1, 256, 1, 1]               0\n",
      "ChannelOperation-366          [-1, 256, 14, 14]               0\n",
      "          Conv2d-367          [-1, 256, 14, 14]           2,560\n",
      "     BatchNorm2d-368          [-1, 256, 14, 14]             512\n",
      "            ReLU-369          [-1, 256, 14, 14]               0\n",
      "          Conv2d-370            [-1, 1, 14, 14]             256\n",
      "         Sigmoid-371            [-1, 1, 14, 14]               0\n",
      "SpatialOperation-372          [-1, 256, 14, 14]               0\n",
      "AdaptiveAvgPool2d-373            [-1, 256, 1, 1]               0\n",
      "          Conv2d-374            [-1, 256, 1, 1]          65,536\n",
      "         Sigmoid-375            [-1, 256, 1, 1]               0\n",
      "ChannelOperation-376          [-1, 256, 14, 14]               0\n",
      "          Conv2d-377          [-1, 256, 14, 14]           2,560\n",
      "          Conv2d-378          [-1, 256, 14, 14]           2,560\n",
      "         Dropout-379          [-1, 256, 14, 14]               0\n",
      "AdditiveTokenMixer-380          [-1, 256, 14, 14]               0\n",
      "        Identity-381          [-1, 256, 14, 14]               0\n",
      "     BatchNorm2d-382          [-1, 256, 14, 14]             512\n",
      "          Conv2d-383         [-1, 1024, 14, 14]         263,168\n",
      "            GELU-384         [-1, 1024, 14, 14]               0\n",
      "         Dropout-385         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-386          [-1, 256, 14, 14]         262,400\n",
      "         Dropout-387          [-1, 256, 14, 14]               0\n",
      "             Mlp-388          [-1, 256, 14, 14]               0\n",
      "        Identity-389          [-1, 256, 14, 14]               0\n",
      "   AdditiveBlock-390          [-1, 256, 14, 14]               0\n",
      "          Conv2d-391          [-1, 256, 14, 14]          65,792\n",
      "     BatchNorm2d-392          [-1, 256, 14, 14]             512\n",
      "          Conv2d-393          [-1, 256, 14, 14]           2,560\n",
      "            GELU-394          [-1, 256, 14, 14]               0\n",
      "          Conv2d-395          [-1, 256, 14, 14]          65,792\n",
      "LocalIntegration-396          [-1, 256, 14, 14]               0\n",
      "     BatchNorm2d-397          [-1, 256, 14, 14]             512\n",
      "          Conv2d-398          [-1, 768, 14, 14]         196,608\n",
      "          Conv2d-399          [-1, 256, 14, 14]           2,560\n",
      "     BatchNorm2d-400          [-1, 256, 14, 14]             512\n",
      "            ReLU-401          [-1, 256, 14, 14]               0\n",
      "          Conv2d-402            [-1, 1, 14, 14]             256\n",
      "         Sigmoid-403            [-1, 1, 14, 14]               0\n",
      "SpatialOperation-404          [-1, 256, 14, 14]               0\n",
      "AdaptiveAvgPool2d-405            [-1, 256, 1, 1]               0\n",
      "          Conv2d-406            [-1, 256, 1, 1]          65,536\n",
      "         Sigmoid-407            [-1, 256, 1, 1]               0\n",
      "ChannelOperation-408          [-1, 256, 14, 14]               0\n",
      "          Conv2d-409          [-1, 256, 14, 14]           2,560\n",
      "     BatchNorm2d-410          [-1, 256, 14, 14]             512\n",
      "            ReLU-411          [-1, 256, 14, 14]               0\n",
      "          Conv2d-412            [-1, 1, 14, 14]             256\n",
      "         Sigmoid-413            [-1, 1, 14, 14]               0\n",
      "SpatialOperation-414          [-1, 256, 14, 14]               0\n",
      "AdaptiveAvgPool2d-415            [-1, 256, 1, 1]               0\n",
      "          Conv2d-416            [-1, 256, 1, 1]          65,536\n",
      "         Sigmoid-417            [-1, 256, 1, 1]               0\n",
      "ChannelOperation-418          [-1, 256, 14, 14]               0\n",
      "          Conv2d-419          [-1, 256, 14, 14]           2,560\n",
      "          Conv2d-420          [-1, 256, 14, 14]           2,560\n",
      "         Dropout-421          [-1, 256, 14, 14]               0\n",
      "AdditiveTokenMixer-422          [-1, 256, 14, 14]               0\n",
      "        Identity-423          [-1, 256, 14, 14]               0\n",
      "     BatchNorm2d-424          [-1, 256, 14, 14]             512\n",
      "          Conv2d-425         [-1, 1024, 14, 14]         263,168\n",
      "            GELU-426         [-1, 1024, 14, 14]               0\n",
      "         Dropout-427         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-428          [-1, 256, 14, 14]         262,400\n",
      "         Dropout-429          [-1, 256, 14, 14]               0\n",
      "             Mlp-430          [-1, 256, 14, 14]               0\n",
      "        Identity-431          [-1, 256, 14, 14]               0\n",
      "   AdditiveBlock-432          [-1, 256, 14, 14]               0\n",
      "          Conv2d-433          [-1, 256, 14, 14]          65,792\n",
      "     BatchNorm2d-434          [-1, 256, 14, 14]             512\n",
      "          Conv2d-435          [-1, 256, 14, 14]           2,560\n",
      "            GELU-436          [-1, 256, 14, 14]               0\n",
      "          Conv2d-437          [-1, 256, 14, 14]          65,792\n",
      "LocalIntegration-438          [-1, 256, 14, 14]               0\n",
      "     BatchNorm2d-439          [-1, 256, 14, 14]             512\n",
      "          Conv2d-440          [-1, 768, 14, 14]         196,608\n",
      "          Conv2d-441          [-1, 256, 14, 14]           2,560\n",
      "     BatchNorm2d-442          [-1, 256, 14, 14]             512\n",
      "            ReLU-443          [-1, 256, 14, 14]               0\n",
      "          Conv2d-444            [-1, 1, 14, 14]             256\n",
      "         Sigmoid-445            [-1, 1, 14, 14]               0\n",
      "SpatialOperation-446          [-1, 256, 14, 14]               0\n",
      "AdaptiveAvgPool2d-447            [-1, 256, 1, 1]               0\n",
      "          Conv2d-448            [-1, 256, 1, 1]          65,536\n",
      "         Sigmoid-449            [-1, 256, 1, 1]               0\n",
      "ChannelOperation-450          [-1, 256, 14, 14]               0\n",
      "          Conv2d-451          [-1, 256, 14, 14]           2,560\n",
      "     BatchNorm2d-452          [-1, 256, 14, 14]             512\n",
      "            ReLU-453          [-1, 256, 14, 14]               0\n",
      "          Conv2d-454            [-1, 1, 14, 14]             256\n",
      "         Sigmoid-455            [-1, 1, 14, 14]               0\n",
      "SpatialOperation-456          [-1, 256, 14, 14]               0\n",
      "AdaptiveAvgPool2d-457            [-1, 256, 1, 1]               0\n",
      "          Conv2d-458            [-1, 256, 1, 1]          65,536\n",
      "         Sigmoid-459            [-1, 256, 1, 1]               0\n",
      "ChannelOperation-460          [-1, 256, 14, 14]               0\n",
      "          Conv2d-461          [-1, 256, 14, 14]           2,560\n",
      "          Conv2d-462          [-1, 256, 14, 14]           2,560\n",
      "         Dropout-463          [-1, 256, 14, 14]               0\n",
      "AdditiveTokenMixer-464          [-1, 256, 14, 14]               0\n",
      "        Identity-465          [-1, 256, 14, 14]               0\n",
      "     BatchNorm2d-466          [-1, 256, 14, 14]             512\n",
      "          Conv2d-467         [-1, 1024, 14, 14]         263,168\n",
      "            GELU-468         [-1, 1024, 14, 14]               0\n",
      "         Dropout-469         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-470          [-1, 256, 14, 14]         262,400\n",
      "         Dropout-471          [-1, 256, 14, 14]               0\n",
      "             Mlp-472          [-1, 256, 14, 14]               0\n",
      "        Identity-473          [-1, 256, 14, 14]               0\n",
      "   AdditiveBlock-474          [-1, 256, 14, 14]               0\n",
      "          Conv2d-475          [-1, 256, 14, 14]          65,792\n",
      "     BatchNorm2d-476          [-1, 256, 14, 14]             512\n",
      "          Conv2d-477          [-1, 256, 14, 14]           2,560\n",
      "            GELU-478          [-1, 256, 14, 14]               0\n",
      "          Conv2d-479          [-1, 256, 14, 14]          65,792\n",
      "LocalIntegration-480          [-1, 256, 14, 14]               0\n",
      "     BatchNorm2d-481          [-1, 256, 14, 14]             512\n",
      "          Conv2d-482          [-1, 768, 14, 14]         196,608\n",
      "          Conv2d-483          [-1, 256, 14, 14]           2,560\n",
      "     BatchNorm2d-484          [-1, 256, 14, 14]             512\n",
      "            ReLU-485          [-1, 256, 14, 14]               0\n",
      "          Conv2d-486            [-1, 1, 14, 14]             256\n",
      "         Sigmoid-487            [-1, 1, 14, 14]               0\n",
      "SpatialOperation-488          [-1, 256, 14, 14]               0\n",
      "AdaptiveAvgPool2d-489            [-1, 256, 1, 1]               0\n",
      "          Conv2d-490            [-1, 256, 1, 1]          65,536\n",
      "         Sigmoid-491            [-1, 256, 1, 1]               0\n",
      "ChannelOperation-492          [-1, 256, 14, 14]               0\n",
      "          Conv2d-493          [-1, 256, 14, 14]           2,560\n",
      "     BatchNorm2d-494          [-1, 256, 14, 14]             512\n",
      "            ReLU-495          [-1, 256, 14, 14]               0\n",
      "          Conv2d-496            [-1, 1, 14, 14]             256\n",
      "         Sigmoid-497            [-1, 1, 14, 14]               0\n",
      "SpatialOperation-498          [-1, 256, 14, 14]               0\n",
      "AdaptiveAvgPool2d-499            [-1, 256, 1, 1]               0\n",
      "          Conv2d-500            [-1, 256, 1, 1]          65,536\n",
      "         Sigmoid-501            [-1, 256, 1, 1]               0\n",
      "ChannelOperation-502          [-1, 256, 14, 14]               0\n",
      "          Conv2d-503          [-1, 256, 14, 14]           2,560\n",
      "          Conv2d-504          [-1, 256, 14, 14]           2,560\n",
      "         Dropout-505          [-1, 256, 14, 14]               0\n",
      "AdditiveTokenMixer-506          [-1, 256, 14, 14]               0\n",
      "        Identity-507          [-1, 256, 14, 14]               0\n",
      "     BatchNorm2d-508          [-1, 256, 14, 14]             512\n",
      "          Conv2d-509         [-1, 1024, 14, 14]         263,168\n",
      "            GELU-510         [-1, 1024, 14, 14]               0\n",
      "         Dropout-511         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-512          [-1, 256, 14, 14]         262,400\n",
      "         Dropout-513          [-1, 256, 14, 14]               0\n",
      "             Mlp-514          [-1, 256, 14, 14]               0\n",
      "        Identity-515          [-1, 256, 14, 14]               0\n",
      "   AdditiveBlock-516          [-1, 256, 14, 14]               0\n",
      "          Conv2d-517            [-1, 512, 7, 7]       1,180,160\n",
      "     BatchNorm2d-518            [-1, 512, 7, 7]           1,024\n",
      "       Embedding-519            [-1, 512, 7, 7]               0\n",
      "          Conv2d-520            [-1, 512, 7, 7]         262,656\n",
      "     BatchNorm2d-521            [-1, 512, 7, 7]           1,024\n",
      "          Conv2d-522            [-1, 512, 7, 7]           5,120\n",
      "            GELU-523            [-1, 512, 7, 7]               0\n",
      "          Conv2d-524            [-1, 512, 7, 7]         262,656\n",
      "LocalIntegration-525            [-1, 512, 7, 7]               0\n",
      "     BatchNorm2d-526            [-1, 512, 7, 7]           1,024\n",
      "          Conv2d-527           [-1, 1536, 7, 7]         786,432\n",
      "          Conv2d-528            [-1, 512, 7, 7]           5,120\n",
      "     BatchNorm2d-529            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-530            [-1, 512, 7, 7]               0\n",
      "          Conv2d-531              [-1, 1, 7, 7]             512\n",
      "         Sigmoid-532              [-1, 1, 7, 7]               0\n",
      "SpatialOperation-533            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-534            [-1, 512, 1, 1]               0\n",
      "          Conv2d-535            [-1, 512, 1, 1]         262,144\n",
      "         Sigmoid-536            [-1, 512, 1, 1]               0\n",
      "ChannelOperation-537            [-1, 512, 7, 7]               0\n",
      "          Conv2d-538            [-1, 512, 7, 7]           5,120\n",
      "     BatchNorm2d-539            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-540            [-1, 512, 7, 7]               0\n",
      "          Conv2d-541              [-1, 1, 7, 7]             512\n",
      "         Sigmoid-542              [-1, 1, 7, 7]               0\n",
      "SpatialOperation-543            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-544            [-1, 512, 1, 1]               0\n",
      "          Conv2d-545            [-1, 512, 1, 1]         262,144\n",
      "         Sigmoid-546            [-1, 512, 1, 1]               0\n",
      "ChannelOperation-547            [-1, 512, 7, 7]               0\n",
      "          Conv2d-548            [-1, 512, 7, 7]           5,120\n",
      "          Conv2d-549            [-1, 512, 7, 7]           5,120\n",
      "         Dropout-550            [-1, 512, 7, 7]               0\n",
      "AdditiveTokenMixer-551            [-1, 512, 7, 7]               0\n",
      "        Identity-552            [-1, 512, 7, 7]               0\n",
      "     BatchNorm2d-553            [-1, 512, 7, 7]           1,024\n",
      "          Conv2d-554           [-1, 2048, 7, 7]       1,050,624\n",
      "            GELU-555           [-1, 2048, 7, 7]               0\n",
      "         Dropout-556           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-557            [-1, 512, 7, 7]       1,049,088\n",
      "         Dropout-558            [-1, 512, 7, 7]               0\n",
      "             Mlp-559            [-1, 512, 7, 7]               0\n",
      "        Identity-560            [-1, 512, 7, 7]               0\n",
      "   AdditiveBlock-561            [-1, 512, 7, 7]               0\n",
      "          Conv2d-562            [-1, 512, 7, 7]         262,656\n",
      "     BatchNorm2d-563            [-1, 512, 7, 7]           1,024\n",
      "          Conv2d-564            [-1, 512, 7, 7]           5,120\n",
      "            GELU-565            [-1, 512, 7, 7]               0\n",
      "          Conv2d-566            [-1, 512, 7, 7]         262,656\n",
      "LocalIntegration-567            [-1, 512, 7, 7]               0\n",
      "     BatchNorm2d-568            [-1, 512, 7, 7]           1,024\n",
      "          Conv2d-569           [-1, 1536, 7, 7]         786,432\n",
      "          Conv2d-570            [-1, 512, 7, 7]           5,120\n",
      "     BatchNorm2d-571            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-572            [-1, 512, 7, 7]               0\n",
      "          Conv2d-573              [-1, 1, 7, 7]             512\n",
      "         Sigmoid-574              [-1, 1, 7, 7]               0\n",
      "SpatialOperation-575            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-576            [-1, 512, 1, 1]               0\n",
      "          Conv2d-577            [-1, 512, 1, 1]         262,144\n",
      "         Sigmoid-578            [-1, 512, 1, 1]               0\n",
      "ChannelOperation-579            [-1, 512, 7, 7]               0\n",
      "          Conv2d-580            [-1, 512, 7, 7]           5,120\n",
      "     BatchNorm2d-581            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-582            [-1, 512, 7, 7]               0\n",
      "          Conv2d-583              [-1, 1, 7, 7]             512\n",
      "         Sigmoid-584              [-1, 1, 7, 7]               0\n",
      "SpatialOperation-585            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-586            [-1, 512, 1, 1]               0\n",
      "          Conv2d-587            [-1, 512, 1, 1]         262,144\n",
      "         Sigmoid-588            [-1, 512, 1, 1]               0\n",
      "ChannelOperation-589            [-1, 512, 7, 7]               0\n",
      "          Conv2d-590            [-1, 512, 7, 7]           5,120\n",
      "          Conv2d-591            [-1, 512, 7, 7]           5,120\n",
      "         Dropout-592            [-1, 512, 7, 7]               0\n",
      "AdditiveTokenMixer-593            [-1, 512, 7, 7]               0\n",
      "        Identity-594            [-1, 512, 7, 7]               0\n",
      "     BatchNorm2d-595            [-1, 512, 7, 7]           1,024\n",
      "          Conv2d-596           [-1, 2048, 7, 7]       1,050,624\n",
      "            GELU-597           [-1, 2048, 7, 7]               0\n",
      "         Dropout-598           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-599            [-1, 512, 7, 7]       1,049,088\n",
      "         Dropout-600            [-1, 512, 7, 7]               0\n",
      "             Mlp-601            [-1, 512, 7, 7]               0\n",
      "        Identity-602            [-1, 512, 7, 7]               0\n",
      "   AdditiveBlock-603            [-1, 512, 7, 7]               0\n",
      "          Conv2d-604            [-1, 512, 7, 7]         262,656\n",
      "     BatchNorm2d-605            [-1, 512, 7, 7]           1,024\n",
      "          Conv2d-606            [-1, 512, 7, 7]           5,120\n",
      "            GELU-607            [-1, 512, 7, 7]               0\n",
      "          Conv2d-608            [-1, 512, 7, 7]         262,656\n",
      "LocalIntegration-609            [-1, 512, 7, 7]               0\n",
      "     BatchNorm2d-610            [-1, 512, 7, 7]           1,024\n",
      "          Conv2d-611           [-1, 1536, 7, 7]         786,432\n",
      "          Conv2d-612            [-1, 512, 7, 7]           5,120\n",
      "     BatchNorm2d-613            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-614            [-1, 512, 7, 7]               0\n",
      "          Conv2d-615              [-1, 1, 7, 7]             512\n",
      "         Sigmoid-616              [-1, 1, 7, 7]               0\n",
      "SpatialOperation-617            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-618            [-1, 512, 1, 1]               0\n",
      "          Conv2d-619            [-1, 512, 1, 1]         262,144\n",
      "         Sigmoid-620            [-1, 512, 1, 1]               0\n",
      "ChannelOperation-621            [-1, 512, 7, 7]               0\n",
      "          Conv2d-622            [-1, 512, 7, 7]           5,120\n",
      "     BatchNorm2d-623            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-624            [-1, 512, 7, 7]               0\n",
      "          Conv2d-625              [-1, 1, 7, 7]             512\n",
      "         Sigmoid-626              [-1, 1, 7, 7]               0\n",
      "SpatialOperation-627            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-628            [-1, 512, 1, 1]               0\n",
      "          Conv2d-629            [-1, 512, 1, 1]         262,144\n",
      "         Sigmoid-630            [-1, 512, 1, 1]               0\n",
      "ChannelOperation-631            [-1, 512, 7, 7]               0\n",
      "          Conv2d-632            [-1, 512, 7, 7]           5,120\n",
      "          Conv2d-633            [-1, 512, 7, 7]           5,120\n",
      "         Dropout-634            [-1, 512, 7, 7]               0\n",
      "AdditiveTokenMixer-635            [-1, 512, 7, 7]               0\n",
      "        Identity-636            [-1, 512, 7, 7]               0\n",
      "     BatchNorm2d-637            [-1, 512, 7, 7]           1,024\n",
      "          Conv2d-638           [-1, 2048, 7, 7]       1,050,624\n",
      "            GELU-639           [-1, 2048, 7, 7]               0\n",
      "         Dropout-640           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-641            [-1, 512, 7, 7]       1,049,088\n",
      "         Dropout-642            [-1, 512, 7, 7]               0\n",
      "             Mlp-643            [-1, 512, 7, 7]               0\n",
      "        Identity-644            [-1, 512, 7, 7]               0\n",
      "   AdditiveBlock-645            [-1, 512, 7, 7]               0\n",
      "          Conv2d-646            [-1, 512, 4, 4]       2,359,808\n",
      "     BatchNorm2d-647            [-1, 512, 4, 4]           1,024\n",
      "       Embedding-648            [-1, 512, 4, 4]               0\n",
      "          Conv2d-649            [-1, 512, 4, 4]         262,656\n",
      "     BatchNorm2d-650            [-1, 512, 4, 4]           1,024\n",
      "          Conv2d-651            [-1, 512, 4, 4]           5,120\n",
      "            GELU-652            [-1, 512, 4, 4]               0\n",
      "          Conv2d-653            [-1, 512, 4, 4]         262,656\n",
      "LocalIntegration-654            [-1, 512, 4, 4]               0\n",
      "     BatchNorm2d-655            [-1, 512, 4, 4]           1,024\n",
      "          Conv2d-656           [-1, 1536, 4, 4]         786,432\n",
      "          Conv2d-657            [-1, 512, 4, 4]           5,120\n",
      "     BatchNorm2d-658            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-659            [-1, 512, 4, 4]               0\n",
      "          Conv2d-660              [-1, 1, 4, 4]             512\n",
      "         Sigmoid-661              [-1, 1, 4, 4]               0\n",
      "SpatialOperation-662            [-1, 512, 4, 4]               0\n",
      "AdaptiveAvgPool2d-663            [-1, 512, 1, 1]               0\n",
      "          Conv2d-664            [-1, 512, 1, 1]         262,144\n",
      "         Sigmoid-665            [-1, 512, 1, 1]               0\n",
      "ChannelOperation-666            [-1, 512, 4, 4]               0\n",
      "          Conv2d-667            [-1, 512, 4, 4]           5,120\n",
      "     BatchNorm2d-668            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-669            [-1, 512, 4, 4]               0\n",
      "          Conv2d-670              [-1, 1, 4, 4]             512\n",
      "         Sigmoid-671              [-1, 1, 4, 4]               0\n",
      "SpatialOperation-672            [-1, 512, 4, 4]               0\n",
      "AdaptiveAvgPool2d-673            [-1, 512, 1, 1]               0\n",
      "          Conv2d-674            [-1, 512, 1, 1]         262,144\n",
      "         Sigmoid-675            [-1, 512, 1, 1]               0\n",
      "ChannelOperation-676            [-1, 512, 4, 4]               0\n",
      "          Conv2d-677            [-1, 512, 4, 4]           5,120\n",
      "          Conv2d-678            [-1, 512, 4, 4]           5,120\n",
      "         Dropout-679            [-1, 512, 4, 4]               0\n",
      "AdditiveTokenMixer-680            [-1, 512, 4, 4]               0\n",
      "        Identity-681            [-1, 512, 4, 4]               0\n",
      "     BatchNorm2d-682            [-1, 512, 4, 4]           1,024\n",
      "          Conv2d-683           [-1, 2048, 4, 4]       1,050,624\n",
      "            GELU-684           [-1, 2048, 4, 4]               0\n",
      "         Dropout-685           [-1, 2048, 4, 4]               0\n",
      "          Conv2d-686            [-1, 512, 4, 4]       1,049,088\n",
      "         Dropout-687            [-1, 512, 4, 4]               0\n",
      "             Mlp-688            [-1, 512, 4, 4]               0\n",
      "        Identity-689            [-1, 512, 4, 4]               0\n",
      "   AdditiveBlock-690            [-1, 512, 4, 4]               0\n",
      "          Conv2d-691            [-1, 512, 4, 4]         262,656\n",
      "     BatchNorm2d-692            [-1, 512, 4, 4]           1,024\n",
      "          Conv2d-693            [-1, 512, 4, 4]           5,120\n",
      "            GELU-694            [-1, 512, 4, 4]               0\n",
      "          Conv2d-695            [-1, 512, 4, 4]         262,656\n",
      "LocalIntegration-696            [-1, 512, 4, 4]               0\n",
      "     BatchNorm2d-697            [-1, 512, 4, 4]           1,024\n",
      "          Conv2d-698           [-1, 1536, 4, 4]         786,432\n",
      "          Conv2d-699            [-1, 512, 4, 4]           5,120\n",
      "     BatchNorm2d-700            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-701            [-1, 512, 4, 4]               0\n",
      "          Conv2d-702              [-1, 1, 4, 4]             512\n",
      "         Sigmoid-703              [-1, 1, 4, 4]               0\n",
      "SpatialOperation-704            [-1, 512, 4, 4]               0\n",
      "AdaptiveAvgPool2d-705            [-1, 512, 1, 1]               0\n",
      "          Conv2d-706            [-1, 512, 1, 1]         262,144\n",
      "         Sigmoid-707            [-1, 512, 1, 1]               0\n",
      "ChannelOperation-708            [-1, 512, 4, 4]               0\n",
      "          Conv2d-709            [-1, 512, 4, 4]           5,120\n",
      "     BatchNorm2d-710            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-711            [-1, 512, 4, 4]               0\n",
      "          Conv2d-712              [-1, 1, 4, 4]             512\n",
      "         Sigmoid-713              [-1, 1, 4, 4]               0\n",
      "SpatialOperation-714            [-1, 512, 4, 4]               0\n",
      "AdaptiveAvgPool2d-715            [-1, 512, 1, 1]               0\n",
      "          Conv2d-716            [-1, 512, 1, 1]         262,144\n",
      "         Sigmoid-717            [-1, 512, 1, 1]               0\n",
      "ChannelOperation-718            [-1, 512, 4, 4]               0\n",
      "          Conv2d-719            [-1, 512, 4, 4]           5,120\n",
      "          Conv2d-720            [-1, 512, 4, 4]           5,120\n",
      "         Dropout-721            [-1, 512, 4, 4]               0\n",
      "AdditiveTokenMixer-722            [-1, 512, 4, 4]               0\n",
      "        Identity-723            [-1, 512, 4, 4]               0\n",
      "     BatchNorm2d-724            [-1, 512, 4, 4]           1,024\n",
      "          Conv2d-725           [-1, 2048, 4, 4]       1,050,624\n",
      "            GELU-726           [-1, 2048, 4, 4]               0\n",
      "         Dropout-727           [-1, 2048, 4, 4]               0\n",
      "          Conv2d-728            [-1, 512, 4, 4]       1,049,088\n",
      "         Dropout-729            [-1, 512, 4, 4]               0\n",
      "             Mlp-730            [-1, 512, 4, 4]               0\n",
      "        Identity-731            [-1, 512, 4, 4]               0\n",
      "   AdditiveBlock-732            [-1, 512, 4, 4]               0\n",
      "     BatchNorm2d-733            [-1, 512, 4, 4]           1,024\n",
      "          Linear-734                   [-1, 64]          32,832\n",
      "================================================================\n",
      "Total params: 31,065,760\n",
      "Trainable params: 31,065,760\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 549.45\n",
      "Params size (MB): 118.51\n",
      "Estimated Total Size (MB): 668.54\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "modelFp32.eval()\n",
    "modelFp32.cuda()\n",
    "\n",
    "\n",
    "torchsummary.summary(modelFp32.cuda(), (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = input_getter.local_image_getter(\"/home/centar15-desktop1/LPCV_2025_T1/datasets/imagenet/train/banana/0000316013.jpeg\")\n",
    "input_tensor = input.get_input_torch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1998e-01, -1.4891e+00, -3.6274e-01, -2.6878e-02, -5.1499e-01,\n",
       "         -5.9988e-01, -1.1006e+00, -1.1575e+00,  8.0365e-01,  2.1609e-01,\n",
       "          7.6623e-01, -3.3292e-01, -5.7637e-01, -4.9860e-01, -2.2946e+00,\n",
       "         -1.7250e-01,  2.1389e-01,  3.1646e-01, -1.9605e-01, -4.6308e-01,\n",
       "         -1.5258e-01,  1.7325e-01, -1.9079e-01, -2.0852e-01,  3.8172e-01,\n",
       "          4.3666e-01, -4.4710e-01,  5.5896e-01,  2.9313e-01, -4.2712e-01,\n",
       "         -3.3304e-03, -9.6425e-01, -1.2692e-01, -2.6702e-01, -1.7247e-01,\n",
       "          6.6138e+00, -4.8179e-01, -1.6574e-01, -5.4325e-02,  1.2667e-01,\n",
       "         -4.4822e-01,  9.3577e-02, -7.3777e-01, -3.2085e-01, -2.4339e-01,\n",
       "          1.0207e-02, -6.3629e-01,  9.7662e-03, -7.2313e-01, -4.3015e-01,\n",
       "          6.4800e-01,  6.4997e-01,  4.2009e-01,  6.2996e-01,  6.1407e-01,\n",
       "         -1.1645e-01,  6.3233e-01, -1.0669e-01,  5.0293e-02,  2.0683e-01,\n",
       "         -2.7261e-01,  3.2725e-02, -4.0474e-02,  1.0070e+00]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelFp32(input.get_input_torch().cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import DatasetReader\n",
    "from dataset.utils import GLOBAL_CLASSES\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "    transforms.Resize((224, 224)),\n",
    "    # transforms.ToTensor(),  \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 predictions for Torch Local on :\n",
      "35 Banana                92.5%\n",
      "63 Hair Drier             0.4%\n",
      "8 Traffic Light          0.3%\n",
      "10 Parking Meter          0.2%\n",
      "51 Remote                 0.2%\n"
     ]
    }
   ],
   "source": [
    "# categories = helper.get_imagenet_categories()\n",
    "categories = GLOBAL_CLASSES\n",
    "\n",
    "helper.print_probablities_from_output(modelFp32.cpu()(transform(input.get_input_torch())), categories=categories, modelname = 'Torch Local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/centar15-desktop1/Desktop/SliciceZaTest/Igrica/image_533.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[569], line 39\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ke\u001b[38;5;241m.\u001b[39mequalize(img)\n\u001b[1;32m     28\u001b[0m transformHistEq \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     29\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mLambda(\u001b[38;5;28;01mlambda\u001b[39;00m img: img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m img\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m img),\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# transforms.RandomEqualize(p = 1),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.288\u001b[39m, \u001b[38;5;241m0.288\u001b[39m, \u001b[38;5;241m0.288\u001b[39m])  \n\u001b[1;32m     36\u001b[0m ])\n\u001b[0;32m---> 39\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolderpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/image_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43midx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# image = Image.open(folderpath + \"/imagenet_test_\" + f\"{idx}.JPEG\")\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# image = Image.open(folderpath + \"ILSVRC2012_test_\" + str(idx).zfill(8) + \".JPEG\")\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGBA\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# Convert the image to RGB (remove alpha channel)\u001b[39;00m\n",
      "File \u001b[0;32m~/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/PIL/Image.py:3465\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3462\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fp)\n\u001b[1;32m   3464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3465\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3466\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/centar15-desktop1/Desktop/SliciceZaTest/Igrica/image_533.jpg'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import skimage as ski\n",
    "from utils.helper import print_probablities_from_output\n",
    "categories = GLOBAL_CLASSES\n",
    "\n",
    "\n",
    "folderpath = \"/home/centar15-desktop1/Desktop/SliciceZaTest/Igrica\"\n",
    "\n",
    "# folderpath = \"/home/centar15-desktop1/Desktop/imagenet_test/\"\n",
    "categories = GLOBAL_CLASSES\n",
    "\n",
    "\n",
    "transformBest = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "    # transforms.RandomEqualize(p = 1),\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),  \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n",
    "])\n",
    "\n",
    "import kornia.enhance as ke\n",
    "\n",
    "class KorniaHistogramEqualization:\n",
    "    def __call__(self, img:torch.Tensor) -> torch.Tensor:\n",
    "        return ke.equalize(img)\n",
    "\n",
    "\n",
    "transformHistEq = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "    # transforms.RandomEqualize(p = 1),\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),\n",
    "    KorniaHistogramEqualization(),\n",
    "    \n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.288, 0.288, 0.288])  \n",
    "])\n",
    "\n",
    "\n",
    "image = Image.open(folderpath + \"/image_\" + f\"{idx}.jpg\")\n",
    "# image = Image.open(folderpath + \"/imagenet_test_\" + f\"{idx}.JPEG\")\n",
    "\n",
    "# image = Image.open(folderpath + \"ILSVRC2012_test_\" + str(idx).zfill(8) + \".JPEG\")\n",
    "\n",
    "if image.mode == 'RGBA':\n",
    "    # Convert the image to RGB (remove alpha channel)\n",
    "    image = image.convert('RGB')\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "modelFp32.cpu()\n",
    "modelFp32.eval()\n",
    "# modelDesktop.cpu()\n",
    "# modelDesktop.eval()\n",
    "\n",
    "print_probablities_from_output(modelFp32(transformBest(image).unsqueeze(0)), categories, 5, 'Model trained on jet')\n",
    "print(\"\")\n",
    "# print_probablities_from_output(modelDesktop(transformBest(image).unsqueeze(0)), categories, 5, 'Model trained on desktop')\n",
    "# print(\"\")\n",
    "# print_probablities_from_output(modelNoHistEq(transformBest(image).unsqueeze(0).cuda()), categories, 5, 'No EQ Stable diffusion training')\n",
    "\n",
    "idx = idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading tmps1pn1c03.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[34m\u001b[0m| 120M/120M [00:05<00:00, 21.4MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled compile job (j5wwk4qm5) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/j5wwk4qm5/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import qai_hub\n",
    "\n",
    "input_shape = (1, 3, 224, 224)\n",
    "dummy_input = torch.randn(input_shape)\n",
    "\n",
    "pt_model = torch.jit.trace(model, dummy_input)\n",
    "\n",
    "compile_job = qai_hub.submit_compile_job(\n",
    "    pt_model,\n",
    "    name=\"CASVIT_T_MODIFIED\", # Replace with your model name\n",
    "    device=qai_hub.Device(\"Snapdragon 8 Elite QRD\"),\n",
    "    input_specs=dict(image=input_shape)\n",
    ")\n",
    "\n",
    "# compile_job.modify_sharing(add_emails=['lowpowervision@gmail.com']) ## Share your model for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for compile job (j5wwk4qm5) completion. Type Ctrl+C to stop waiting at any time.\n",
      "     SUCCESS                          \u0007\n",
      "Scheduled profile job (jgn04krj5) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/jgn04krj5/\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ProfileJob\n",
       "----------\n",
       "job_id  : jgn04krj5\n",
       "url     : https://app.aihub.qualcomm.com/jobs/jgn04krj5/\n",
       "status  : CREATED\n",
       "model   : Model(model_id='mnoeo837q', name='job_j5wwk4qm5_optimized_tflite')\n",
       "name    : CASVIT_T_MODIFIED\n",
       "options : \n",
       "shapes  : {}\n",
       "device  : Device(name='Snapdragon 8 Elite QRD', os='15', attributes=['os:android', 'framework:tflite', 'framework:onnx', 'vendor:qualcomm', 'abi:aarch64-android', 'format:phone', 'chipset:qualcomm-snapdragon-8-elite', 'chipset:sm8750', 'hexagon:v79', 'soc-model:69', 'htp-supports-fp16:true', 'framework:qnn'])\n",
       "date    : 2025-03-21 20:23:27"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qai_hub.submit_profile_job(compile_job.get_target_model(), device=qai_hub.Device(\"Snapdragon 8 Elite QRD\"), name = \"CASVIT_T_MODIFIED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-21 21:12:23,076 - root - INFO - AIMET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/lib/python3.10/abc.py:106: FutureWarning: `NLLLoss2d` has been deprecated. Please use `NLLLoss` instead as a drop-in replacement and see https://pytorch.org/docs/main/nn.html#torch.nn.NLLLoss for more details.\n",
      "  cls = super().__new__(mcls, name, bases, namespace, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-21 21:12:25,868 - ConnectedGraph - WARNING - Unable to isolate model outputs.\n",
      "2025-03-21 21:12:26,085 - Quant - INFO - No config file provided, defaulting to config file at /home/centar15-desktop1/LPCV_2025_T1/.venv310/lib/python3.10/site-packages/aimet_common/quantsim_config/default_config.json\n",
      "2025-03-21 21:12:26,097 - Quant - INFO - Unsupported op type Squeeze\n",
      "2025-03-21 21:12:26,097 - Quant - INFO - Unsupported op type Mean\n",
      "2025-03-21 21:12:26,128 - Quant - INFO - Selecting DefaultOpInstanceConfigGenerator to compute the specialized config. hw_version:default\n"
     ]
    }
   ],
   "source": [
    "from aimet_torch.cross_layer_equalization import equalize_model\n",
    "\n",
    "from aimet_torch.quantsim import QuantizationSimModel\n",
    "from aimet_common.defs import QuantScheme\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Dummy input to define the model input size\n",
    "dummy_input = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "# Step 1: Create QuantizationSimModel\n",
    "sim = QuantizationSimModel(model, dummy_input=dummy_input,\n",
    "                                     quant_scheme=QuantScheme.post_training_tf_enhanced,\n",
    "                                     default_param_bw=8, default_output_bw=8)\n",
    "\n",
    "from dataset import DatasetReader\n",
    "\n",
    "# Step 2: Compute Encodings (calibration)\n",
    "def calibration_function(model, eval_iterations = 100, use_cuda = False):\n",
    "    for i in range(eval_iterations):\n",
    "        data = torch.randn(1, 3, 224, 224)\n",
    "        print(i)\n",
    "        model(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RCViT(\n",
      "  (patch_embed): Sequential(\n",
      "    (0): QuantizedConv2d(\n",
      "      3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
      "      (param_quantizers): ModuleDict(\n",
      "        (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "        (bias): None\n",
      "      )\n",
      "      (input_quantizers): ModuleList(\n",
      "        (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "      )\n",
      "      (output_quantizers): ModuleList(\n",
      "        (0): None\n",
      "      )\n",
      "    )\n",
      "    (1): QuantizedBatchNorm2d(\n",
      "      48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "      (param_quantizers): ModuleDict(\n",
      "        (weight): None\n",
      "        (bias): None\n",
      "      )\n",
      "      (input_quantizers): ModuleList(\n",
      "        (0): None\n",
      "      )\n",
      "      (output_quantizers): ModuleList(\n",
      "        (0): None\n",
      "      )\n",
      "    )\n",
      "    (2): QuantizedReLU(\n",
      "      (param_quantizers): ModuleDict()\n",
      "      (input_quantizers): ModuleList(\n",
      "        (0): None\n",
      "      )\n",
      "      (output_quantizers): ModuleList(\n",
      "        (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "      )\n",
      "    )\n",
      "    (3): QuantizedConv2d(\n",
      "      48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
      "      (param_quantizers): ModuleDict(\n",
      "        (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "        (bias): None\n",
      "      )\n",
      "      (input_quantizers): ModuleList(\n",
      "        (0): None\n",
      "      )\n",
      "      (output_quantizers): ModuleList(\n",
      "        (0): None\n",
      "      )\n",
      "    )\n",
      "    (4): QuantizedBatchNorm2d(\n",
      "      96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "      (param_quantizers): ModuleDict(\n",
      "        (weight): None\n",
      "        (bias): None\n",
      "      )\n",
      "      (input_quantizers): ModuleList(\n",
      "        (0): None\n",
      "      )\n",
      "      (output_quantizers): ModuleList(\n",
      "        (0): None\n",
      "      )\n",
      "    )\n",
      "    (5): QuantizedReLU(\n",
      "      (param_quantizers): ModuleDict()\n",
      "      (input_quantizers): ModuleList(\n",
      "        (0): None\n",
      "      )\n",
      "      (output_quantizers): ModuleList(\n",
      "        (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (network): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): QuantizedConv2d(\n",
      "              96, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedConv2d(\n",
      "              96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (3): QuantizedGELU(\n",
      "              approximate='none'\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (4): QuantizedConv2d(\n",
      "              96, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm1): QuantizedBatchNorm2d(\n",
      "          96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): QuantizedConv2d(\n",
      "            96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  96, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  96, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): QuantizedConv2d(\n",
      "            96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj): QuantizedConv2d(\n",
      "            96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj_drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedBatchNorm2d(\n",
      "          96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedConv2d(\n",
      "            96, 384, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (act): QuantizedGELU(\n",
      "            approximate='none'\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            384, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): QuantizedConv2d(\n",
      "              96, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedConv2d(\n",
      "              96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (3): QuantizedGELU(\n",
      "              approximate='none'\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (4): QuantizedConv2d(\n",
      "              96, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm1): QuantizedBatchNorm2d(\n",
      "          96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): QuantizedConv2d(\n",
      "            96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  96, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  96, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): QuantizedConv2d(\n",
      "            96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj): QuantizedConv2d(\n",
      "            96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj_drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedBatchNorm2d(\n",
      "          96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedConv2d(\n",
      "            96, 384, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (act): QuantizedGELU(\n",
      "            approximate='none'\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            384, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): QuantizedConv2d(\n",
      "              96, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedConv2d(\n",
      "              96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (3): QuantizedGELU(\n",
      "              approximate='none'\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (4): QuantizedConv2d(\n",
      "              96, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm1): QuantizedBatchNorm2d(\n",
      "          96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): QuantizedConv2d(\n",
      "            96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  96, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  96, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): QuantizedConv2d(\n",
      "            96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj): QuantizedConv2d(\n",
      "            96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj_drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedBatchNorm2d(\n",
      "          96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedConv2d(\n",
      "            96, 384, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (act): QuantizedGELU(\n",
      "            approximate='none'\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            384, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Embedding(\n",
      "      (proj): QuantizedConv2d(\n",
      "        96, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (norm): QuantizedBatchNorm2d(\n",
      "        128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): None\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): QuantizedConv2d(\n",
      "              128, 128, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedConv2d(\n",
      "              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (3): QuantizedGELU(\n",
      "              approximate='none'\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (4): QuantizedConv2d(\n",
      "              128, 128, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm1): QuantizedBatchNorm2d(\n",
      "          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): QuantizedConv2d(\n",
      "            128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): QuantizedConv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj): QuantizedConv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj_drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedBatchNorm2d(\n",
      "          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedConv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (act): QuantizedGELU(\n",
      "            approximate='none'\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): QuantizedConv2d(\n",
      "              128, 128, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedConv2d(\n",
      "              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (3): QuantizedGELU(\n",
      "              approximate='none'\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (4): QuantizedConv2d(\n",
      "              128, 128, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm1): QuantizedBatchNorm2d(\n",
      "          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): QuantizedConv2d(\n",
      "            128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): QuantizedConv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj): QuantizedConv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj_drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedBatchNorm2d(\n",
      "          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedConv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (act): QuantizedGELU(\n",
      "            approximate='none'\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): QuantizedConv2d(\n",
      "              128, 128, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedConv2d(\n",
      "              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (3): QuantizedGELU(\n",
      "              approximate='none'\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (4): QuantizedConv2d(\n",
      "              128, 128, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm1): QuantizedBatchNorm2d(\n",
      "          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): QuantizedConv2d(\n",
      "            128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): QuantizedConv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj): QuantizedConv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj_drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedBatchNorm2d(\n",
      "          128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedConv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (act): QuantizedGELU(\n",
      "            approximate='none'\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): Embedding(\n",
      "      (proj): QuantizedConv2d(\n",
      "        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (norm): QuantizedBatchNorm2d(\n",
      "        256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): None\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): QuantizedConv2d(\n",
      "              256, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedConv2d(\n",
      "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (3): QuantizedGELU(\n",
      "              approximate='none'\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (4): QuantizedConv2d(\n",
      "              256, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm1): QuantizedBatchNorm2d(\n",
      "          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): QuantizedConv2d(\n",
      "            256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): QuantizedConv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj): QuantizedConv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj_drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedBatchNorm2d(\n",
      "          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedConv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (act): QuantizedGELU(\n",
      "            approximate='none'\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): QuantizedConv2d(\n",
      "              256, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedConv2d(\n",
      "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (3): QuantizedGELU(\n",
      "              approximate='none'\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (4): QuantizedConv2d(\n",
      "              256, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm1): QuantizedBatchNorm2d(\n",
      "          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): QuantizedConv2d(\n",
      "            256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): QuantizedConv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj): QuantizedConv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj_drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedBatchNorm2d(\n",
      "          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedConv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (act): QuantizedGELU(\n",
      "            approximate='none'\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): QuantizedConv2d(\n",
      "              256, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedConv2d(\n",
      "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (3): QuantizedGELU(\n",
      "              approximate='none'\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (4): QuantizedConv2d(\n",
      "              256, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm1): QuantizedBatchNorm2d(\n",
      "          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): QuantizedConv2d(\n",
      "            256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): QuantizedConv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj): QuantizedConv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj_drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedBatchNorm2d(\n",
      "          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedConv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (act): QuantizedGELU(\n",
      "            approximate='none'\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): QuantizedConv2d(\n",
      "              256, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedConv2d(\n",
      "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (3): QuantizedGELU(\n",
      "              approximate='none'\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (4): QuantizedConv2d(\n",
      "              256, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm1): QuantizedBatchNorm2d(\n",
      "          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): QuantizedConv2d(\n",
      "            256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): QuantizedConv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj): QuantizedConv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj_drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedBatchNorm2d(\n",
      "          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedConv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (act): QuantizedGELU(\n",
      "            approximate='none'\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): QuantizedConv2d(\n",
      "              256, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedConv2d(\n",
      "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (3): QuantizedGELU(\n",
      "              approximate='none'\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (4): QuantizedConv2d(\n",
      "              256, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm1): QuantizedBatchNorm2d(\n",
      "          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): QuantizedConv2d(\n",
      "            256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): QuantizedConv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj): QuantizedConv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj_drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedBatchNorm2d(\n",
      "          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedConv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (act): QuantizedGELU(\n",
      "            approximate='none'\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): QuantizedConv2d(\n",
      "              256, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedConv2d(\n",
      "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (3): QuantizedGELU(\n",
      "              approximate='none'\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (4): QuantizedConv2d(\n",
      "              256, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm1): QuantizedBatchNorm2d(\n",
      "          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): QuantizedConv2d(\n",
      "            256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): QuantizedConv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj): QuantizedConv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj_drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedBatchNorm2d(\n",
      "          256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedConv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (act): QuantizedGELU(\n",
      "            approximate='none'\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): Embedding(\n",
      "      (proj): QuantizedConv2d(\n",
      "        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (norm): QuantizedBatchNorm2d(\n",
      "        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): None\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): QuantizedConv2d(\n",
      "              512, 512, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedConv2d(\n",
      "              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (3): QuantizedGELU(\n",
      "              approximate='none'\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (4): QuantizedConv2d(\n",
      "              512, 512, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm1): QuantizedBatchNorm2d(\n",
      "          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): QuantizedConv2d(\n",
      "            512, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): QuantizedConv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj): QuantizedConv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj_drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedBatchNorm2d(\n",
      "          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedConv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (act): QuantizedGELU(\n",
      "            approximate='none'\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): QuantizedConv2d(\n",
      "              512, 512, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedConv2d(\n",
      "              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (3): QuantizedGELU(\n",
      "              approximate='none'\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (4): QuantizedConv2d(\n",
      "              512, 512, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm1): QuantizedBatchNorm2d(\n",
      "          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): QuantizedConv2d(\n",
      "            512, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): QuantizedConv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj): QuantizedConv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj_drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedBatchNorm2d(\n",
      "          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedConv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (act): QuantizedGELU(\n",
      "            approximate='none'\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): QuantizedConv2d(\n",
      "              512, 512, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedConv2d(\n",
      "              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (3): QuantizedGELU(\n",
      "              approximate='none'\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (4): QuantizedConv2d(\n",
      "              512, 512, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm1): QuantizedBatchNorm2d(\n",
      "          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): QuantizedConv2d(\n",
      "            512, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): QuantizedConv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj): QuantizedConv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj_drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedBatchNorm2d(\n",
      "          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedConv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (act): QuantizedGELU(\n",
      "            approximate='none'\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): Embedding(\n",
      "      (proj): QuantizedConv2d(\n",
      "        512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (norm): QuantizedBatchNorm2d(\n",
      "        512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): None\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): Sequential(\n",
      "      (0): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): QuantizedConv2d(\n",
      "              512, 512, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedConv2d(\n",
      "              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (3): QuantizedGELU(\n",
      "              approximate='none'\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (4): QuantizedConv2d(\n",
      "              512, 512, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm1): QuantizedBatchNorm2d(\n",
      "          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): QuantizedConv2d(\n",
      "            512, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): QuantizedConv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj): QuantizedConv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj_drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedBatchNorm2d(\n",
      "          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedConv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (act): QuantizedGELU(\n",
      "            approximate='none'\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): AdditiveBlock(\n",
      "        (local_perception): LocalIntegration(\n",
      "          (network): Sequential(\n",
      "            (0): QuantizedConv2d(\n",
      "              512, 512, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "            )\n",
      "            (1): QuantizedBatchNorm2d(\n",
      "              512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): None\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (2): QuantizedConv2d(\n",
      "              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (3): QuantizedGELU(\n",
      "              approximate='none'\n",
      "              (param_quantizers): ModuleDict()\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "            (4): QuantizedConv2d(\n",
      "              512, 512, kernel_size=(1, 1), stride=(1, 1)\n",
      "              (param_quantizers): ModuleDict(\n",
      "                (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                (bias): None\n",
      "              )\n",
      "              (input_quantizers): ModuleList(\n",
      "                (0): None\n",
      "              )\n",
      "              (output_quantizers): ModuleList(\n",
      "                (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm1): QuantizedBatchNorm2d(\n",
      "          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (attn): AdditiveTokenMixer(\n",
      "          (qkv): QuantizedConv2d(\n",
      "            512, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (oper_q): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (oper_k): Sequential(\n",
      "            (0): SpatialOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedConv2d(\n",
      "                  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedBatchNorm2d(\n",
      "                  512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): None\n",
      "                    (bias): None\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedReLU(\n",
      "                  inplace=True\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (3): QuantizedConv2d(\n",
      "                  512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (4): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): ChannelOperation(\n",
      "              (block): Sequential(\n",
      "                (0): QuantizedAdaptiveAvgPool2d(\n",
      "                  output_size=(1, 1)\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (1): QuantizedConv2d(\n",
      "                  512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "                  (param_quantizers): ModuleDict(\n",
      "                    (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "                  )\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "                (2): QuantizedSigmoid(\n",
      "                  (param_quantizers): ModuleDict()\n",
      "                  (input_quantizers): ModuleList(\n",
      "                    (0): None\n",
      "                  )\n",
      "                  (output_quantizers): ModuleList(\n",
      "                    (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (dwc): QuantizedConv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj): QuantizedConv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (proj_drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): QuantizedBatchNorm2d(\n",
      "          512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): Mlp(\n",
      "          (fc1): QuantizedConv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (act): QuantizedGELU(\n",
      "            approximate='none'\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (fc2): QuantizedConv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (param_quantizers): ModuleDict(\n",
      "              (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "              (bias): None\n",
      "            )\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "            )\n",
      "          )\n",
      "          (drop): QuantizedDropout(\n",
      "            p=0.0, inplace=False\n",
      "            (param_quantizers): ModuleDict()\n",
      "            (input_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "            (output_quantizers): ModuleList(\n",
      "              (0): None\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): QuantizedBatchNorm2d(\n",
      "    512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "    (param_quantizers): ModuleDict(\n",
      "      (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "      (bias): None\n",
      "    )\n",
      "    (input_quantizers): ModuleList(\n",
      "      (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "    )\n",
      "    (output_quantizers): ModuleList(\n",
      "      (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "    )\n",
      "  )\n",
      "  (head): QuantizedLinear(\n",
      "    in_features=512, out_features=64, bias=True\n",
      "    (param_quantizers): ModuleDict(\n",
      "      (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "      (bias): None\n",
      "    )\n",
      "    (input_quantizers): ModuleList(\n",
      "      (0): None\n",
      "    )\n",
      "    (output_quantizers): ModuleList(\n",
      "      (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "    )\n",
      "  )\n",
      "  (dist_head): QuantizedLinear(\n",
      "    in_features=512, out_features=64, bias=True\n",
      "    (param_quantizers): ModuleDict(\n",
      "      (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "      (bias): None\n",
      "    )\n",
      "    (input_quantizers): ModuleList(\n",
      "      (0): None\n",
      "    )\n",
      "    (output_quantizers): ModuleList(\n",
      "      (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(sim.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DequantizedTensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                    0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                    0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                    0., 0., 0., 0.]], grad_fn=<AliasBackward0>)"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim.model(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "sim.compute_encodings(calibration_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 predictions for Torch Local on :\n",
      "63 Hair Drier             1.6%\n",
      "62 Teddy Bear             1.6%\n",
      "61 Vase                   1.6%\n",
      "60 Clock                  1.6%\n",
      "59 Book                   1.6%\n"
     ]
    }
   ],
   "source": [
    "helper.print_probablities_from_output(sim.model(transform(input.get_input_torch())), categories=categories, modelname = 'Torch Local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-21 20:28:03,493 - Utils - INFO - successfully created onnx model with 645/1198 node names updated\n",
      "2025-03-21 20:28:03,718 - Quant - WARNING - number of input quantizers: 1 available for layer: network.0.0.norm1 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,723 - Quant - WARNING - number of input quantizers: 1 available for layer: network.0.0.norm2 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,726 - Quant - WARNING - number of input quantizers: 1 available for layer: network.0.1.norm1 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,731 - Quant - WARNING - number of input quantizers: 1 available for layer: network.0.1.norm2 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,733 - Quant - WARNING - number of input quantizers: 1 available for layer: network.0.2.norm1 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,738 - Quant - WARNING - number of input quantizers: 1 available for layer: network.0.2.norm2 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,741 - Quant - WARNING - number of input quantizers: 1 available for layer: network.2.0.norm1 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,746 - Quant - WARNING - number of input quantizers: 1 available for layer: network.2.0.norm2 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,748 - Quant - WARNING - number of input quantizers: 1 available for layer: network.2.1.norm1 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,753 - Quant - WARNING - number of input quantizers: 1 available for layer: network.2.1.norm2 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,756 - Quant - WARNING - number of input quantizers: 1 available for layer: network.2.2.norm1 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,761 - Quant - WARNING - number of input quantizers: 1 available for layer: network.2.2.norm2 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,765 - Quant - WARNING - number of input quantizers: 1 available for layer: network.4.0.norm1 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,770 - Quant - WARNING - number of input quantizers: 1 available for layer: network.4.0.norm2 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,772 - Quant - WARNING - number of input quantizers: 1 available for layer: network.4.1.norm1 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,777 - Quant - WARNING - number of input quantizers: 1 available for layer: network.4.1.norm2 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,780 - Quant - WARNING - number of input quantizers: 1 available for layer: network.4.2.norm1 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,785 - Quant - WARNING - number of input quantizers: 1 available for layer: network.4.2.norm2 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,787 - Quant - WARNING - number of input quantizers: 1 available for layer: network.4.3.norm1 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,792 - Quant - WARNING - number of input quantizers: 1 available for layer: network.4.3.norm2 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,795 - Quant - WARNING - number of input quantizers: 1 available for layer: network.4.4.norm1 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,800 - Quant - WARNING - number of input quantizers: 1 available for layer: network.4.4.norm2 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,802 - Quant - WARNING - number of input quantizers: 1 available for layer: network.4.5.norm1 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,807 - Quant - WARNING - number of input quantizers: 1 available for layer: network.4.5.norm2 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,810 - Quant - WARNING - number of input quantizers: 1 available for layer: network.6.0.norm1 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,815 - Quant - WARNING - number of input quantizers: 1 available for layer: network.6.0.norm2 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,818 - Quant - WARNING - number of input quantizers: 1 available for layer: network.6.1.norm1 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,823 - Quant - WARNING - number of input quantizers: 1 available for layer: network.6.1.norm2 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,826 - Quant - WARNING - number of input quantizers: 1 available for layer: network.6.2.norm1 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,831 - Quant - WARNING - number of input quantizers: 1 available for layer: network.6.2.norm2 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,834 - Quant - WARNING - number of input quantizers: 1 available for layer: network.8.0.norm1 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,839 - Quant - WARNING - number of input quantizers: 1 available for layer: network.8.0.norm2 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,841 - Quant - WARNING - number of input quantizers: 1 available for layer: network.8.1.norm1 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,846 - Quant - WARNING - number of input quantizers: 1 available for layer: network.8.1.norm2 doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,848 - Quant - WARNING - number of input quantizers: 1 available for layer: norm doesn't match with number of input tensors: 3\n",
      "2025-03-21 20:28:03,849 - Quant - WARNING - The following layers were not found in the exported onnx model. Encodings for these layers will not appear in the exported encodings file, however it will continue to exist in torch encoding file:\n",
      "['dist_head']\n",
      "This can be due to several reasons:\n",
      "\t- The layer is set to quantize with float datatype, but was not exercised in compute encodings. Not an issue if the layer is not meant to be run.\n",
      "\t- The layer has valid encodings but was not seen while exporting to onnx using the dummy input provided in sim.export(). Ensure that the dummy input covers all layers.\n",
      "2025-03-21 20:28:03,849 - Quant - INFO - Layers excluded from quantization: []\n",
      "2025-03-21 20:28:03,856 - Quant - WARNING - \u001b[31;21mQuantsim export will stop exporting encodings for saving and loading in a future AIMET release.\n",
      "To export encodings for saving and loading, use QuantizationSimModel's save_encodings_to_json() utility instead.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folderpath = \"q1\"\n",
    "\n",
    "os.makedirs(folderpath, exist_ok=True)\n",
    "\n",
    "sim.export(path=folderpath, filename_prefix='CASVIT_T_EXTENDED_BIG', dummy_input=dummy_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading Q2.aimet.zip\n",
      "2025-03-21 20:29:30,283 - root - INFO - Uploading asset to https://tetrahub-qprod-userdata.s3-accelerate.amazonaws.com/models/mnjlo5g9q_TdIqELt6pRSXxhFg.aimet.zip?uploadId=GvlHNGMEy_ZBBMe0GYBuBGXBIoLkue2IHcmxF0RmCuh43Tbtdlu4gI5C.I8JJvfsWfLoP5Ss4uj1oiEOegryFluD71vjSB9hHGuYQuJiBPwRjyMTeGPtG2SyPQFwtwg4RN2i04BCz9isz3e1bUofsKsN434COP4mmgqnsyjPLbU-&partNumber=1&A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[34m\u001b[0m| 46.9M/46.9M [00:03<00:00, 15.8MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-21 20:29:33,390 - root - INFO - Successfully uploaded asset with response status: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled compile job (jp3nd6lz5) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/jp3nd6lz5/\n",
      "\n",
      "Uploading Q2.aimet.zip\n",
      "2025-03-21 20:29:36,857 - root - INFO - Uploading asset to https://tetrahub-qprod-userdata.s3-accelerate.amazonaws.com/models/mqed2kvym_x5Dr4rz2JzJ3Hjiq.aimet.zip?uploadId=VAGiC.tI6vTQs1kqjwGvjeCaJlvNN4m5Z4IIpEnr6ESNqY47YOfwMyY69l7VkXElVl2gqRTeqec4Teoswuisq796BeURd1MYI8KXPhm.64cu9vTnENYeSq9KtN7OVb0eZqrABpKzJDSmsf7okfjBxSsbirzwz8NNXWdyFZx14WU-&partNumber=1&A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[34m\u001b[0m| 46.9M/46.9M [00:02<00:00, 17.5MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-21 20:29:39,673 - root - INFO - Successfully uploaded asset with response status: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled compile job (jgozx87dp) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/jgozx87dp/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import qai_hub\n",
    "\n",
    "name = 'CASVIT_T_EXTENDED'\n",
    "\n",
    "compile_job = qai_hub.submit_compile_job(\n",
    "    model=\"Q2.aimet\",\n",
    "    device=qai_hub.Device(\"Snapdragon 8 Elite QRD\"),\n",
    "    name = name + 'tflite'\n",
    ")\n",
    "assert isinstance(compile_job, qai_hub.CompileJob)\n",
    "\n",
    "# Compile to a QNN Model Library\n",
    "compile_job_qnn = qai_hub.submit_compile_job(\n",
    "    model=\"Q2.aimet\",\n",
    "    device=qai_hub.Device(\"Snapdragon 8 Elite QRD\"),\n",
    "    options=\"--target_runtime qnn_lib_aarch64_android --quantize_full_type int8\",\n",
    "    name = name + 'qnn'\n",
    ")\n",
    "assert isinstance(compile_job_qnn, qai_hub.CompileJob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled profile job (jg90ry8vg) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/jg90ry8vg/\n",
      "\n",
      "Scheduled profile job (j572mlkrp) successfully. To see the status and results:\n",
      "    https://app.aihub.qualcomm.com/jobs/j572mlkrp/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile_job_tflite = qai_hub.submit_profile_job(\n",
    "\n",
    "    model = compile_job.get_target_model(),\n",
    "    device=qai_hub.Device(\"Snapdragon 8 Elite QRD\"),\n",
    "    name = name + 'tflite'\n",
    ")\n",
    "# assert isinstance(compile_job, qai_hub.CompileJob)\n",
    "\n",
    "# Compile to a QNN Model Library\n",
    "profile_job_qnn = qai_hub.submit_profile_job(\n",
    "    model= compile_job_qnn.get_target_model(),\n",
    "    device=qai_hub.Device(\"Snapdragon 8 Elite QRD\"),\n",
    "    name = name + 'qnn'\n",
    ")\n",
    "# assert isinstance(compile_job_qnn, qai_hub.CompileJob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
